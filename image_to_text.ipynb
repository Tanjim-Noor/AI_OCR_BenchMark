{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da15fcbc",
   "metadata": {},
   "source": [
    "# AI OCR Benchmark System\n",
    "\n",
    "This notebook demonstrates a comprehensive OCR (Optical Character Recognition) system that can extract structured data from images using multiple AI models including Google Gemini, OpenAI GPT-4V, and Claude.\n",
    "\n",
    "## Features\n",
    "- ‚úÖ **Model Agnostic** - Switch between different AI models\n",
    "- ‚úÖ **Batch Processing** - Process entire directories of images\n",
    "- ‚úÖ **Custom Prompts** - Flexible prompt engineering for different document types\n",
    "- ‚úÖ **JSON Output** - Structured data extraction with automatic parsing\n",
    "- ‚úÖ **Image Optimization** - Automatic image resizing and compression\n",
    "- ‚úÖ **Scoring System** - Benchmark and compare model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8717b0fc",
   "metadata": {},
   "source": [
    "## Library Imports and Dependencies\n",
    "\n",
    "This section imports all necessary libraries for the OCR system including LangChain components, image processing libraries, and AI model clients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04918598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All required libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import base64\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional, Union\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import re\n",
    "from datetime import datetime\n",
    "from openai import OpenAI\n",
    "from pydantic import SecretStr\n",
    "\n",
    "# LangChain imports\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "# For image processing\n",
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "print(\"All required libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dddaab3",
   "metadata": {},
   "source": [
    "## OCR Benchmark Class Definition\n",
    "\n",
    "This section defines the main OCRBenchmark class with global configuration variables, utility functions for JSON parsing, and the complete implementation of the model-agnostic OCR system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "938319a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OCRBenchmark class defined successfully!\n",
      "üîß Global settings: Optimization=True, Resize ratio=0.5\n"
     ]
    }
   ],
   "source": [
    "# Global Configuration Variables for Image Processing\n",
    "# Set to False to disable image resizing/optimization\n",
    "ENABLE_IMAGE_OPTIMIZATION = True\n",
    "# Resize ratio (0.5 = 50% of original size, 1.0 = no resize)\n",
    "IMAGE_RESIZE_RATIO = 0.5\n",
    "MAX_IMAGE_DIMENSION = 1024  # Maximum width or height in pixels\n",
    "MAX_FILE_SIZE_MB = 4  # Maximum file size in MB before optimization\n",
    "JPEG_QUALITY = 70  # JPEG compression quality (1-100)\n",
    "\n",
    "# JSON Parsing and Formatting Utilities\n",
    "def parse_json_from_text(text: str):\n",
    "    \"\"\"\n",
    "    Extract and parse JSON from text that might contain markdown code blocks or other formatting.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # First try to parse directly\n",
    "        return json.loads(text)\n",
    "    except json.JSONDecodeError:\n",
    "        pass\n",
    "    \n",
    "    # Try to extract JSON from markdown code blocks\n",
    "    json_patterns = [\n",
    "        r'```json\\s*(\\{.*?\\})\\s*```',  # ```json ... ```\n",
    "        r'```\\s*(\\{.*?\\})\\s*```',      # ``` ... ```\n",
    "        r'(\\{.*?\\})',                   # Just the JSON object\n",
    "    ]\n",
    "    \n",
    "    for pattern in json_patterns:\n",
    "        matches = re.findall(pattern, text, re.DOTALL)\n",
    "        for match in matches:\n",
    "            try:\n",
    "                return json.loads(match.strip())\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "    \n",
    "    # If all parsing fails, return the original text\n",
    "    return text\n",
    "\n",
    "def format_extraction_result(result: Dict) -> Dict:\n",
    "    \"\"\"\n",
    "    Format the extraction result with parsed JSON data for better readability.\n",
    "    \"\"\"\n",
    "    if result.get(\"success\") and \"extracted_data\" in result:\n",
    "        try:\n",
    "            # Try to parse the extracted data as JSON\n",
    "            parsed_data = parse_json_from_text(result[\"extracted_data\"])\n",
    "            \n",
    "            # If successfully parsed, add it as a separate field\n",
    "            if isinstance(parsed_data, (dict, list)):\n",
    "                result[\"extracted_data_parsed\"] = parsed_data\n",
    "                result[\"extraction_format\"] = \"json\"\n",
    "            else:\n",
    "                result[\"extraction_format\"] = \"text\"\n",
    "                \n",
    "        except Exception as e:\n",
    "            result[\"extraction_format\"] = \"text\"\n",
    "            result[\"parsing_error\"] = str(e)\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "class OCRBenchmark:\n",
    "    \"\"\"\n",
    "    A model-agnostic OCR system using LangChain that can extract data from images\n",
    "    using different AI models (Gemini, OpenAI GPT-4V, Claude).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_name: str = \"gemini\", temperature: float = 0.1):\n",
    "        \"\"\"\n",
    "        Initialize the OCR Benchmark system.\n",
    "\n",
    "        Args:\n",
    "            model_name: The model to use (\"gemini\", \"openai\", \"anthropic\")\n",
    "            temperature: Temperature for model generation\n",
    "        \"\"\"\n",
    "        # Load environment variables\n",
    "        load_dotenv()\n",
    "\n",
    "        self.model_name = model_name\n",
    "        self.temperature = temperature\n",
    "        self.model = None\n",
    "\n",
    "        # Supported image formats\n",
    "        self.supported_formats = {'.png', '.jpg', '.jpeg', '.webp', '.gif'}\n",
    "\n",
    "        # MIME type mapping for correct image format handling\n",
    "        self.mime_type_map = {\n",
    "            '.jpg': 'jpeg',\n",
    "            '.jpeg': 'jpeg',\n",
    "            '.png': 'png',\n",
    "            '.webp': 'webp',\n",
    "            '.gif': 'gif'\n",
    "        }\n",
    "\n",
    "        # Image optimization settings (now using global variables)\n",
    "        self.enable_optimization = ENABLE_IMAGE_OPTIMIZATION\n",
    "        self.resize_ratio = IMAGE_RESIZE_RATIO\n",
    "        self.max_image_size = (MAX_IMAGE_DIMENSION, MAX_IMAGE_DIMENSION)\n",
    "        self.max_file_size_mb = MAX_FILE_SIZE_MB\n",
    "        self.jpeg_quality = JPEG_QUALITY\n",
    "\n",
    "        # Try to initialize model, but don't fail if API key is missing\n",
    "        try:\n",
    "            self.model = self._initialize_model()\n",
    "        except ValueError as e:\n",
    "            print(f\"‚ö†Ô∏è Warning: {e}\")\n",
    "            print(\n",
    "                f\"Model '{model_name}' will be initialized when needed if API key becomes available.\")\n",
    "\n",
    "    def _initialize_model(self):\n",
    "        \"\"\"Initialize the specified model.\"\"\"\n",
    "        if self.model_name.lower() == \"gemini\":\n",
    "            api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "            if not api_key or api_key == \"your_google_api_key_here\":\n",
    "                raise ValueError(\n",
    "                    \"GOOGLE_API_KEY not found or not set. Please set your GOOGLE_API_KEY in the .env file\")\n",
    "            return ChatGoogleGenerativeAI(\n",
    "                model=\"gemini-2.0-flash\",  # Updated to newer model\n",
    "                temperature=self.temperature,\n",
    "                google_api_key=api_key,\n",
    "                timeout=60  # Set reasonable timeout\n",
    "            )\n",
    "\n",
    "        elif self.model_name.lower() == \"openai\":\n",
    "            api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "            if not api_key or api_key == \"your_openai_api_key_here\":\n",
    "                raise ValueError(\n",
    "                    \"OPENAI_API_KEY not found or not set. Please set your OPENAI_API_KEY in the .env file\")\n",
    "            return ChatOpenAI(\n",
    "                model=\"gpt-4o\",\n",
    "                temperature=self.temperature,\n",
    "                timeout=60\n",
    "            )\n",
    "\n",
    "        elif self.model_name.lower() == \"anthropic\":\n",
    "            return ChatAnthropic(\n",
    "                model_name=\"claude-3-5-sonnet-20241022\",\n",
    "                temperature=self.temperature,\n",
    "                timeout=60,\n",
    "                stop=None\n",
    "            )\n",
    "            \n",
    "        elif self.model_name.lower() == \"huggingface\":\n",
    "            api_key = os.getenv(\"HF_TOKEN\")\n",
    "            if not api_key:\n",
    "                raise ValueError(\n",
    "                    \"HF_TOKEN not found. Please set your Hugging Face API token in the .env file\"\n",
    "                )\n",
    "            return ChatOpenAI(\n",
    "                model=\"Qwen/Qwen2.5-VL-72B-Instruct:nebius\",  # or make this configurable\n",
    "                temperature=self.temperature,\n",
    "                base_url=\"https://router.huggingface.co/v1\",\n",
    "                api_key=SecretStr(api_key),\n",
    "                timeout=60,\n",
    "    )\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported model: {self.model_name}\")\n",
    "\n",
    "    def _optimize_image(self, image_path: str) -> bytes:\n",
    "        \"\"\"Optimize image size and quality for API processing.\"\"\"\n",
    "        try:\n",
    "            # Check if optimization is enabled\n",
    "            if not self.enable_optimization:\n",
    "                print(f\"üîß Image optimization disabled - using original file\")\n",
    "                with open(image_path, \"rb\") as f:\n",
    "                    return f.read()\n",
    "\n",
    "            # Check file size first\n",
    "            file_size_mb = os.path.getsize(image_path) / (1024 * 1024)\n",
    "            print(f\"üìè Original file size: {file_size_mb:.2f} MB\")\n",
    "\n",
    "            with Image.open(image_path) as img:\n",
    "                # Convert to RGB if necessary\n",
    "                if img.mode in ('RGBA', 'P'):\n",
    "                    img = img.convert('RGB')\n",
    "\n",
    "                original_size = img.size\n",
    "                print(\n",
    "                    f\"üìê Original dimensions: {original_size[0]}x{original_size[1]}\")\n",
    "\n",
    "                # Apply more aggressive optimization for HuggingFace due to strict size limits\n",
    "                if self.model_name.lower() == \"huggingface\":\n",
    "                    # Use smaller resize ratio for HuggingFace\n",
    "                    hf_resize_ratio = min(self.resize_ratio, 0.5)  # Max 50% of original size\n",
    "                    hf_max_dimension = min(self.max_image_size[0], 900)  # Max 900px\n",
    "                    hf_quality = min(self.jpeg_quality, 50)  # Lower quality\n",
    "                    \n",
    "                    new_width = int(img.size[0] * hf_resize_ratio)\n",
    "                    new_height = int(img.size[1] * hf_resize_ratio)\n",
    "                    img = img.resize((new_width, new_height), Image.Resampling.LANCZOS)\n",
    "                    print(f\"üîß HuggingFace aggressive resize by ratio {hf_resize_ratio}: {img.size[0]}x{img.size[1]}\")\n",
    "                    \n",
    "                    # # Further resize if still too large\n",
    "                    # if img.size[0] > hf_max_dimension or img.size[1] > hf_max_dimension:\n",
    "                    #     img.thumbnail((hf_max_dimension, hf_max_dimension), Image.Resampling.LANCZOS)\n",
    "                    #     print(f\"üîß HuggingFace thumbnail resize to: {img.size[0]}x{img.size[1]}\")\n",
    "                    \n",
    "                    # Save with lower quality for HuggingFace\n",
    "                    img_byte_arr = io.BytesIO()\n",
    "                    if Path(image_path).suffix.lower() in ['.jpg', '.jpeg']:\n",
    "                        img.save(img_byte_arr, format='JPEG', quality=hf_quality, optimize=True)\n",
    "                        print(f\"üíæ HuggingFace JPEG quality: {hf_quality}\")\n",
    "                    else:\n",
    "                        img.save(img_byte_arr, format='PNG', optimize=True)\n",
    "                        print(f\"üíæ HuggingFace PNG optimized\")\n",
    "                    \n",
    "                    optimized_data = img_byte_arr.getvalue()\n",
    "                    optimized_size_mb = len(optimized_data) / (1024 * 1024)\n",
    "                    print(f\"‚úÖ HuggingFace optimized size: {optimized_size_mb:.2f} MB\")\n",
    "                    \n",
    "                    # # If still too large, try even more aggressive compression\n",
    "                    # if optimized_size_mb > 0.3:  # 300KB limit for HuggingFace\n",
    "                    #     print(f\"‚ö†Ô∏è Still too large, applying ultra-compression...\")\n",
    "                    #     # Ultra-aggressive resizing\n",
    "                    #     ultra_dimension = 512\n",
    "                    #     img.thumbnail((ultra_dimension, ultra_dimension), Image.Resampling.LANCZOS)\n",
    "                    #     print(f\"üîß Ultra resize to: {img.size[0]}x{img.size[1]}\")\n",
    "                        \n",
    "                    #     # Ultra-low quality\n",
    "                    #     img_byte_arr = io.BytesIO()\n",
    "                    #     if Path(image_path).suffix.lower() in ['.jpg', '.jpeg']:\n",
    "                    #         img.save(img_byte_arr, format='JPEG', quality=60, optimize=True)\n",
    "                    #         print(f\"üíæ Ultra JPEG quality: 60\")\n",
    "                    #     else:\n",
    "                    #         img.save(img_byte_arr, format='PNG', optimize=True)\n",
    "                        \n",
    "                    #     optimized_data = img_byte_arr.getvalue()\n",
    "                    #     optimized_size_mb = len(optimized_data) / (1024 * 1024)\n",
    "                    #     print(f\"‚úÖ Ultra-compressed size: {optimized_size_mb:.2f} MB\")\n",
    "                    \n",
    "                    return optimized_data\n",
    "\n",
    "                # Standard optimization for other models\n",
    "                # Apply resize ratio if specified\n",
    "                if self.resize_ratio != 1.0:\n",
    "                    new_width = int(img.size[0] * self.resize_ratio)\n",
    "                    new_height = int(img.size[1] * self.resize_ratio)\n",
    "                    img = img.resize((new_width, new_height),\n",
    "                                     Image.Resampling.LANCZOS)\n",
    "                    print(\n",
    "                        f\"üîß Resized by ratio {self.resize_ratio}: {img.size[0]}x{img.size[1]}\")\n",
    "\n",
    "                # Additional resize if still too large\n",
    "                elif (img.size[0] > self.max_image_size[0] or\n",
    "                      img.size[1] > self.max_image_size[1] or\n",
    "                      file_size_mb > self.max_file_size_mb):\n",
    "\n",
    "                    img.thumbnail(self.max_image_size,\n",
    "                                  Image.Resampling.LANCZOS)\n",
    "                    print(\n",
    "                        f\"üîß Thumbnail resize to: {img.size[0]}x{img.size[1]}\")\n",
    "\n",
    "                # Save to bytes with optimized quality\n",
    "                img_byte_arr = io.BytesIO()\n",
    "\n",
    "                # Choose format and quality based on original\n",
    "                if Path(image_path).suffix.lower() in ['.jpg', '.jpeg']:\n",
    "                    img.save(img_byte_arr, format='JPEG',\n",
    "                             quality=self.jpeg_quality, optimize=True)\n",
    "                    print(f\"üíæ JPEG quality: {self.jpeg_quality}\")\n",
    "                else:\n",
    "                    img.save(img_byte_arr, format='PNG', optimize=True)\n",
    "                    print(f\"üíæ PNG optimized\")\n",
    "\n",
    "                optimized_data = img_byte_arr.getvalue()\n",
    "                optimized_size_mb = len(optimized_data) / (1024 * 1024)\n",
    "                print(f\"‚úÖ Final optimized size: {optimized_size_mb:.2f} MB\")\n",
    "\n",
    "                return optimized_data\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Image optimization failed, using original: {e}\")\n",
    "            # Fallback to original file\n",
    "            with open(image_path, \"rb\") as f:\n",
    "                return f.read()\n",
    "\n",
    "    def _ensure_model_initialized(self):\n",
    "        \"\"\"Ensure model is initialized before use.\"\"\"\n",
    "        if self.model is None:\n",
    "            try:\n",
    "                self.model = self._initialize_model()\n",
    "            except ValueError as e:\n",
    "                raise ValueError(\n",
    "                    f\"Cannot initialize model '{self.model_name}': {e}\")\n",
    "\n",
    "    def encode_image(self, image_path: str) -> str:\n",
    "        \"\"\"Encode image to base64 string.\"\"\"\n",
    "        with open(image_path, \"rb\") as image_file:\n",
    "            return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "    def extract_data_from_image(self, image_path: str, prompt: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Extract data from a single image using the specified prompt.\n",
    "\n",
    "        Args:\n",
    "            image_path: Path to the image file\n",
    "            prompt: Custom prompt for data extraction\n",
    "\n",
    "        Returns:\n",
    "            Dictionary containing the extracted data and metadata\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(f\"üöÄ Starting image processing...\")\n",
    "\n",
    "            # Ensure model is initialized\n",
    "            self._ensure_model_initialized()\n",
    "\n",
    "            # Validate image file\n",
    "            if not os.path.exists(image_path):\n",
    "                return {\n",
    "                    \"error\": f\"Image file not found: {image_path}\",\n",
    "                    \"success\": False\n",
    "                }\n",
    "\n",
    "            # Check file extension\n",
    "            file_ext = Path(image_path).suffix.lower()\n",
    "            if file_ext not in self.supported_formats:\n",
    "                return {\n",
    "                    \"error\": f\"Unsupported image format: {file_ext}\",\n",
    "                    \"success\": False\n",
    "                }\n",
    "\n",
    "            # Get correct MIME type\n",
    "            mime_type = self.mime_type_map.get(file_ext, file_ext[1:])\n",
    "            print(\n",
    "                f\"üîç File extension: {file_ext} ‚Üí MIME type: image/{mime_type}\")\n",
    "\n",
    "            # Optimize image\n",
    "            print(f\"üñºÔ∏è Optimizing image...\")\n",
    "            image_data = self._optimize_image(image_path)\n",
    "\n",
    "            # Encode to base64\n",
    "            print(f\"üìù Encoding to base64...\")\n",
    "            base64_image = base64.b64encode(image_data).decode()\n",
    "            base64_size_mb = len(base64_image) / (1024 * 1024)\n",
    "            print(f\"üìä Base64 size: {base64_size_mb:.2f} MB\")\n",
    "\n",
    "            # # Check if base64 is still too large for HuggingFace\n",
    "            # if self.model_name.lower() == \"huggingface\" and base64_size_mb > 0.5:\n",
    "            #     print(f\"‚ö†Ô∏è Base64 still too large for HuggingFace ({base64_size_mb:.2f} MB > 0.5 MB)\")\n",
    "            #     return {\n",
    "            #         \"error\": f\"Image too large for HuggingFace API even after optimization. Base64 size: {base64_size_mb:.2f} MB. Try using a smaller or lower quality image.\",\n",
    "            #         \"success\": False,\n",
    "            #         \"image_path\": image_path,\n",
    "            #         \"base64_size_mb\": base64_size_mb\n",
    "            #     }\n",
    "\n",
    "            # Construct data URL\n",
    "            data_url = f\"data:image/{mime_type};base64,{base64_image}\"\n",
    "\n",
    "            # Prepare message\n",
    "            print(f\"ü§ñ Sending to {self.model_name.upper()}...\")\n",
    "            message = HumanMessage(\n",
    "                content=[\n",
    "                    {\"type\": \"text\", \"text\": prompt},\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\"url\": data_url}\n",
    "                    }\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            # Get response with timeout handling\n",
    "            if self.model is not None:\n",
    "                import time\n",
    "                start_time = time.time()\n",
    "                response = self.model.invoke([message])\n",
    "                end_time = time.time()\n",
    "                print(f\"‚è±Ô∏è API call completed in {end_time - start_time:.2f} seconds\")\n",
    "            else:\n",
    "                raise ValueError(\"Model not initialized\")\n",
    "\n",
    "            result = {\n",
    "                \"image_path\": image_path,\n",
    "                \"model_used\": self.model_name,\n",
    "                \"prompt\": prompt,\n",
    "                \"extracted_data\": response.content,\n",
    "                \"success\": True,\n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "                \"processing_time_seconds\": end_time - start_time if 'end_time' in locals() else None\n",
    "            }\n",
    "            \n",
    "            # Format the result with parsed JSON data\n",
    "            return format_extraction_result(result)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error during processing: {str(e)}\")\n",
    "            return {\n",
    "                \"image_path\": image_path,\n",
    "                \"error\": str(e),\n",
    "                \"success\": False,\n",
    "                \"timestamp\": datetime.now().isoformat()\n",
    "            }\n",
    "\n",
    "    def get_images_from_directory(self, directory_path: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Get all supported image files from a directory.\n",
    "\n",
    "        Args:\n",
    "            directory_path: Path to the directory containing images\n",
    "\n",
    "        Returns:\n",
    "            List of image file paths\n",
    "        \"\"\"\n",
    "        directory = Path(directory_path)\n",
    "        if not directory.exists():\n",
    "            raise ValueError(f\"Directory not found: {directory_path}\")\n",
    "\n",
    "        image_files = []\n",
    "        for file_path in directory.iterdir():\n",
    "            if file_path.is_file() and file_path.suffix.lower() in self.supported_formats:\n",
    "                image_files.append(str(file_path))\n",
    "\n",
    "        return sorted(image_files)\n",
    "\n",
    "    def save_results(self, results: List[Dict], output_file: str):\n",
    "        \"\"\"Save results to a JSON file.\"\"\"\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    def save_results_formatted(self, results: List[Dict], output_file: str, \n",
    "                             include_parsed_data: bool = True):\n",
    "        \"\"\"\n",
    "        Save results with enhanced formatting and optional separation of parsed data.\n",
    "        \n",
    "        Args:\n",
    "            results: List of OCR results\n",
    "            output_file: Path to save the formatted results\n",
    "            include_parsed_data: Whether to include parsed JSON data separately\n",
    "        \"\"\"\n",
    "        # Create formatted results\n",
    "        formatted_results = []\n",
    "        \n",
    "        for result in results:\n",
    "            formatted_result = result.copy()\n",
    "            \n",
    "            # If we have parsed JSON data, optionally restructure\n",
    "            if include_parsed_data and \"extracted_data_parsed\" in result:\n",
    "                # Move parsed data to top level for easier access\n",
    "                formatted_result[\"structured_data\"] = result[\"extracted_data_parsed\"]\n",
    "                \n",
    "                # Keep original text for reference but make it shorter in display\n",
    "                original_text = result[\"extracted_data\"]\n",
    "                if len(original_text) > 200:\n",
    "                    formatted_result[\"extracted_data_preview\"] = original_text[:200] + \"...\"\n",
    "                    formatted_result[\"extracted_data_full\"] = original_text\n",
    "                    del formatted_result[\"extracted_data\"]\n",
    "                \n",
    "            formatted_results.append(formatted_result)\n",
    "        \n",
    "        # Save with proper formatting\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(formatted_results, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        print(f\"üìÑ Formatted results saved to: {output_file}\")\n",
    "        \n",
    "        # Also create a summary file with just the structured data\n",
    "        if include_parsed_data:\n",
    "            summary_file = output_file.replace('.json', '_structured_only.json')\n",
    "            structured_only = []\n",
    "            \n",
    "            for result in formatted_results:\n",
    "                if \"structured_data\" in result:\n",
    "                    structured_only.append({\n",
    "                        \"image_path\": result[\"image_path\"],\n",
    "                        \"model_used\": result[\"model_used\"],\n",
    "                        \"success\": result[\"success\"],\n",
    "                        \"structured_data\": result[\"structured_data\"],\n",
    "                        \"timestamp\": result[\"timestamp\"]\n",
    "                    })\n",
    "            \n",
    "            with open(summary_file, 'w', encoding='utf-8') as f:\n",
    "                json.dump(structured_only, f, indent=2, ensure_ascii=False)\n",
    "            \n",
    "            print(f\"üìã Structured data summary saved to: {summary_file}\")\n",
    "\n",
    "    def process_directory(self, directory_path: str, prompt: str,\n",
    "                          output_file: Optional[str] = None,\n",
    "                          save_formatted: bool = True) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Process all images in a directory with enhanced result formatting.\n",
    "        \"\"\"\n",
    "        image_files = self.get_images_from_directory(directory_path)\n",
    "\n",
    "        if not image_files:\n",
    "            print(f\"No supported image files found in {directory_path}\")\n",
    "            return []\n",
    "\n",
    "        print(f\"Found {len(image_files)} image(s) to process...\")\n",
    "\n",
    "        results = []\n",
    "        for i, image_path in enumerate(image_files, 1):\n",
    "            print(f\"Processing image {i}/{len(image_files)}: {Path(image_path).name}\")\n",
    "            result = self.extract_data_from_image(image_path, prompt)\n",
    "            results.append(result)\n",
    "\n",
    "            if result[\"success\"]:\n",
    "                print(f\"‚úÖ Successfully processed {Path(image_path).name}\")\n",
    "                # Show preview of structured data if available\n",
    "                if \"extracted_data_parsed\" in result:\n",
    "                    print(f\"üìä Extracted structured data format: {result.get('extraction_format', 'unknown')}\")\n",
    "            else:\n",
    "                print(f\"‚ùå Failed to process {Path(image_path).name}: {result.get('error', 'Unknown error')}\")\n",
    "\n",
    "        # Save results with enhanced formatting\n",
    "        if output_file:\n",
    "            if save_formatted:\n",
    "                self.save_results_formatted(results, output_file, include_parsed_data=True)\n",
    "            else:\n",
    "                self.save_results(results, output_file)\n",
    "            print(f\"Results saved to {output_file}\")\n",
    "\n",
    "        return results\n",
    "\n",
    "    def change_model(self, model_name: str):\n",
    "        \"\"\"Change the model being used.\"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.model = None  # Reset model\n",
    "        try:\n",
    "            self.model = self._initialize_model()\n",
    "            print(f\"‚úÖ Model changed to: {model_name}\")\n",
    "        except ValueError as e:\n",
    "            print(f\"‚ö†Ô∏è Warning: Failed to initialize {model_name}: {e}\")\n",
    "            print(f\"Model will be initialized when needed if API key becomes available.\")\n",
    "\n",
    "    def check_api_keys(self) -> Dict[str, bool]:\n",
    "        \"\"\"Check which API keys are available.\"\"\"\n",
    "        keys_status = {}\n",
    "\n",
    "        google_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "        keys_status[\"gemini\"] = bool(\n",
    "            google_key and google_key != \"your_google_api_key_here\")\n",
    "\n",
    "        openai_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "        keys_status[\"openai\"] = bool(\n",
    "            openai_key and openai_key != \"your_openai_api_key_here\")\n",
    "\n",
    "        anthropic_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "        keys_status[\"anthropic\"] = bool(\n",
    "            anthropic_key and anthropic_key != \"your_anthropic_api_key_here\")\n",
    "\n",
    "        return keys_status\n",
    "\n",
    "\n",
    "print(\"OCRBenchmark class defined successfully!\")\n",
    "print(\n",
    "    f\"üîß Global settings: Optimization={ENABLE_IMAGE_OPTIMIZATION}, Resize ratio={IMAGE_RESIZE_RATIO}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108b4eee",
   "metadata": {},
   "source": [
    "## Sample Prompts Creation\n",
    "\n",
    "This section creates various pre-defined prompts for different OCR tasks including payslip extraction, cash expense extraction, general OCR, and structured data extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66c3c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prompts created:\n",
      "\n",
      "üìù GENERAL_OCR:\n",
      "   Extract all text from this image and format it nicely. Preserve the structure and layout as much as possible....\n",
      "\n",
      "üìù PAYSLIP_EXTRACTION:\n",
      "   \n",
      "        This is a payslip/salary slip image. Please extract the following information in JSON format:\n",
      "        {\n",
      "          \"employee_name\": \"\",\n",
      "          \"employee_id\": \"\",\n",
      "          \"pay_period\": \"\",\n",
      "          \"gross_salary\": \"\",\n",
      "          \"net_salary\": \"\",\n",
      "          \"deductions\": [],\n",
      "          \"company_name\": \"\"\n",
      "        }\n",
      "        If any field is not found, use null.\n",
      "        ...\n",
      "\n",
      "üìù STRUCTURED_DATA:\n",
      "   \n",
      "        Extract all structured data from this image and organize it in a clear, readable format. \n",
      "        Identify tables, forms, or any structured information and present it clearly.\n",
      "        ...\n",
      "\n",
      "üìù KEY_VALUE_PAIRS:\n",
      "   \n",
      "        Extract all key-value pairs from this image. Present them as:\n",
      "        Key: Value\n",
      "        Key: Value\n",
      "        etc.\n",
      "        ...\n",
      "\n",
      "üìù CASH_EXPENSE_EXTRACTION:\n",
      "   \n",
      "You are an expert document processing assistant. Your task is to extract structured data from invoice/receipt images and return the information in a specific JSON format.\n",
      "\n",
      "## Instructions:\n",
      "1. Analyze the provided image carefully to identify all relevant financial information\n",
      "2. Extract the data according to the JSON schema provided below\n",
      "3. If any required field is not visible or unclear in the image, use reasonable defaults or null values\n",
      "4. For monetary amounts, use numbers without currency symbols\n",
      "5. For dates, use the format found in the document or convert to YYYY-MM-DD if possible\n",
      "6. Be precise with calculations - ensure totals match the extracted item details\n",
      "7. The document may contain text in both English and Bangla (Bengali). For Bangla text, provide English transliteration (not translation) - write the Bangla words using English letters to show how they sound. For Bangla numbers convert them to\n",
      "    English numerals (e.g., \"‡ßß‡ß®‡ß©\" becomes \"123\").\n",
      "## Required JSON Schema:\n",
      "Retur...\n",
      "\n",
      "üìù CASH_EXPENSE_EXTRACTION_V2:\n",
      "   \n",
      "You are an expert document processing assistant. Your task is to extract structured data from invoice/receipt images and return the information in a specific JSON format.\n",
      "\n",
      "## Instructions:\n",
      "1. Analyze the provided image carefully to identify all relevant financial information\n",
      "2. Extract the data according to the JSON schema provided below\n",
      "3. If any required field is not visible or unclear in the image, use reasonable defaults or null values\n",
      "4. For monetary amounts, use numbers without currency symbols\n",
      "5. For dates, use the format found in the document or convert to YYYY-MM-DD if possible\n",
      "6. Be precise with calculations - ensure totals match the extracted item details\n",
      "7. The document may contain text in both English and Bangla (Bengali). For Bangla text, provide English transliteration (not translation) - write the Bangla words using English letters to show how they sound. For Bangla numbers convert them to\n",
      "    English numerals (e.g., \"‡ßß‡ß®‡ß©\" becomes \"123\").\n",
      "## Required JSON Schema:\n",
      "Retur...\n",
      "\n",
      "üìù CASH_EXPENSE_EXTRACTION_V3:\n",
      "   \n",
      "You are an expert document processing assistant. Your task is to extract structured data from invoice/receipt images and return the information in a specific JSON format.\n",
      "\n",
      "## Instructions:\n",
      "1. Analyze the provided image carefully to identify all relevant financial information\n",
      "2. Extract the data according to the JSON schema provided below\n",
      "3. If any required field is not visible or unclear in the image, use reasonable defaults or null values\n",
      "4. For monetary amounts, use numbers without currency symbols\n",
      "5. For dates, use the format found in the document or convert to YYYY-MM-DD if possible\n",
      "6. Be precise with calculations - ensure totals match the extracted item details\n",
      "7. The document may contain text in both English and Bangla (Bengali). For Bangla text, provide English transliteration (not translation) - write the Bangla words using English letters to show how they sound. For Bangla numbers convert them to\n",
      "    English numerals (e.g., \"‡ßß‡ß®‡ß©\" becomes \"123\").  \n",
      "8. Only extract information ...\n",
      "\n",
      "üìù CASH_EXPENSE_EXTRACTION_V4:\n",
      "   \n",
      "You are an expert document processing assistant. Extract structured data from invoice/receipt images and return as JSON.\n",
      "\n",
      "CORE RULES:\n",
      "- Extract only visible information - never guess missing data\n",
      "- Use null for unclear/obscured text\n",
      "- DO NOT hallucinate or invent information not present in the image\n",
      "- If you cannot see text clearly, use null rather than guessing\n",
      "- Convert Bangla numerals to English (‡ßß‡ß®‡ß© ‚Üí 123)\n",
      "- Transliterate Bangla text to English phonetics (‡¶ü‡¶æ‡¶ï‡¶æ ‚Üí taka)\n",
      "- Verify calculations match document totals\n",
      "- Return ONLY the JSON object\n",
      "\n",
      "JSON SCHEMA:\n",
      "{\n",
      "  \"payment_details\": {\n",
      "    \"supplier\": \"string - Name of the supplier/vendor\",\n",
      "    \"payment_account\": \"string - Account used for payment\",\n",
      "    \"payment_date\": \"string - Date of payment\",\n",
      "    \"payment_method\": \"string - Mode of payment (cash, card, bank transfer, etc.)\",\n",
      "    \"ref_no\": \"string - Reference/invoice number\",\n",
      "    \"tags\": [\"array of relevant tags\"]\n",
      "  },\n",
      "  \"item_details\": [\n",
      "    {\n",
      "      \"category\": \"string - Expense cate...\n",
      "\n",
      "Ready to use! Check the next cells for usage examples.\n"
     ]
    }
   ],
   "source": [
    "# Example usage and demonstration\n",
    "def create_sample_prompts():\n",
    "    \"\"\"Create sample prompts for different OCR tasks.\"\"\"\n",
    "    return {\n",
    "        \"general_ocr\": \"Extract all text from this image and format it nicely. Preserve the structure and layout as much as possible.\",\n",
    "\n",
    "        \"payslip_extraction\": \"\"\"\n",
    "        This is a payslip/salary slip image. Please extract the following information in JSON format:\n",
    "        {\n",
    "          \"employee_name\": \"\",\n",
    "          \"employee_id\": \"\",\n",
    "          \"pay_period\": \"\",\n",
    "          \"gross_salary\": \"\",\n",
    "          \"net_salary\": \"\",\n",
    "          \"deductions\": [],\n",
    "          \"company_name\": \"\"\n",
    "        }\n",
    "        If any field is not found, use null.\n",
    "        \"\"\",\n",
    "\n",
    "        \"structured_data\": \"\"\"\n",
    "        Extract all structured data from this image and organize it in a clear, readable format. \n",
    "        Identify tables, forms, or any structured information and present it clearly.\n",
    "        \"\"\",\n",
    "\n",
    "        \"key_value_pairs\": \"\"\"\n",
    "        Extract all key-value pairs from this image. Present them as:\n",
    "        Key: Value\n",
    "        Key: Value\n",
    "        etc.\n",
    "        \"\"\",\n",
    "\n",
    "        \"cash_expense_extraction\": \"\"\"\n",
    "You are an expert document processing assistant. Your task is to extract structured data from invoice/receipt images and return the information in a specific JSON format.\n",
    "\n",
    "## Instructions:\n",
    "1. Analyze the provided image carefully to identify all relevant financial information\n",
    "2. Extract the data according to the JSON schema provided below\n",
    "3. If any required field is not visible or unclear in the image, use reasonable defaults or null values\n",
    "4. For monetary amounts, use numbers without currency symbols\n",
    "5. For dates, use the format found in the document or convert to YYYY-MM-DD if possible\n",
    "6. Be precise with calculations - ensure totals match the extracted item details\n",
    "7. The document may contain text in both English and Bangla (Bengali). For Bangla text, provide English transliteration (not translation) - write the Bangla words using English letters to show how they sound. For Bangla numbers convert them to\n",
    "    English numerals (e.g., \"‡ßß‡ß®‡ß©\" becomes \"123\").\n",
    "## Required JSON Schema:\n",
    "Return your response as a valid JSON object following this exact structure:\n",
    "\n",
    "{\n",
    "  \"payment_details\": {\n",
    "    \"supplier\": \"string - Name of the supplier/vendor\",\n",
    "    \"payment_account\": \"string - Account used for payment\",\n",
    "    \"payment_date\": \"string - Date of payment\",\n",
    "    \"payment_method\": \"string - Mode of payment (cash, card, bank transfer, etc.)\",\n",
    "    \"ref_no\": \"string - Reference/invoice number\",\n",
    "    \"tags\": [\"array of relevant tags\"]\n",
    "  },\n",
    "  \"item_details\": [\n",
    "    {\n",
    "      \"category\": \"string - Expense category\",\n",
    "      \"description\": \"string - Item description (optional)\",\n",
    "      \"quantity\": \"number - Quantity of items\",\n",
    "      \"unit_price\": \"number - Price per unit\",\n",
    "      \"total\": \"number - Total amount for this item\"\n",
    "    },\n",
    "    {\n",
    "      \"category\": \"string - Expense category for item 2\",\n",
    "      \"description\": \"string - Item description (optional)\",\n",
    "      \"quantity\": \"number - Quantity of items\",\n",
    "      \"unit_price\": \"number - Price per unit\", \n",
    "      \"total\": \"number - Total amount for this item\"\n",
    "    }\n",
    "    // ... additional items as found in the document\n",
    "  ],\n",
    "  \"attachment\": \"string - File reference if mentioned\",\n",
    "  \"memo\": \"string - Any additional notes or memo\",\n",
    "  \"totals\": {\n",
    "    \"sub_total\": \"number - Subtotal before tax and discount\",\n",
    "    \"sales_tax\": {\n",
    "      \"is_percentage\": \"boolean - true if tax is %, false if fixed amount\",\n",
    "      \"tax\": \"number - Tax rate / VAT rate (%)\",\n",
    "      \"amount\": \"number - Calculated tax amount\"\n",
    "    },\n",
    "    \"discount\": {\n",
    "      \"is_percentage\": \"boolean - true if discount is %, false if fixed amount\", \n",
    "      \"discount\": \"number - Discount rate (%)\",\n",
    "      \"calculated_amount\": \"number - Calculated discount amount\"\n",
    "    },\n",
    "    \"total_amount\": \"number - Final total amount\"\n",
    "  }\n",
    "}\n",
    "\n",
    "## Extraction Guidelines:\n",
    "- **Supplier**: Look for business name, vendor name, or \"Bill To\" information\n",
    "- **Payment Account**: Extract account numbers, card details, or payment method info\n",
    "- **Payment Date**: Find transaction date, invoice date, or payment date\n",
    "- **Payment Method**: Identify if paid by cash, card, check, bank transfer, etc.\n",
    "- **Reference Number**: Look for invoice #, receipt #, transaction ID, or reference number\n",
    "- **Tags**: Generate relevant tags based on the business type or expense category\n",
    "- **Item Details**: Extract ALL line items from the document - create a separate object for each item/product/service listed\n",
    "- **Categories**: Classify expenses (office supplies, travel, meals, equipment, etc.)\n",
    "- **Calculations**: Verify that item totals sum to subtotal, and final calculations are accurate\n",
    "- **Language Handling**: If you encounter Bangla/Bengali text, transliterate it into English letters (e.g., \"‡¶ü‡¶æ‡¶ï‡¶æ\" becomes \"taka\", \"‡¶®‡¶æ‡¶Æ\" becomes \"naam\"). Do not translate the meaning, just write how the Bangla words sound in English.\n",
    "\n",
    "## Important Notes:\n",
    "- Return ONLY the JSON object, no additional text or explanations\n",
    "- If information is missing, use null for strings and 0 for numbers\n",
    "- Ensure all numbers are numeric values, not strings\n",
    "- **Extract every single line item** - the item_details array should contain one object for each product/service listed in the document\n",
    "- Double-check mathematical accuracy of totals and ensure all item totals sum to the subtotal\n",
    "- VAT and tax should be considered same.\n",
    "- If no tax/VAT or discount is present, set the respective amounts to 0\n",
    "\n",
    "Now please analyze the provided image and extract the data according to this schema.\n",
    "        \"\"\",\n",
    "        \"payslip_extraction\": \"\"\"\n",
    "        This is a payslip/salary slip image. Please extract the following information in JSON format:\n",
    "        {\n",
    "          \"employee_name\": \"\",\n",
    "          \"employee_id\": \"\",\n",
    "          \"pay_period\": \"\",\n",
    "          \"gross_salary\": \"\",\n",
    "          \"net_salary\": \"\",\n",
    "          \"deductions\": [],\n",
    "          \"company_name\": \"\"\n",
    "        }\n",
    "        If any field is not found, use null.\n",
    "        \"\"\",\n",
    "\n",
    "        \"structured_data\": \"\"\"\n",
    "        Extract all structured data from this image and organize it in a clear, readable format. \n",
    "        Identify tables, forms, or any structured information and present it clearly.\n",
    "        \"\"\",\n",
    "\n",
    "        \"key_value_pairs\": \"\"\"\n",
    "        Extract all key-value pairs from this image. Present them as:\n",
    "        Key: Value\n",
    "        Key: Value\n",
    "        etc.\n",
    "        \"\"\",\n",
    "\n",
    "        \"cash_expense_extraction_V2\": \"\"\"\n",
    "You are an expert document processing assistant. Your task is to extract structured data from invoice/receipt images and return the information in a specific JSON format.\n",
    "\n",
    "## Instructions:\n",
    "1. Analyze the provided image carefully to identify all relevant financial information\n",
    "2. Extract the data according to the JSON schema provided below\n",
    "3. If any required field is not visible or unclear in the image, use reasonable defaults or null values\n",
    "4. For monetary amounts, use numbers without currency symbols\n",
    "5. For dates, use the format found in the document or convert to YYYY-MM-DD if possible\n",
    "6. Be precise with calculations - ensure totals match the extracted item details\n",
    "7. The document may contain text in both English and Bangla (Bengali). For Bangla text, provide English transliteration (not translation) - write the Bangla words using English letters to show how they sound. For Bangla numbers convert them to\n",
    "    English numerals (e.g., \"‡ßß‡ß®‡ß©\" becomes \"123\").\n",
    "## Required JSON Schema:\n",
    "Return your response as a valid JSON object following this EXACT structure:\n",
    "\n",
    "{\n",
    "  \"payment_details\": {\n",
    "    \"supplier\": \"string - Name of the supplier/vendor\",\n",
    "    \"payment_account\": \"string - Account used for payment\",\n",
    "    \"payment_date\": \"string - Date of payment\",\n",
    "    \"payment_method\": \"string - Mode of payment (cash, card, bank transfer, etc.)\",\n",
    "    \"ref_no\": \"string - Reference/invoice number\",\n",
    "    \"tags\": [\"array of relevant tags\"]\n",
    "  },\n",
    "  \"item_details\": [\n",
    "    {\n",
    "      \"category\": \"string - Expense category\",\n",
    "      \"description\": \"string - Item description (optional)\",\n",
    "      \"quantity\": \"number - Quantity of items\",\n",
    "      \"unit_price\": \"number - Price per unit\",\n",
    "      \"total\": \"number - Total amount for this item\"\n",
    "    },\n",
    "    {\n",
    "      \"category\": \"string - Expense category for item 2\",\n",
    "      \"description\": \"string - Item description (optional)\",\n",
    "      \"quantity\": \"number - Quantity of items\",\n",
    "      \"unit_price\": \"number - Price per unit\", \n",
    "      \"total\": \"number - Total amount for this item\"\n",
    "    }\n",
    "    // ... additional items as found in the document\n",
    "  ],\n",
    "  \"attachment\": \"string - File reference if mentioned\",\n",
    "  \"memo\": \"string - Any additional notes or memo\",\n",
    "  \"totals\": {\n",
    "    \"sub_total\": \"number - Subtotal before tax and discount\",\n",
    "    \"sales_tax\": {\n",
    "      \"is_percentage\": \"boolean - true if tax/VAT is %, false if fixed amount\",\n",
    "      \"tax\": \"number - Tax/VAT rate (%)\", \n",
    "      \"amount\": \"number - Calculated tax/VAT amount\"\n",
    "    },\n",
    "    \"discount\": {\n",
    "      \"is_percentage\": \"boolean - true if discount is %, false if fixed amount\", \n",
    "      \"discount\": \"number - Discount rate (%)\",\n",
    "      \"calculated_amount\": \"number - Calculated discount amount\"\n",
    "    },\n",
    "    \"total_amount\": \"number - Final total amount\"\n",
    "  }\n",
    "}\n",
    "\n",
    "## Extraction Guidelines:\n",
    "- **Supplier**: Look for business name, vendor name, or \"Bill To\" information\n",
    "- **Payment Account**: Extract account numbers, card details, or payment method info\n",
    "- **Payment Date**: Find transaction date, invoice date, or payment date\n",
    "- **Payment Method**: Identify if paid by cash, card, check, bank transfer, etc.\n",
    "- **Reference Number**: Look for invoice #, receipt #, transaction ID, or reference number\n",
    "- **Tags**: Generate relevant tags based on the business type or expense category\n",
    "- **Item Details**: Extract ALL line items from the document - create a separate object for each item/product/service listed\n",
    "- **Categories**: Classify expenses (office supplies, travel, meals, equipment, etc.)\n",
    "- **Calculations**: Verify that item totals sum to subtotal, and final calculations are accurate\n",
    "- **Language Handling**: If you encounter Bangla/Bengali text, transliterate it into English letters (e.g., \"‡¶ü‡¶æ‡¶ï‡¶æ\" becomes \"taka\", \"‡¶®‡¶æ‡¶Æ\" becomes \"naam\"). Do not translate the meaning, just write how the Bangla words sound in English.\n",
    "- **Tax/VAT Recognition**: Look for any tax-related terms including \"Tax\", \"VAT\", \"Sales Tax\", \"Service Tax\", \"GST\", or similar terms in both English and Bangla. Treat all of these as sales tax in the JSON structure.\n",
    "\n",
    "\n",
    "## Important Notes:\n",
    "- Return ONLY the JSON object, no additional text or explanations\n",
    "- If information is missing, use null for strings and 0 for numbers\n",
    "- Ensure all numbers are numeric values, not strings\n",
    "- **Extract every single line item** - the item_details array should contain one object for each product/service listed in the document\n",
    "- Double-check mathematical accuracy of totals and ensure all item totals sum to the subtotal\n",
    "- VAT and tax should be considered same.\n",
    "- If no tax/VAT or discount is present, set the respective amounts to 0\n",
    "\n",
    "Now please analyze the provided image and extract the data according to this schema.\n",
    "        \"\"\",\n",
    "        \n",
    "        \n",
    "        \n",
    "        \"cash_expense_extraction_V3\": \"\"\"\n",
    "You are an expert document processing assistant. Your task is to extract structured data from invoice/receipt images and return the information in a specific JSON format.\n",
    "\n",
    "## Instructions:\n",
    "1. Analyze the provided image carefully to identify all relevant financial information\n",
    "2. Extract the data according to the JSON schema provided below\n",
    "3. If any required field is not visible or unclear in the image, use reasonable defaults or null values\n",
    "4. For monetary amounts, use numbers without currency symbols\n",
    "5. For dates, use the format found in the document or convert to YYYY-MM-DD if possible\n",
    "6. Be precise with calculations - ensure totals match the extracted item details\n",
    "7. The document may contain text in both English and Bangla (Bengali). For Bangla text, provide English transliteration (not translation) - write the Bangla words using English letters to show how they sound. For Bangla numbers convert them to\n",
    "    English numerals (e.g., \"‡ßß‡ß®‡ß©\" becomes \"123\").  \n",
    "8. Only extract information that is clearly visible in the image - never guess or infer missing data\n",
    "9. If text is unclear or partially obscured, use null rather than making assumptions\n",
    "10. Verify all mathematical calculations before finalizing the JSON response\n",
    "## Required JSON Schema:\n",
    "Return your response as a valid JSON object following this exact structure:\n",
    "\n",
    "{\n",
    "  \"payment_details\": {\n",
    "    \"supplier\": \"string - Name of the supplier/vendor\",\n",
    "    \"payment_account\": \"string - Account used for payment\",\n",
    "    \"payment_date\": \"string - Date of payment\",\n",
    "    \"payment_method\": \"string - Mode of payment (cash, card, bank transfer, etc.)\",\n",
    "    \"ref_no\": \"string - Reference/invoice number\",\n",
    "    \"tags\": [\"array of relevant tags\"]\n",
    "  },\n",
    "  \"item_details\": [\n",
    "    {\n",
    "      \"category\": \"string - Expense category\",\n",
    "      \"description\": \"string - Item description (optional)\",\n",
    "      \"quantity\": \"number - Quantity of items\",\n",
    "      \"unit_price\": \"number - Price per unit\",\n",
    "      \"total\": \"number - Total amount for this item\"\n",
    "    },\n",
    "    {\n",
    "      \"category\": \"string - Expense category for item 2\",\n",
    "      \"description\": \"string - Item description (optional)\",\n",
    "      \"quantity\": \"number - Quantity of items\",\n",
    "      \"unit_price\": \"number - Price per unit\", \n",
    "      \"total\": \"number - Total amount for this item\"\n",
    "    }\n",
    "    // ... additional items as found in the document\n",
    "  ],\n",
    "  \"attachment\": \"string - File reference if mentioned\",\n",
    "  \"memo\": \"string - Any additional notes or memo\",\n",
    "  \"totals\": {\n",
    "    \"sub_total\": \"number - Subtotal before tax and discount\",\n",
    "    \"sales_tax\": {\n",
    "      \"is_percentage\": \"boolean - true if tax/VAT is %, false if fixed amount\",\n",
    "      \"tax\": \"number - Tax/VAT rate (%)\", \n",
    "      \"amount\": \"number - Calculated tax/VAT amount\"\n",
    "    },\n",
    "    \"discount\": {\n",
    "      \"is_percentage\": \"boolean - true if discount is %, false if fixed amount\", \n",
    "      \"discount\": \"number - Discount rate (%)\",\n",
    "      \"calculated_amount\": \"number - Calculated discount amount\"\n",
    "    },\n",
    "    \"total_amount\": \"number - Final total amount\"\n",
    "  }\n",
    "}\n",
    "\n",
    "## Extraction Guidelines:\n",
    "- **Supplier**: Look for business name, vendor name, or \"Bill To\" information\n",
    "- **Payment Account**: Extract account numbers, card details, or payment method info\n",
    "- **Payment Date**: Find transaction date, invoice date, or payment date\n",
    "- **Payment Method**: Identify if paid by cash, card, check, bank transfer, etc.\n",
    "- **Reference Number**: Look for invoice #, receipt #, transaction ID, or reference number\n",
    "- **Tags**: Generate relevant tags based on the business type or expense category\n",
    "- **Item Details**: Extract ALL line items from the document - create a separate object for each item/product/service listed\n",
    "- **Categories**: Classify expenses (office supplies, travel, meals, equipment, etc.)\n",
    "- **Calculations**: Verify that item totals sum to subtotal, and final calculations are accurate\n",
    "- **Language Handling**: If you encounter Bangla/Bengali text, transliterate it into English letters (e.g., \"‡¶ü‡¶æ‡¶ï‡¶æ\" becomes \"taka\", \"‡¶®‡¶æ‡¶Æ\" becomes \"naam\"). Do not translate the meaning, just write how the Bangla words sound in English.\n",
    "- **Tax/VAT Recognition**: Look for any tax-related terms including \"Tax\", \"VAT\", \"Sales Tax\", \"Service Tax\", \"GST\", or similar terms in both English and Bangla. Treat all of these as sales tax in the JSON structure.\n",
    "\n",
    "## Validation Rules:\n",
    "- **Mathematical Accuracy**: Ensure sum of all item totals equals subtotal\n",
    "- **Date Formats**: Only use dates that are clearly visible; don't assume date formats\n",
    "- **Number Precision**: Use exact numbers from the document, don't round unless specified\n",
    "- **Text Clarity**: If you cannot read text clearly due to image quality, use null\n",
    "- **Cross-Reference**: Verify supplier name appears consistently throughout the document\n",
    "- **Currency Consistency**: Ensure all amounts use the same currency basis\n",
    "\n",
    "## Important Notes:\n",
    "- Return ONLY the JSON object, no additional text or explanations\n",
    "- If information is missing, use null for strings and 0 for numbers\n",
    "- Ensure all numbers are numeric values, not strings\n",
    "- **Extract every single line item** - the item_details array should contain one object for each product/service listed in the document\n",
    "- Double-check mathematical accuracy of totals and ensure all item totals sum to the subtotal\n",
    "- VAT and tax should be considered same.\n",
    "- If no tax/VAT or discount is present, set the respective amounts to 0\n",
    "- Do not interpolate or guess missing information** - use null for unclear data\n",
    "- Double-check all calculations manually** before submitting the JSON\n",
    "- If image quality is poor and text is unreadable, explicitly use null values**\n",
    "- Verify that reference numbers and dates match standard formats for the region**\n",
    "- Cross-validate totals: item_totals ‚Üí subtotal ‚Üí (¬±tax/discount) ‚Üí final_total**\n",
    "\n",
    "## Final Check:\n",
    "Before responding, verify:\n",
    "1. All visible numbers are accurately transcribed\n",
    "2. Mathematical relationships are correct\n",
    "3. No information has been assumed or fabricated\n",
    "4. Bangla text is transliterated, not translated\n",
    "5. All required fields contain appropriate data types\n",
    "\n",
    "Now please analyze the provided image and extract the data according to this schema.\n",
    "        \"\"\",\n",
    "        \n",
    "                \"cash_expense_extraction_V4\": \"\"\"\n",
    "You are an expert document processing assistant. Extract structured data from invoice/receipt images and return as JSON.\n",
    "\n",
    "CORE RULES:\n",
    "- Extract only visible information - never guess missing data\n",
    "- Use null for unclear/obscured text\n",
    "- DO NOT hallucinate or invent information not present in the image\n",
    "- If you cannot see text clearly, use null rather than guessing\n",
    "- Convert Bangla numerals to English (‡ßß‡ß®‡ß© ‚Üí 123)\n",
    "- Transliterate Bangla text to English phonetics (‡¶ü‡¶æ‡¶ï‡¶æ ‚Üí taka)\n",
    "- Verify calculations match document totals\n",
    "- Return ONLY the JSON object\n",
    "\n",
    "JSON SCHEMA:\n",
    "{\n",
    "  \"payment_details\": {\n",
    "    \"supplier\": \"string - Name of the supplier/vendor\",\n",
    "    \"payment_account\": \"string - Account used for payment\",\n",
    "    \"payment_date\": \"string - Date of payment\",\n",
    "    \"payment_method\": \"string - Mode of payment (cash, card, bank transfer, etc.)\",\n",
    "    \"ref_no\": \"string - Reference/invoice number\",\n",
    "    \"tags\": [\"array of relevant tags\"]\n",
    "  },\n",
    "  \"item_details\": [\n",
    "    {\n",
    "      \"category\": \"string - Expense category\",\n",
    "      \"description\": \"string - Item description (optional)\",\n",
    "      \"quantity\": \"number - Quantity of items\",\n",
    "      \"unit_price\": \"number - Price per unit\",\n",
    "      \"total\": \"number - Total amount for this item\"\n",
    "    },\n",
    "    {\n",
    "      \"category\": \"string - Expense category for item 2\",\n",
    "      \"description\": \"string - Item description (optional)\",\n",
    "      \"quantity\": \"number - Quantity of items\",\n",
    "      \"unit_price\": \"number - Price per unit\", \n",
    "      \"total\": \"number - Total amount for this item\"\n",
    "    }\n",
    "    // ... additional items as found in the document\n",
    "  ],\n",
    "  \"attachment\": \"string - File reference if mentioned\",\n",
    "  \"memo\": \"string - Any additional notes or memo\",\n",
    "  \"totals\": {\n",
    "    \"sub_total\": \"number - Subtotal before tax and discount\",\n",
    "    \"sales_tax\": {\n",
    "      \"is_percentage\": \"boolean - true if tax/VAT is %, false if fixed amount\",\n",
    "      \"tax\": \"number - Tax/VAT rate (%)\", \n",
    "      \"amount\": \"number - Calculated tax/VAT amount\"\n",
    "    },\n",
    "    \"discount\": {\n",
    "      \"is_percentage\": \"boolean - true if discount is %, false if fixed amount\", \n",
    "      \"discount\": \"number - Discount rate (%)\",\n",
    "      \"calculated_amount\": \"number - Calculated discount amount\"\n",
    "    },\n",
    "    \"total_amount\": \"number - Final total amount\"\n",
    "  }\n",
    "}\n",
    "\n",
    "EXTRACTION POINTS:\n",
    "- Supplier: Business/vendor name\n",
    "- Payment: Account, date, method (cash/card/transfer)\n",
    "- Reference: Invoice/receipt/transaction number\n",
    "- Items: Extract ALL line items as separate objects\n",
    "- Categories: Classify expenses (office supplies, travel, meals, etc.)\n",
    "- Tax/VAT: Include all tax variants (GST, Service Tax, etc.)\n",
    "- Calculations: Verify item totals ‚Üí subtotal ‚Üí final total\n",
    "\n",
    "VALIDATION:\n",
    "- Sum of item totals = subtotal\n",
    "- Use exact numbers from document\n",
    "- Use null for missing/unclear data\n",
    "- Use 0 for absent tax/discount amounts\n",
    "\n",
    "Analyze the image and extract data according to this schema.\n",
    "\"\"\"\n",
    "    }\n",
    "\n",
    "\n",
    "# Create sample prompts\n",
    "sample_prompts = create_sample_prompts()\n",
    "\n",
    "print(\"Sample prompts created:\")\n",
    "for name, prompt in sample_prompts.items():\n",
    "    print(f\"\\nüìù {name.upper()}:\")\n",
    "    print(f\"   {prompt[:1000]}...\")\n",
    "\n",
    "print(\"\\nReady to use! Check the next cells for usage examples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2599040",
   "metadata": {},
   "source": [
    "## OCR System Initialization\n",
    "\n",
    "Initialize the OCR Benchmark system with the chosen AI model (default: Gemini) and verify image directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "004fd257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ OCR Benchmark system initialized successfully with Hugging Face!\n",
      "Current model: huggingface\n",
      "Found 10 image(s) in Pay Slip directory\n",
      "Sample images:\n",
      "  1. IMG_20250826_154715.jpg\n",
      "  2. IMG_20250826_154929.jpg\n",
      "  3. IMG_20250826_155237.jpg\n",
      "  4. IMG_20250826_155355.jpg\n",
      "  5. IMG_20250826_155901.jpg\n",
      "  6. IMG_20250826_160504.jpg\n",
      "  7. IMG_20250826_160529.jpg\n",
      "  8. IMG_20250826_160544.jpg\n",
      "  9. IMG_20250826_160721.jpg\n",
      "  10. IMG_20250826_160928.jpg\n"
     ]
    }
   ],
   "source": [
    "# Initialize the OCR Benchmark system\n",
    "# By default, it uses Gemini model\n",
    "\n",
    "try:\n",
    "    # Initialize Default\n",
    "    ocr = OCRBenchmark(model_name=\"huggingface\", temperature=0.1)\n",
    "    print(\"‚úÖ OCR Benchmark system initialized successfully with Hugging Face!\")\n",
    "    print(f\"Current model: {ocr.model_name}\")\n",
    "    \n",
    "    # Check if we have images to process\n",
    "    pay_slip_dir = \"/home/tanjim_noor/Work/AI OCR BenchMark/Good Pictures\"\n",
    "    if os.path.exists(pay_slip_dir):\n",
    "        image_files = ocr.get_images_from_directory(pay_slip_dir)\n",
    "        print(f\"Found {len(image_files)} image(s) in Pay Slip directory\")\n",
    "        if image_files:\n",
    "            print(\"Sample images:\")\n",
    "            for i, img in enumerate(image_files):\n",
    "                print(f\"  {i+1}. {Path(img).name}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error initializing OCR system: {e}\")\n",
    "    print(\"Please make sure to set your API keys in the .env file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f88260d",
   "metadata": {},
   "source": [
    "## Single Image Processing Example\n",
    "\n",
    "Demonstrate processing a single image from the directory using the cash expense extraction prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0daa950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: IMG_20250826_154715.jpg\n",
      "Using cash OCR prompt...\n",
      "üöÄ Starting image processing...\n",
      "üîç File extension: .jpg ‚Üí MIME type: image/jpeg\n",
      "üñºÔ∏è Optimizing image...\n",
      "üìè Original file size: 4.31 MB\n",
      "üìê Original dimensions: 4624x3472\n",
      "üîß HuggingFace aggressive resize by ratio 0.5: 2312x1736\n",
      "üíæ HuggingFace JPEG quality: 50\n",
      "‚úÖ HuggingFace optimized size: 0.35 MB\n",
      "üìù Encoding to base64...\n",
      "üìä Base64 size: 0.46 MB\n",
      "ü§ñ Sending to HUGGINGFACE...\n",
      "‚è±Ô∏è API call completed in 14.53 seconds\n",
      "\n",
      "‚úÖ SUCCESS!\n",
      "Model used: huggingface\n",
      "Extracted data:\n",
      "```json\n",
      "{\n",
      "  \"payment_details\": {\n",
      "    \"supplier\": \"ACI Logistics Limited\",\n",
      "    \"payment_account\": null,\n",
      "    \"payment_date\": \"26/06/2025\",\n",
      "    \"payment_method\": \"City Bank\",\n",
      "    \"ref_no\": \"L54008\",\n",
      "    \"tags\": [\"retail\", \"grocery\", \"food\"]\n",
      "  },\n",
      "  \"item_details\": [\n",
      "    {\n",
      "      \"category\": \"grocery\",\n",
      "      \"description\": \"Fresh Soyabean Oil 5ltr\",\n",
      "      \"quantity\": 1.00,\n",
      "      \"unit_price\": 922.00,\n",
      "      \"total\": 922.00\n",
      "    },\n",
      "    {\n",
      "      \"category\": \"grocery\",\n",
      "      \"description\": \"ULTRA Sweet Curd 500g\",\n",
      "      \"quantity\": 1.00,\n",
      "      \"unit_price\": 150.00,\n",
      "      \"total\": 150.00\n",
      "    }\n",
      "  ],\n",
      "  \"attachment\": null,\n",
      "  \"memo\": null,\n",
      "  \"totals\": {\n",
      "    \"sub_total\": 1072.00,\n",
      "    \"sales_tax\": {\n",
      "      \"is_percentage\": false,\n",
      "      \"tax\": 0,\n",
      "      \"amount\": 0.00\n",
      "    },\n",
      "    \"discount\": {\n",
      "      \"is_percentage\": false,\n",
      "      \"discount\": 0,\n",
      "      \"calculated_amount\": 0.00\n",
      "    },\n",
      "    \"total_amount\": 1072.00\n",
      "  }\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# Example 1: Process a single image\n",
    "# Let's try to process one image from the Pay Slip directory\n",
    "\n",
    "try:\n",
    "    pay_slip_dir = \"/home/tanjim_noor/Work/AI OCR BenchMark/Good Pictures\"\n",
    "    if os.path.exists(pay_slip_dir):\n",
    "        image_files = ocr.get_images_from_directory(pay_slip_dir)\n",
    "        \n",
    "        if image_files:\n",
    "            # Process the first image with payslip extraction prompt\n",
    "            first_image = image_files[0]\n",
    "            print(f\"Processing: {Path(first_image).name}\")\n",
    "            print(\"Using cash OCR prompt...\")\n",
    "\n",
    "            result = ocr.extract_data_from_image(first_image, sample_prompts[\"cash_expense_extraction_V2\"])\n",
    "            \n",
    "            if result[\"success\"]:\n",
    "                print(\"\\n‚úÖ SUCCESS!\")\n",
    "                print(f\"Model used: {result['model_used']}\")\n",
    "                print(f\"Extracted data:\\n{result['extracted_data']}\")\n",
    "            else:\n",
    "                print(f\"\\n‚ùå FAILED: {result.get('error', 'Unknown error')}\")\n",
    "        else:\n",
    "            print(\"No images found in the directory\")\n",
    "    else:\n",
    "        print(\"Pay Slip directory not found\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"Make sure your API key is set correctly in the .env file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3b287d",
   "metadata": {},
   "source": [
    "## Batch Processing - Process All Images\n",
    "\n",
    "This section demonstrates batch processing of all images in a directory, saving results to JSON files with enhanced formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e460ee7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting batch processing...\n",
      "Input directory: /home/tanjim_noor/Work/AI OCR BenchMark/For benchmark\n",
      "Output file: /home/tanjim_noor/Work/AI OCR BenchMark/OCR_Output/ocr_result_qwen_1.json\n",
      "==================================================\n",
      "Found 10 image(s) to process...\n",
      "Processing image 1/10: IMG_20250826_154308~2.jpg\n",
      "üöÄ Starting image processing...\n",
      "üîç File extension: .jpg ‚Üí MIME type: image/jpeg\n",
      "üñºÔ∏è Optimizing image...\n",
      "üìè Original file size: 1.33 MB\n",
      "üìê Original dimensions: 3472x4624\n",
      "üîß HuggingFace aggressive resize by ratio 0.5: 1736x2312\n",
      "üíæ HuggingFace JPEG quality: 50\n",
      "‚úÖ HuggingFace optimized size: 0.17 MB\n",
      "üìù Encoding to base64...\n",
      "üìä Base64 size: 0.23 MB\n",
      "ü§ñ Sending to HUGGINGFACE...\n",
      "‚è±Ô∏è API call completed in 11.79 seconds\n",
      "‚úÖ Successfully processed IMG_20250826_154308~2.jpg\n",
      "üìä Extracted structured data format: json\n",
      "Processing image 2/10: IMG_20250826_154831.jpg\n",
      "üöÄ Starting image processing...\n",
      "üîç File extension: .jpg ‚Üí MIME type: image/jpeg\n",
      "üñºÔ∏è Optimizing image...\n",
      "üìè Original file size: 4.62 MB\n",
      "üìê Original dimensions: 4624x3472\n",
      "üîß HuggingFace aggressive resize by ratio 0.5: 2312x1736\n",
      "üíæ HuggingFace JPEG quality: 50\n",
      "‚úÖ HuggingFace optimized size: 0.20 MB\n",
      "üìù Encoding to base64...\n",
      "üìä Base64 size: 0.26 MB\n",
      "ü§ñ Sending to HUGGINGFACE...\n",
      "‚è±Ô∏è API call completed in 15.51 seconds\n",
      "‚úÖ Successfully processed IMG_20250826_154831.jpg\n",
      "üìä Extracted structured data format: json\n",
      "Processing image 3/10: IMG_20250826_154929.jpg\n",
      "üöÄ Starting image processing...\n",
      "üîç File extension: .jpg ‚Üí MIME type: image/jpeg\n",
      "üñºÔ∏è Optimizing image...\n",
      "üìè Original file size: 4.69 MB\n",
      "üìê Original dimensions: 4624x3472\n",
      "üîß HuggingFace aggressive resize by ratio 0.5: 2312x1736\n",
      "üíæ HuggingFace JPEG quality: 50\n",
      "‚úÖ HuggingFace optimized size: 0.40 MB\n",
      "üìù Encoding to base64...\n",
      "üìä Base64 size: 0.53 MB\n",
      "ü§ñ Sending to HUGGINGFACE...\n",
      "‚ùå Error during processing: Error code: 413 - {'error': 'request entity too large'}\n",
      "‚ùå Failed to process IMG_20250826_154929.jpg: Error code: 413 - {'error': 'request entity too large'}\n",
      "Processing image 4/10: IMG_20250826_155237.jpg\n",
      "üöÄ Starting image processing...\n",
      "üîç File extension: .jpg ‚Üí MIME type: image/jpeg\n",
      "üñºÔ∏è Optimizing image...\n",
      "üìè Original file size: 4.21 MB\n",
      "üìê Original dimensions: 4624x3472\n",
      "üîß HuggingFace aggressive resize by ratio 0.5: 2312x1736\n",
      "üíæ HuggingFace JPEG quality: 50\n",
      "‚úÖ HuggingFace optimized size: 0.32 MB\n",
      "üìù Encoding to base64...\n",
      "üìä Base64 size: 0.42 MB\n",
      "ü§ñ Sending to HUGGINGFACE...\n",
      "‚è±Ô∏è API call completed in 51.67 seconds\n",
      "‚úÖ Successfully processed IMG_20250826_155237.jpg\n",
      "üìä Extracted structured data format: json\n",
      "Processing image 5/10: IMG_20250826_155754.jpg\n",
      "üöÄ Starting image processing...\n",
      "üîç File extension: .jpg ‚Üí MIME type: image/jpeg\n",
      "üñºÔ∏è Optimizing image...\n",
      "üìè Original file size: 3.26 MB\n",
      "üìê Original dimensions: 4624x3472\n",
      "üîß HuggingFace aggressive resize by ratio 0.5: 2312x1736\n",
      "üíæ HuggingFace JPEG quality: 50\n",
      "‚úÖ HuggingFace optimized size: 0.19 MB\n",
      "üìù Encoding to base64...\n",
      "üìä Base64 size: 0.25 MB\n",
      "ü§ñ Sending to HUGGINGFACE...\n",
      "‚è±Ô∏è API call completed in 15.00 seconds\n",
      "‚úÖ Successfully processed IMG_20250826_155754.jpg\n",
      "üìä Extracted structured data format: json\n",
      "Processing image 6/10: IMG_20250826_160041.jpg\n",
      "üöÄ Starting image processing...\n",
      "üîç File extension: .jpg ‚Üí MIME type: image/jpeg\n",
      "üñºÔ∏è Optimizing image...\n",
      "üìè Original file size: 2.93 MB\n",
      "üìê Original dimensions: 4624x3472\n",
      "üîß HuggingFace aggressive resize by ratio 0.5: 2312x1736\n",
      "üíæ HuggingFace JPEG quality: 50\n",
      "‚úÖ HuggingFace optimized size: 0.18 MB\n",
      "üìù Encoding to base64...\n",
      "üìä Base64 size: 0.25 MB\n",
      "ü§ñ Sending to HUGGINGFACE...\n",
      "‚è±Ô∏è API call completed in 15.53 seconds\n",
      "‚úÖ Successfully processed IMG_20250826_160041.jpg\n",
      "üìä Extracted structured data format: json\n",
      "Processing image 7/10: IMG_20250826_160529.jpg\n",
      "üöÄ Starting image processing...\n",
      "üîç File extension: .jpg ‚Üí MIME type: image/jpeg\n",
      "üñºÔ∏è Optimizing image...\n",
      "üìè Original file size: 4.22 MB\n",
      "üìê Original dimensions: 4624x3472\n",
      "üîß HuggingFace aggressive resize by ratio 0.5: 2312x1736\n",
      "üíæ HuggingFace JPEG quality: 50\n",
      "‚úÖ HuggingFace optimized size: 0.32 MB\n",
      "üìù Encoding to base64...\n",
      "üìä Base64 size: 0.43 MB\n",
      "ü§ñ Sending to HUGGINGFACE...\n",
      "‚è±Ô∏è API call completed in 16.03 seconds\n",
      "‚úÖ Successfully processed IMG_20250826_160529.jpg\n",
      "üìä Extracted structured data format: json\n",
      "Processing image 8/10: IMG_20250826_160644.jpg\n",
      "üöÄ Starting image processing...\n",
      "üîç File extension: .jpg ‚Üí MIME type: image/jpeg\n",
      "üñºÔ∏è Optimizing image...\n",
      "üìè Original file size: 4.54 MB\n",
      "üìê Original dimensions: 4624x3472\n",
      "üîß HuggingFace aggressive resize by ratio 0.5: 2312x1736\n",
      "üíæ HuggingFace JPEG quality: 50\n",
      "‚úÖ HuggingFace optimized size: 0.22 MB\n",
      "üìù Encoding to base64...\n",
      "üìä Base64 size: 0.30 MB\n",
      "ü§ñ Sending to HUGGINGFACE...\n",
      "‚è±Ô∏è API call completed in 10.81 seconds\n",
      "‚úÖ Successfully processed IMG_20250826_160644.jpg\n",
      "üìä Extracted structured data format: json\n",
      "Processing image 9/10: IMG_20250826_160928.jpg\n",
      "üöÄ Starting image processing...\n",
      "üîç File extension: .jpg ‚Üí MIME type: image/jpeg\n",
      "üñºÔ∏è Optimizing image...\n",
      "üìè Original file size: 5.80 MB\n",
      "üìê Original dimensions: 4624x3472\n",
      "üîß HuggingFace aggressive resize by ratio 0.5: 2312x1736\n",
      "üíæ HuggingFace JPEG quality: 50\n",
      "‚úÖ HuggingFace optimized size: 0.33 MB\n",
      "üìù Encoding to base64...\n",
      "üìä Base64 size: 0.44 MB\n",
      "ü§ñ Sending to HUGGINGFACE...\n",
      "‚è±Ô∏è API call completed in 39.28 seconds\n",
      "‚úÖ Successfully processed IMG_20250826_160928.jpg\n",
      "üìä Extracted structured data format: json\n",
      "Processing image 10/10: IMG_20250826_160949.jpg\n",
      "üöÄ Starting image processing...\n",
      "üîç File extension: .jpg ‚Üí MIME type: image/jpeg\n",
      "üñºÔ∏è Optimizing image...\n",
      "üìè Original file size: 4.73 MB\n",
      "üìê Original dimensions: 4624x3472\n",
      "üîß HuggingFace aggressive resize by ratio 0.5: 2312x1736\n",
      "üíæ HuggingFace JPEG quality: 50\n",
      "‚úÖ HuggingFace optimized size: 0.24 MB\n",
      "üìù Encoding to base64...\n",
      "üìä Base64 size: 0.32 MB\n",
      "ü§ñ Sending to HUGGINGFACE...\n",
      "‚è±Ô∏è API call completed in 16.01 seconds\n",
      "‚úÖ Successfully processed IMG_20250826_160949.jpg\n",
      "üìä Extracted structured data format: json\n",
      "üìÑ Formatted results saved to: /home/tanjim_noor/Work/AI OCR BenchMark/OCR_Output/ocr_result_qwen_1.json\n",
      "üìã Structured data summary saved to: /home/tanjim_noor/Work/AI OCR BenchMark/OCR_Output/ocr_result_qwen_1_structured_only.json\n",
      "Results saved to /home/tanjim_noor/Work/AI OCR BenchMark/OCR_Output/ocr_result_qwen_1.json\n",
      "==================================================\n",
      "üìä PROCESSING SUMMARY:\n",
      "Total images: 10\n",
      "Successful: 9\n",
      "Failed: 1\n",
      "Results saved to: /home/tanjim_noor/Work/AI OCR BenchMark/OCR_Output/ocr_result_qwen_1.json\n",
      "Batch processing code is commented out.\n"
     ]
    }
   ],
   "source": [
    "# Example 2: Process all images in directory (Batch Processing)\n",
    "# This will process all images in the Pay Slip directory\n",
    "\n",
    "# Uncomment the following code to process all images:\n",
    "\n",
    "try:\n",
    "    pay_slip_dir = \"/home/tanjim_noor/Work/AI OCR BenchMark/For benchmark\"\n",
    "    output_file = \"/home/tanjim_noor/Work/AI OCR BenchMark/OCR_Output/ocr_result_qwen_1.json\"\n",
    "    \n",
    "    print(\"Starting batch processing...\")\n",
    "    print(f\"Input directory: {pay_slip_dir}\")\n",
    "    print(f\"Output file: {output_file}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Process all images with payslip extraction prompt\n",
    "    results = ocr.process_directory(\n",
    "        directory_path=pay_slip_dir,\n",
    "        prompt=sample_prompts[\"cash_expense_extraction_V2\"],\n",
    "        output_file=output_file\n",
    "    )\n",
    "    \n",
    "    # Summary\n",
    "    successful = sum(1 for r in results if r[\"success\"])\n",
    "    failed = len(results) - successful\n",
    "    \n",
    "    print(\"=\" * 50)\n",
    "    print(f\"üìä PROCESSING SUMMARY:\")\n",
    "    print(f\"Total images: {len(results)}\")\n",
    "    print(f\"Successful: {successful}\")\n",
    "    print(f\"Failed: {failed}\")\n",
    "    print(f\"Results saved to: {output_file}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error during batch processing: {e}\")\n",
    "\n",
    "\n",
    "print(\"Batch processing code is commented out.\")\n",
    "#print(\"Uncomment the code above to process all images in the directory.\")\n",
    "#print(\"Note: This will use API calls for each image, so be mindful of costs!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58dce0e5",
   "metadata": {},
   "source": [
    "## Test Data Samples\n",
    "\n",
    "Sample JSON data structures for testing and validation purposes, including both predicted output and actual ground truth data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbedb491",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_json = {\n",
    "    \"payment_details\": {\n",
    "        \"supplier\": \"Maa Fruits & Departmental Store\",\n",
    "        \"payment_account\": None,\n",
    "        \"payment_date\": \"2020-06-20\",\n",
    "        \"payment_method\": \"Cash\",\n",
    "        \"ref_no\": None,\n",
    "        \"tags\": [\n",
    "            \"Fruits\",\n",
    "            \"Departmental Store\",\n",
    "            \"Retail\",\n",
    "            \"Wholesale\"\n",
    "        ]\n",
    "    },\n",
    "    \"item_details\": [\n",
    "        {\n",
    "            \"category\": \"Fruits\",\n",
    "            \"description\": \"Tomatoes\",\n",
    "            \"quantity\": 80,\n",
    "            \"unit_price\": 7.5,\n",
    "            \"total\": 600\n",
    "        }\n",
    "    ],\n",
    "    \"attachment\": None,\n",
    "    \"memo\": \"Soles goods are not returnable. Welcome come again.\",\n",
    "    \"totals\": {\n",
    "        \"sub_total\": 600,\n",
    "        \"sales_tax\": {\n",
    "            \"is_percentage\": False,\n",
    "            \"tax\": 0,\n",
    "            \"amount\": 0\n",
    "        },\n",
    "        \"discount\": {\n",
    "            \"is_percentage\": False,\n",
    "            \"discount\": 0,\n",
    "            \"calculated_amount\": 0\n",
    "        },\n",
    "        \"total_amount\": 600\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "actual_json = {\n",
    "    \"payment_details\": {\n",
    "        \"supplier\": \"Maa Fruits & Departmental Store\",\n",
    "        \"payment_account\": None,\n",
    "        \"payment_date\": \"2020-06-20\",\n",
    "        \"payment_method\": \"Cash\",\n",
    "        \"ref_no\": None,\n",
    "        \"tags\": [\n",
    "            \"Fruits\",\n",
    "            \"Departmental Store\",\n",
    "            \"Retail\",\n",
    "            \"Wholesale\"\n",
    "        ]\n",
    "    },\n",
    "    \"item_details\": [\n",
    "        {\n",
    "            \"category\": \"Fruits\",\n",
    "            \"description\": \"Tomatoes\",\n",
    "            \"quantity\": 80,\n",
    "            \"unit_price\": 10,\n",
    "            \"total\": 800\n",
    "        }\n",
    "    ],\n",
    "    \"attachment\": None,\n",
    "    \"memo\": \"Soled goods are not returnable. Welcome come again.\",\n",
    "    \"totals\": {\n",
    "        \"sub_total\": 800,\n",
    "        \"sales_tax\": {\n",
    "            \"is_percentage\": False,\n",
    "            \"tax\": 0,\n",
    "            \"amount\": 0\n",
    "        },\n",
    "        \"discount\": {\n",
    "            \"is_percentage\": False,\n",
    "            \"discount\": 0,\n",
    "            \"calculated_amount\": 0\n",
    "        },\n",
    "        \"total_amount\": 800\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "output_json_2 = {\n",
    "  \"payment_details\": {\n",
    "    \"supplier\": \"SHWAPNO (ACI Logistics Limited)\",\n",
    "    \"payment_account\": \"City Bank\",\n",
    "    \"payment_date\": \"2025-06-26\",\n",
    "    \"payment_method\": \"Bank Transfer/Card (implied by City Bank)\",\n",
    "    \"ref_no\": \"D0612506260068\",\n",
    "    \"tags\": [\"Groceries\", \"Supermarket\", \"Retail\", \"Food\"]\n",
    "  },\n",
    "  \"item_details\": [\n",
    "    {\n",
    "      \"category\": \"Groceries\",\n",
    "      \"description\": \"Beef Premium Cube kg\",\n",
    "      \"quantity\": 6.33,\n",
    "      \"unit_price\": 775.00,\n",
    "      \"total\": 4901.88\n",
    "    },\n",
    "    {\n",
    "      \"category\": \"Groceries\",\n",
    "      \"description\": \"Broiler Chicken Breast Bon\",\n",
    "      \"quantity\": 0.50,\n",
    "      \"unit_price\": 534.00,\n",
    "      \"total\": 267.00\n",
    "    },\n",
    "    {\n",
    "      \"category\": \"Groceries\",\n",
    "      \"description\": \"Capsicum Green kg\",\n",
    "      \"quantity\": 1.01,\n",
    "      \"unit_price\": 320.00,\n",
    "      \"total\": 321.60\n",
    "    },\n",
    "    {\n",
    "      \"category\": \"Groceries\",\n",
    "      \"description\": \"Carrot China (China Gajor) K\",\n",
    "      \"quantity\": 1.40,\n",
    "      \"unit_price\": 160.00,\n",
    "      \"total\": 224.00\n",
    "    },\n",
    "    {\n",
    "      \"category\": \"Groceries\",\n",
    "      \"description\": \"Cucumber (Shosha) kg\",\n",
    "      \"quantity\": 2.69,\n",
    "      \"unit_price\": 55.00,\n",
    "      \"total\": 147.68\n",
    "    },\n",
    "    {\n",
    "      \"category\": \"Groceries\",\n",
    "      \"description\": \"Green Chili (Kacha Morich)\",\n",
    "      \"quantity\": 0.67,\n",
    "      \"unit_price\": 100.00,\n",
    "      \"total\": 66.50\n",
    "    },\n",
    "    {\n",
    "      \"category\": \"Groceries\",\n",
    "      \"description\": \"Green Papaya (Kacha Pepe)\",\n",
    "      \"quantity\": 3.00,\n",
    "      \"unit_price\": 45.00,\n",
    "      \"total\": 134.78\n",
    "    },\n",
    "    {\n",
    "      \"category\": \"Groceries\",\n",
    "      \"description\": \"Lemon Long (Lomba Lebu) PC\",\n",
    "      \"quantity\": 8.00,\n",
    "      \"unit_price\": 6.00,\n",
    "      \"total\": 48.00\n",
    "    },\n",
    "    {\n",
    "      \"category\": \"Groceries\",\n",
    "      \"description\": \"Long Bean (Boroboti) KG\",\n",
    "      \"quantity\": 1.83,\n",
    "      \"unit_price\": 90.00,\n",
    "      \"total\": 164.70\n",
    "    },\n",
    "    {\n",
    "      \"category\": \"Groceries\",\n",
    "      \"description\": \"Piyaj Deshi Loose kg\",\n",
    "      \"quantity\": 2.66,\n",
    "      \"unit_price\": 58.00,\n",
    "      \"total\": 154.28\n",
    "    },\n",
    "    {\n",
    "      \"category\": \"Groceries\",\n",
    "      \"description\": \"Shonalika Dressed Classic\",\n",
    "      \"quantity\": 1.65,\n",
    "      \"unit_price\": 667.00,\n",
    "      \"total\": 1097.22\n",
    "    },\n",
    "    {\n",
    "      \"category\": \"Groceries\",\n",
    "      \"description\": \"ACI Pure Corn Flour 150g\",\n",
    "      \"quantity\": 1.00,\n",
    "      \"unit_price\": 60.00,\n",
    "      \"total\": 60.00\n",
    "    },\n",
    "    {\n",
    "      \"category\": \"Groceries\",\n",
    "      \"description\": \"Shwapno Black Pepper Powde\",\n",
    "      \"quantity\": 1.00,\n",
    "      \"unit_price\": 129.00,\n",
    "      \"total\": 129.00\n",
    "    }\n",
    "  ],\n",
    "  \"attachment\": False,\n",
    "  \"memo\": \"Thank you for shopping with SHWAPNO. Please visit www.shwapno.com for home delivery. Purchase of defected item must be exchanged by 24 hours with invoice. For any queries, suggestions or complaints, please call 16469 (9:00 AM - 6:00 PM). Earned points will expire within 6 months from the date of the transaction if not redeemed. VAT against this challan is payable through central registration. Prices inclusive of standard VAT except exempted items, VAT Payable TK. : 7.17.\",\n",
    "  \"totals\": {\n",
    "    \"sub_total\": 7716.64,\n",
    "    \"sales_tax\": {\n",
    "      \"is_percentage\": False,\n",
    "      \"tax\": 0,\n",
    "      \"amount\": 0\n",
    "    },\n",
    "    \"discount\": {\n",
    "      \"is_percentage\": False,\n",
    "      \"discount\": 0,\n",
    "      \"calculated_amount\": 0\n",
    "    },\n",
    "    \"total_amount\": 7716.64\n",
    "  }\n",
    "}\n",
    "\n",
    "actual_json_2 = {\"payment_details\": {\n",
    "    \"supplier\": \"SHWAPNO (ACI Logistics Limited)\",\n",
    "    \"payment_account\": \"City Bank\",\n",
    "    \"payment_date\": \"2025-06-26\",\n",
    "    \"payment_method\": \"Bank Transfer/Card (implied by City Bank)\",\n",
    "    \"ref_no\": \"D0612506260068\",\n",
    "    \"tags\": [\"Groceries\", \"Supermarket\", \"Retail\", \"Food\"]\n",
    "  },\n",
    "  \"item_details\": [\n",
    "    {\n",
    "      \"category\": \"Groceries\",\n",
    "      \"description\": \"Beef Premium Cube kg\",\n",
    "      \"quantity\": 6.33,\n",
    "      \"unit_price\": 775.00,\n",
    "      \"total\": 4901.88\n",
    "    },\n",
    "    {\n",
    "      \"category\": \"Groceries\",\n",
    "      \"description\": \"Broiler Chicken Breast Bon\",\n",
    "      \"quantity\": 0.50,\n",
    "      \"unit_price\": 534.00,\n",
    "      \"total\": 267.00\n",
    "    },\n",
    "    {\n",
    "      \"category\": \"Groceries\",\n",
    "      \"description\": \"Capsicum Green kg\",\n",
    "      \"quantity\": 1.01,\n",
    "      \"unit_price\": 320.00,\n",
    "      \"total\": 321.60\n",
    "    },\n",
    "    {\n",
    "      \"category\": \"Groceries\",\n",
    "      \"description\": \"Carrot China (China Gajor) K\",\n",
    "      \"quantity\": 1.40,\n",
    "      \"unit_price\": 160.00,\n",
    "      \"total\": 224.00\n",
    "    },\n",
    "    {\n",
    "      \"category\": \"Groceries\",\n",
    "      \"description\": \"Cucumber (Shosha) kg\",\n",
    "      \"quantity\": 2.69,\n",
    "      \"unit_price\": 55.00,\n",
    "      \"total\": 147.68\n",
    "    },\n",
    "    {\n",
    "      \"category\": \"Groceries\",\n",
    "      \"description\": \"Green Chili (Kacha Morich)\",\n",
    "      \"quantity\": 0.67,\n",
    "      \"unit_price\": 100.00,\n",
    "      \"total\": 66.50\n",
    "    },\n",
    "    {\n",
    "      \"category\": \"Groceries\",\n",
    "      \"description\": \"Green Papaya (Kacha Pepe)\",\n",
    "      \"quantity\": 3.00,\n",
    "      \"unit_price\": 45.00,\n",
    "      \"total\": 134.78\n",
    "    },\n",
    "    {\n",
    "      \"category\": \"Groceries\",\n",
    "      \"description\": \"Lemon Long (Lomba Lebu) PC\",\n",
    "      \"quantity\": 8.00,\n",
    "      \"unit_price\": 6.00,\n",
    "      \"total\": 48.00\n",
    "    },\n",
    "    {\n",
    "      \"category\": \"Groceries\",\n",
    "      \"description\": \"Long Bean (Boroboti) KG\",\n",
    "      \"quantity\": 1.83,\n",
    "      \"unit_price\": 90.00,\n",
    "      \"total\": 164.70\n",
    "    },\n",
    "    {\n",
    "      \"category\": \"Groceries\",\n",
    "      \"description\": \"Piyaj Deshi Loose kg\",\n",
    "      \"quantity\": 2.66,\n",
    "      \"unit_price\": 58.00,\n",
    "      \"total\": 154.28\n",
    "    },\n",
    "    {\n",
    "      \"category\": \"Groceries\",\n",
    "      \"description\": \"Shonalika Dressed Classic\",\n",
    "      \"quantity\": 1.65,\n",
    "      \"unit_price\": 667.00,\n",
    "      \"total\": 1097.22\n",
    "    },\n",
    "    {\n",
    "      \"category\": \"Groceries\",\n",
    "      \"description\": \"ACI Pure Corn Flour 150g\",\n",
    "      \"quantity\": 1.00,\n",
    "      \"unit_price\": 60.00,\n",
    "      \"total\": 60.00\n",
    "    },\n",
    "    {\n",
    "      \"category\": \"Groceries\",\n",
    "      \"description\": \"Shwapno Black Pepper Powde\",\n",
    "      \"quantity\": 1.00,\n",
    "      \"unit_price\": 129.00,\n",
    "      \"total\": 129.00\n",
    "    }\n",
    "  ],\n",
    "  \"attachment\": False,\n",
    "  \"memo\": \"Thank you for shopping with SHWAPNO. Please visit www.shwapno.com for home delivery. Purchase of defected item must be exchanged by 24 hours with invoice. For any queries, suggestions or complaints, please call 16469 (9:00 AM - 6:00 PM). Earned points will expire within 6 months from the date of the transaction if not redeemed. VAT against this challan is payable through central registration. Prices inclusive of standard VAT except exempted items, VAT Payable TK. : 7.17.\",\n",
    "  \"totals\": {\n",
    "    \"sub_total\": 7716.64,\n",
    "    \"sales_tax\": {\n",
    "      \"is_percentage\": False,\n",
    "      \"tax\": 0,\n",
    "      \"amount\": 7.17\n",
    "    },\n",
    "    \"discount\": {\n",
    "      \"is_percentage\": False,\n",
    "      \"discount\": 0,\n",
    "      \"calculated_amount\": 30.80\n",
    "    },\n",
    "    \"total_amount\": 7686.00\n",
    "  }\n",
    "}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1aab54",
   "metadata": {},
   "source": [
    "## Install Additional Dependencies\n",
    "\n",
    "Install sentence-transformers library for semantic similarity scoring in the benchmark system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ec01ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence-transformers\n",
      "  Downloading sentence_transformers-5.1.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting transformers<5.0.0,>=4.41.0 (from sentence-transformers)\n",
      "  Downloading transformers-4.55.4-py3-none-any.whl.metadata (41 kB)\n",
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.12/site-packages (from sentence-transformers) (4.67.1)\n",
      "Collecting torch>=1.11.0 (from sentence-transformers)\n",
      "  Downloading torch-2.8.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (30 kB)\n",
      "Collecting scikit-learn (from sentence-transformers)\n",
      "  Downloading scikit_learn-1.7.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n",
      "Collecting scipy (from sentence-transformers)\n",
      "  Downloading scipy-1.16.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (61 kB)\n",
      "Collecting huggingface-hub>=0.20.0 (from sentence-transformers)\n",
      "  Downloading huggingface_hub-0.34.4-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: Pillow in ./.venv/lib/python3.12/site-packages (from sentence-transformers) (11.3.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in ./.venv/lib/python3.12/site-packages (from sentence-transformers) (4.15.0)\n",
      "Collecting filelock (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading filelock-3.19.1-py3-none-any.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.3.2)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.7.34)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.32.5)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Downloading fsspec-2025.7.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Downloading hf_xet-1.1.8-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (703 bytes)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (80.9.0)\n",
      "Collecting sympy>=1.13.3 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.8.93 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.8.90 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.8.90 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.10.2.21 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cublas-cu12==12.8.4.1 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cufft-cu12==11.3.3.83 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.9.90 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.7.3.90 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.5.8.93 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.7.1 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl.metadata (7.0 kB)\n",
      "Collecting nvidia-nccl-cu12==2.27.3 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.8.90 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.8.93 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cufile-cu12==1.13.1.3 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==3.4.0 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading triton-3.4.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.12/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.8.3)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn->sentence-transformers)\n",
      "  Downloading joblib-1.5.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn->sentence-transformers)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading sentence_transformers-5.1.0-py3-none-any.whl (483 kB)\n",
      "Downloading transformers-4.55.4-py3-none-any.whl (11.3 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.34.4-py3-none-any.whl (561 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m561.5/561.5 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading hf_xet-1.1.8-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2025.7.0-py3-none-any.whl (199 kB)\n",
      "Downloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (485 kB)\n",
      "Downloading torch-2.8.0-cp312-cp312-manylinux_2_28_x86_64.whl (887.9 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m887.9/887.9 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m  \u001b[33m0:01:13\u001b[0mm0:00:01\u001b[0m00:03\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (594.3 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m594.3/594.3 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m  \u001b[33m0:01:11\u001b[0mm0:00:01\u001b[0m00:03\u001b[0mm\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0meta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m88.0/88.0 MB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m  \u001b[33m0:00:05\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m954.8/954.8 kB\u001b[0m \u001b[31m45.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl (706.8 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m706.8/706.8 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m  \u001b[33m0:00:58\u001b[0mm0:00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (193.1 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m193.1/193.1 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m  \u001b[33m0:00:17\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (63.6 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m63.6/63.6 MB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m  \u001b[33m0:00:04\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (267.5 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m267.5/267.5 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m  \u001b[33m0:00:20\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (288.2 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m288.2/288.2 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m  \u001b[33m0:00:26\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl (287.2 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m287.2/287.2 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m  \u001b[33m0:00:22\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (322.4 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m322.4/322.4 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m  \u001b[33m0:00:24\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
      "Downloading triton-3.4.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (155.6 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m155.6/155.6 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m  \u001b[33m0:00:12\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading filelock-3.19.1-py3-none-any.whl (15 kB)\n",
      "Downloading networkx-3.5-py3-none-any.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading scikit_learn-1.7.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (9.5 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.5.1-py3-none-any.whl (307 kB)\n",
      "Downloading scipy-1.16.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (35.2 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m35.2/35.2 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: nvidia-cusparselt-cu12, mpmath, triton, threadpoolctl, sympy, scipy, safetensors, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, networkx, joblib, hf-xet, fsspec, filelock, scikit-learn, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, huggingface-hub, tokenizers, nvidia-cusolver-cu12, transformers, torch, sentence-transformers\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m31/31\u001b[0m [sentence-transformers]ence-transformers]]\n",
      "\u001b[1A\u001b[2KSuccessfully installed filelock-3.19.1 fsspec-2025.7.0 hf-xet-1.1.8 huggingface-hub-0.34.4 joblib-1.5.1 mpmath-1.3.0 networkx-3.5 nvidia-cublas-cu12-12.8.4.1 nvidia-cuda-cupti-cu12-12.8.90 nvidia-cuda-nvrtc-cu12-12.8.93 nvidia-cuda-runtime-cu12-12.8.90 nvidia-cudnn-cu12-9.10.2.21 nvidia-cufft-cu12-11.3.3.83 nvidia-cufile-cu12-1.13.1.3 nvidia-curand-cu12-10.3.9.90 nvidia-cusolver-cu12-11.7.3.90 nvidia-cusparse-cu12-12.5.8.93 nvidia-cusparselt-cu12-0.7.1 nvidia-nccl-cu12-2.27.3 nvidia-nvjitlink-cu12-12.8.93 nvidia-nvtx-cu12-12.8.90 safetensors-0.6.2 scikit-learn-1.7.1 scipy-1.16.1 sentence-transformers-5.1.0 sympy-1.14.0 threadpoolctl-3.6.0 tokenizers-0.21.4 torch-2.8.0 transformers-4.55.4 triton-3.4.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# %pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a9e44e",
   "metadata": {},
   "source": [
    "## üìä Scoring System Implementation\n",
    "\n",
    "Advanced scoring system to evaluate OCR extraction accuracy using multiple comparison methods including exact matching, numeric tolerance, semantic similarity, and embedding-based similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9521d70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import Any, Dict, List\n",
    "from difflib import SequenceMatcher\n",
    "import numpy as np\n",
    "\n",
    "# Optional: install if not already\n",
    "# pip install sentence-transformers\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load a small model once (fast + lightweight)\n",
    "embed_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "\n",
    "# --- helpers ---\n",
    "def get_by_path(data: dict, path: str):\n",
    "    \"\"\"\n",
    "    Supports:\n",
    "    - normal dot paths (\"totals.sub_total\")\n",
    "    - indexed paths (\"item_details[0].quantity\")\n",
    "    - wildcard array paths (\"item_details.quantity\") -> returns list of values\n",
    "    \"\"\"\n",
    "    keys = path.replace(\"]\", \"\").split(\".\")\n",
    "    val = data\n",
    "    for k in keys:\n",
    "        if isinstance(val, list):\n",
    "            # Apply to each element if list\n",
    "            if k.isdigit():\n",
    "                val = val[int(k)]\n",
    "            else:\n",
    "                val = [v[k] for v in val]\n",
    "        else:\n",
    "            if \"[\" in k:  # explicit index\n",
    "                field, idx = k.split(\"[\")\n",
    "                val = val[field][int(idx)]\n",
    "            else:\n",
    "                val = val[k]\n",
    "    return val\n",
    "\n",
    "\n",
    "# --- comparators ---\n",
    "def exact_match(a, b) -> float:\n",
    "    return 1.0 if str(a) == str(b) else 0.0\n",
    "\n",
    "def numeric_tolerance(a, b, tolerance: float = 0.1) -> float:\n",
    "    try:\n",
    "        a, b = float(a), float(b)\n",
    "    except (ValueError, TypeError):\n",
    "        return 0.0\n",
    "    if a == b:\n",
    "        return 1.0\n",
    "    rel_error = abs(a - b) / (abs(b) + 1e-8)\n",
    "    return max(0.0, 1 - rel_error / tolerance)\n",
    "\n",
    "def semantic_similarity(a, b) -> float:\n",
    "    \"\"\"Cheap semantic: edit similarity.\"\"\"\n",
    "    if not a or not b:\n",
    "        return 0.0\n",
    "    return SequenceMatcher(None, str(a).lower(), str(b).lower()).ratio()\n",
    "\n",
    "def embedding_similarity(a, b) -> float:\n",
    "    \"\"\"Semantic similarity using embeddings + cosine similarity.\"\"\"\n",
    "    if not a or not b:\n",
    "        return 0.0\n",
    "    emb_a = embed_model.encode(str(a), convert_to_numpy=True)\n",
    "    emb_b = embed_model.encode(str(b), convert_to_numpy=True)\n",
    "    cos_sim = np.dot(emb_a, emb_b) / (np.linalg.norm(emb_a) * np.linalg.norm(emb_b))\n",
    "    # map cosine [-1,1] to [0,1]\n",
    "    return float((cos_sim + 1) / 2)\n",
    "\n",
    "\n",
    "COMPARATORS = {\n",
    "    \"exact\": exact_match,\n",
    "    \"numeric\": numeric_tolerance,\n",
    "    \"semantic\": semantic_similarity,   # lightweight string-based\n",
    "    \"embedding\": embedding_similarity, # heavy but meaningful\n",
    "}\n",
    "\n",
    "\n",
    "# --- main scoring ---\n",
    "def score_json(pred: dict, actual: dict, config: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "    total_weight = sum(field[\"weight\"] for field in config)\n",
    "    breakdown = []\n",
    "    weighted_sum = 0.0\n",
    "\n",
    "    for field in config:\n",
    "        path = field[\"path\"]\n",
    "        weight = field.get(\"weight\", 1.0)\n",
    "        comparator = COMPARATORS[field[\"comparator\"]]\n",
    "        tolerance = field.get(\"tolerance\", 0.1)\n",
    "\n",
    "        pred_val = get_by_path(pred, path)\n",
    "        actual_val = get_by_path(actual, path)\n",
    "\n",
    "        # Case 1: scalar\n",
    "        if not isinstance(pred_val, list):\n",
    "            pred_val, actual_val = [pred_val], [actual_val]\n",
    "\n",
    "        # If lengths differ, penalize missing items\n",
    "        max_len = max(len(pred_val), len(actual_val))\n",
    "        scores = []\n",
    "        for i in range(max_len):\n",
    "            pv = pred_val[i] if i < len(pred_val) else None\n",
    "            av = actual_val[i] if i < len(actual_val) else None\n",
    "\n",
    "            if field[\"comparator\"] == \"numeric\":\n",
    "                s = comparator(pv, av, tolerance)\n",
    "            else:\n",
    "                s = comparator(pv, av)\n",
    "            scores.append(s)\n",
    "\n",
    "            breakdown.append({\n",
    "                \"path\": f\"{path}[{i}]\",\n",
    "                \"pred\": pv,\n",
    "                \"actual\": av,\n",
    "                \"comparator\": field[\"comparator\"],\n",
    "                \"score\": round(s, 3),\n",
    "                \"weight\": weight,\n",
    "                \"weighted\": round(weight * s, 3)\n",
    "            })\n",
    "\n",
    "        avg_score = sum(scores) / max_len if max_len > 0 else 0\n",
    "        weighted_sum += weight * avg_score\n",
    "\n",
    "    final_score = weighted_sum / total_weight if total_weight > 0 else 0\n",
    "    return {\n",
    "        \"final_score\": round(final_score, 3),\n",
    "        \"breakdown\": breakdown\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3373ba7a",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Scoring Configuration and Testing\n",
    "\n",
    "Configure the scoring system with field-specific weights and comparison methods, then test it with sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1a4e69e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = [\n",
    "    {\"path\": \"payment_details.supplier\", \"weight\": 1, \"comparator\": \"exact\"},\n",
    "    {\"path\": \"payment_details.payment_date\", \"weight\": 1, \"comparator\": \"exact\"},\n",
    "    {\"path\": \"payment_details.payment_account\", \"weight\": 1, \"comparator\": \"embedding\"},\n",
    "    {\"path\": \"payment_details.payment_method\", \"weight\": 1, \"comparator\": \"exact\"},\n",
    "    {\"path\": \"payment_details.ref_no\", \"weight\": 1, \"comparator\": \"exact\"},\n",
    "    {\"path\": \"payment_details.tags\", \"weight\": 1, \"comparator\": \"semantic\"},\n",
    "    {\"path\": \"item_details.category\", \"weight\": 1, \"comparator\": \"exact\"},\n",
    "    {\"path\": \"item_details.description\", \"weight\": 1, \"comparator\": \"embedding\"},\n",
    "    {\"path\": \"item_details.quantity\", \"weight\": 1, \"comparator\": \"numeric\", \"tolerance\": 0.05},\n",
    "    {\"path\": \"item_details.unit_price\", \"weight\": 1, \"comparator\": \"numeric\", \"tolerance\": 0.1},\n",
    "    {\"path\": \"item_details.total\", \"weight\": 1, \"comparator\": \"numeric\", \"tolerance\": 0.1},\n",
    "    {\"path\": \"attachment\", \"weight\": 1, \"comparator\": \"exact\"},\n",
    "    {\"path\": \"memo\", \"weight\": 1, \"comparator\": \"embedding\"},\n",
    "    {\"path\": \"totals.sub_total\", \"weight\": 1, \"comparator\": \"numeric\", \"tolerance\": 0.1},\n",
    "    {\"path\": \"totals.sales_tax.is_percentage\", \"weight\": 1, \"comparator\": \"exact\"},\n",
    "    {\"path\": \"totals.sales_tax.tax\", \"weight\": 1, \"comparator\": \"numeric\", \"tolerance\": 0.1},\n",
    "    {\"path\": \"totals.sales_tax.amount\", \"weight\": 1, \"comparator\": \"numeric\", \"tolerance\": 0.1},\n",
    "    {\"path\": \"totals.discount.is_percentage\", \"weight\": 1, \"comparator\": \"exact\"},\n",
    "    {\"path\": \"totals.discount.discount\", \"weight\": 1, \"comparator\": \"numeric\", \"tolerance\": 0.1},\n",
    "    {\"path\": \"totals.discount.calculated_amount\", \"weight\": 1, \"comparator\": \"numeric\", \"tolerance\": 0.1},\n",
    "    {\"path\": \"totals.total_amount\", \"weight\": 1, \"comparator\": \"numeric\", \"tolerance\": 0.1},\n",
    "]\n",
    "\n",
    "\n",
    "# result = score_json(output_json_2, actual_json_2, config)\n",
    "\n",
    "# print(\"Final Score:\", result[\"final_score\"])\n",
    "# for r in result[\"breakdown\"]:\n",
    "#     print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40fb5f1",
   "metadata": {},
   "source": [
    "## üìà Batch Scoring and Performance Analysis\n",
    "\n",
    "Implement and execute batch scoring functionality to evaluate OCR performance across multiple images, comparing results with ground truth data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "75289b49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "BATCH SCORING RESULTS\n",
      "================================================================================\n",
      "üìä IMG_20250826_154308~2.jpg: 0.815\n",
      "‚ùå No ground truth found for: IMG_20250826_154831.jpg\n",
      "‚ö†Ô∏è  Missing structured data for: IMG_20250826_154929.jpg\n",
      "üìä IMG_20250826_155237.jpg: 0.582\n",
      "üìä IMG_20250826_155754.jpg: 0.750\n",
      "üìä IMG_20250826_160041.jpg: 0.549\n",
      "üìä IMG_20250826_160529.jpg: 0.495\n",
      "üìä IMG_20250826_160644.jpg: 0.588\n",
      "üìä IMG_20250826_160928.jpg: 0.438\n",
      "üìä IMG_20250826_160949.jpg: 0.492\n",
      "\n",
      "================================================================================\n",
      "SUMMARY STATISTICS\n",
      "================================================================================\n",
      "üìà Total Images Processed: 8\n",
      "üìä Average Score: 0.589\n",
      "üéØ Total Score Sum: 4.709\n",
      "üìâ Lowest Score: 0.438\n",
      "üìà Highest Score: 0.815\n",
      "\n",
      "üìä Score Distribution:\n",
      "   0.0-0.2: 0 images\n",
      "   0.2-0.4: 0 images\n",
      "   0.4-0.6: 6 images\n",
      "   0.6-0.8: 1 images\n",
      "   0.8-1.0: 1 images\n",
      "\n",
      "üíæ Detailed results saved to: /home/tanjim_noor/Work/AI OCR BenchMark/OCR_Output/batch_scoring_results_qwen.json\n"
     ]
    }
   ],
   "source": [
    "# Batch scoring for all images in JSON files\n",
    "import json\n",
    "import os\n",
    "\n",
    "def score_all_images(results_json_path, actual_json_path, config):\n",
    "    \"\"\"\n",
    "    Score all images by comparing results JSON with actual JSON\n",
    "    \n",
    "    Args:\n",
    "        results_json_path: Path to the OCR results JSON file\n",
    "        actual_json_path: Path to the actual/ground truth JSON file\n",
    "        config: Configuration list for scoring (same as above)\n",
    "    \n",
    "    Returns:\n",
    "        Dict with overall statistics and individual image scores\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load JSON files\n",
    "    try:\n",
    "        with open(results_json_path, 'r') as f:\n",
    "            results_data = json.load(f)\n",
    "        with open(actual_json_path, 'r') as f:\n",
    "            actual_data = json.load(f)\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: File not found - {e}\")\n",
    "        return None\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error: Invalid JSON - {e}\")\n",
    "        return None\n",
    "    \n",
    "    # Create lookup dictionary for actual data by image path\n",
    "    actual_lookup = {item['image_path']: item for item in actual_data}\n",
    "    \n",
    "    # Score each image\n",
    "    image_scores = []\n",
    "    total_score = 0\n",
    "    processed_count = 0\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"BATCH SCORING RESULTS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for result_item in results_data:\n",
    "        image_path = result_item['image_path']\n",
    "        image_name = os.path.basename(image_path)\n",
    "        \n",
    "        # Find corresponding actual data\n",
    "        if image_path not in actual_lookup:\n",
    "            print(f\"‚ùå No ground truth found for: {image_name}\")\n",
    "            continue\n",
    "            \n",
    "        actual_item = actual_lookup[image_path]\n",
    "        \n",
    "        # Extract structured data for comparison\n",
    "        result_structured = result_item.get('structured_data', {})\n",
    "        actual_structured = actual_item.get('structured_data', {})\n",
    "        \n",
    "        # Score this image\n",
    "        if result_structured and actual_structured:\n",
    "            score_result = score_json(result_structured, actual_structured, config)\n",
    "            final_score = score_result['final_score']\n",
    "            \n",
    "            image_scores.append({\n",
    "                'image_name': image_name,\n",
    "                'image_path': image_path,\n",
    "                'score': final_score,\n",
    "                'breakdown': score_result['breakdown']\n",
    "            })\n",
    "            \n",
    "            total_score += final_score\n",
    "            processed_count += 1\n",
    "            \n",
    "            # Print score for this image\n",
    "            print(f\"üìä {image_name}: {final_score:.3f}\")\n",
    "            \n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è  Missing structured data for: {image_name}\")\n",
    "    \n",
    "    # Calculate overall statistics\n",
    "    if processed_count > 0:\n",
    "        average_score = total_score / processed_count\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"SUMMARY STATISTICS\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"üìà Total Images Processed: {processed_count}\")\n",
    "        print(f\"üìä Average Score: {average_score:.3f}\")\n",
    "        print(f\"üéØ Total Score Sum: {total_score:.3f}\")\n",
    "        print(f\"üìâ Lowest Score: {min(item['score'] for item in image_scores):.3f}\")\n",
    "        print(f\"üìà Highest Score: {max(item['score'] for item in image_scores):.3f}\")\n",
    "        \n",
    "        # Show breakdown of scores\n",
    "        score_ranges = {'0.0-0.2': 0, '0.2-0.4': 0, '0.4-0.6': 0, '0.6-0.8': 0, '0.8-1.0': 0}\n",
    "        for item in image_scores:\n",
    "            score = item['score']\n",
    "            if score < 0.2:\n",
    "                score_ranges['0.0-0.2'] += 1\n",
    "            elif score < 0.4:\n",
    "                score_ranges['0.2-0.4'] += 1\n",
    "            elif score < 0.6:\n",
    "                score_ranges['0.4-0.6'] += 1\n",
    "            elif score < 0.8:\n",
    "                score_ranges['0.6-0.8'] += 1\n",
    "            else:\n",
    "                score_ranges['0.8-1.0'] += 1\n",
    "        \n",
    "        print(\"\\nüìä Score Distribution:\")\n",
    "        for range_name, count in score_ranges.items():\n",
    "            print(f\"   {range_name}: {count} images\")\n",
    "            \n",
    "    else:\n",
    "        average_score = 0\n",
    "        print(\"‚ùå No images were processed successfully\")\n",
    "    \n",
    "    return {\n",
    "        'processed_count': processed_count,\n",
    "        'average_score': average_score,\n",
    "        'total_score': total_score,\n",
    "        'image_scores': image_scores\n",
    "    }\n",
    "\n",
    "# Example usage:\n",
    "# Set the paths to your JSON files\n",
    "results_json_path = \"/home/tanjim_noor/Work/AI OCR BenchMark/OCR_Output/ocr_results_cash_expense_v2_huggingface_20250902_144246.json\"\n",
    "actual_json_path = \"/home/tanjim_noor/Work/AI OCR BenchMark/OCR_Output/ocr_qwen_actual_1_structured_only.json\"\n",
    "\n",
    "# Use the same config as defined above\n",
    "batch_results = score_all_images(results_json_path, actual_json_path, config)\n",
    "\n",
    "# Optionally, save detailed results to a file\n",
    "if batch_results:\n",
    "    output_path = \"/home/tanjim_noor/Work/AI OCR BenchMark/OCR_Output/batch_scoring_results_qwen.json\"\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(batch_results, f, indent=2)\n",
    "    print(f\"\\nüíæ Detailed results saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42bf65a",
   "metadata": {},
   "source": [
    "### Additional Examples and Utilities\n",
    "\n",
    "This section contains additional examples including batch processing templates, model switching, custom prompts, and benchmarking utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1f2bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Process all images in directory (Batch Processing)\n",
    "# This will process all images in the Pay Slip directory\n",
    "\n",
    "# Uncomment the following code to process all images:\n",
    "\"\"\"\n",
    "try:\n",
    "    pay_slip_dir = \"/home/tanjim_noor/Work/AI OCR BenchMark/Pay Slip\"\n",
    "    output_file = \"/home/tanjim_noor/Work/AI OCR BenchMark/ocr_results.json\"\n",
    "    \n",
    "    print(\"Starting batch processing...\")\n",
    "    print(f\"Input directory: {pay_slip_dir}\")\n",
    "    print(f\"Output file: {output_file}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Process all images with payslip extraction prompt\n",
    "    results = ocr.process_directory(\n",
    "        directory_path=pay_slip_dir,\n",
    "        prompt=sample_prompts[\"payslip_extraction\"],\n",
    "        output_file=output_file\n",
    "    )\n",
    "    \n",
    "    # Summary\n",
    "    successful = sum(1 for r in results if r[\"success\"])\n",
    "    failed = len(results) - successful\n",
    "    \n",
    "    print(\"=\" * 50)\n",
    "    print(f\"üìä PROCESSING SUMMARY:\")\n",
    "    print(f\"Total images: {len(results)}\")\n",
    "    print(f\"Successful: {successful}\")\n",
    "    print(f\"Failed: {failed}\")\n",
    "    print(f\"Results saved to: {output_file}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error during batch processing: {e}\")\n",
    "\"\"\"\n",
    "\n",
    "print(\"Batch processing code is commented out.\")\n",
    "print(\"Uncomment the code above to process all images in the directory.\")\n",
    "print(\"Note: This will use API calls for each image, so be mindful of costs!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3f680f",
   "metadata": {},
   "source": [
    "### Model Switching Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7628ac10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Switch between different models\n",
    "# Demonstrate model-agnostic capabilities\n",
    "\n",
    "print(\"üîÑ Model Switching Examples\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Current model\n",
    "print(f\"Current model: {ocr.model_name}\")\n",
    "\n",
    "print(\"\\nüìù Available models:\")\n",
    "print(\"1. gemini - Google Gemini 1.5 Flash\")\n",
    "print(\"2. openai - OpenAI GPT-4 Vision\")  \n",
    "print(\"3. anthropic - Claude 3.5 Sonnet\")\n",
    "\n",
    "print(\"\\nüîß How to switch models:\")\n",
    "print(\"ocr.change_model('openai')    # Switch to OpenAI\")\n",
    "print(\"ocr.change_model('anthropic') # Switch to Claude\")\n",
    "print(\"ocr.change_model('gemini')    # Switch to Gemini\")\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è Note: Make sure to set the corresponding API keys in .env file\")\n",
    "\n",
    "# Example of switching (commented out)\n",
    "\"\"\"\n",
    "# Switch to OpenAI\n",
    "try:\n",
    "    ocr.change_model('openai')\n",
    "    # Process an image with OpenAI\n",
    "    result = ocr.extract_data_from_image(image_path, prompt)\n",
    "except Exception as e:\n",
    "    print(f\"OpenAI error: {e}\")\n",
    "\n",
    "# Switch to Claude\n",
    "try:\n",
    "    ocr.change_model('anthropic')\n",
    "    # Process an image with Claude\n",
    "    result = ocr.extract_data_from_image(image_path, prompt)\n",
    "except Exception as e:\n",
    "    print(f\"Claude error: {e}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d6429a",
   "metadata": {},
   "source": [
    "### Custom Prompts and Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "848af369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Custom Prompt Examples\n",
      "==============================\n",
      "\n",
      "üìù PAYSLIP_DETAILED:\n",
      "   Extract all form fields and their values from this document. Focus specifically on extracting: employee name, employee ID, salary period, gross pay, net pay, tax deductions, company name. Present the information in a clear, structured format.\n",
      "\n",
      "üìù RECEIPT_ANALYSIS:\n",
      "   Extract receipt information including store name, date, items purchased, prices, and total amount. Focus specifically on extracting: store name, purchase date, items, total amount, payment method. Present the information in a clear, structured format.\n",
      "\n",
      "üìù TABLE_EXTRACTION:\n",
      "   Extract table data and organize it in a structured format with headers and rows. Present the information in a clear, structured format.\n",
      "\n",
      "üìù GENERAL_OCR:\n",
      "   Extract all text content from this image, preserving structure and formatting. Present the information in a clear, structured format.\n",
      "\n",
      "‚ú® You can use these prompts with:\n",
      "   result = ocr.extract_data_from_image(image_path, custom_prompts['payslip_detailed'])\n"
     ]
    }
   ],
   "source": [
    "# Example 4: Custom Prompts and Utilities\n",
    "\n",
    "def create_custom_prompt(task_type: str, specific_fields: Optional[List[str]] = None) -> str:\n",
    "    \"\"\"\n",
    "    Helper function to create custom prompts for different OCR tasks.\n",
    "    \n",
    "    Args:\n",
    "        task_type: Type of document ('invoice', 'receipt', 'form', 'table', 'general')\n",
    "        specific_fields: List of specific fields to extract\n",
    "    \"\"\"\n",
    "    base_prompts = {\n",
    "        'invoice': \"Extract invoice information including invoice number, date, vendor, amount, tax, and line items.\",\n",
    "        'receipt': \"Extract receipt information including store name, date, items purchased, prices, and total amount.\",\n",
    "        'form': \"Extract all form fields and their values from this document.\",\n",
    "        'table': \"Extract table data and organize it in a structured format with headers and rows.\",\n",
    "        'general': \"Extract all text content from this image, preserving structure and formatting.\"\n",
    "    }\n",
    "    \n",
    "    prompt = base_prompts.get(task_type, base_prompts['general'])\n",
    "    \n",
    "    if specific_fields:\n",
    "        fields_str = \", \".join(specific_fields)\n",
    "        prompt += f\" Focus specifically on extracting: {fields_str}.\"\n",
    "    \n",
    "    prompt += \" Present the information in a clear, structured format.\"\n",
    "    return prompt\n",
    "\n",
    "# Example custom prompts\n",
    "print(\"üéØ Custom Prompt Examples\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Create some custom prompts\n",
    "custom_prompts = {\n",
    "    \"payslip_detailed\": create_custom_prompt(\"form\", [\n",
    "        \"employee name\", \"employee ID\", \"salary period\", \n",
    "        \"gross pay\", \"net pay\", \"tax deductions\", \"company name\"\n",
    "    ]),\n",
    "    \n",
    "    \"receipt_analysis\": create_custom_prompt(\"receipt\", [\n",
    "        \"store name\", \"purchase date\", \"items\", \"total amount\", \"payment method\"\n",
    "    ]),\n",
    "    \n",
    "    \"table_extraction\": create_custom_prompt(\"table\"),\n",
    "    \n",
    "    \"general_ocr\": create_custom_prompt(\"general\")\n",
    "}\n",
    "\n",
    "for name, prompt in custom_prompts.items():\n",
    "    print(f\"\\nüìù {name.upper()}:\")\n",
    "    print(f\"   {prompt}\")\n",
    "\n",
    "print(\"\\n‚ú® You can use these prompts with:\")\n",
    "print(\"   result = ocr.extract_data_from_image(image_path, custom_prompts['payslip_detailed'])\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e670135f",
   "metadata": {},
   "source": [
    "### üèÜ Benchmarking Multiple Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bcf62d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 5: Benchmarking Multiple Models\n",
    "# Compare results from different models on the same image\n",
    "\n",
    "def benchmark_models(image_path: str, prompt: str, models: Optional[List[str]] = None) -> Dict:\n",
    "    \"\"\"\n",
    "    Benchmark multiple models on the same image and prompt.\n",
    "    \n",
    "    Args:\n",
    "        image_path: Path to the image file\n",
    "        prompt: Prompt to use for extraction\n",
    "        models: List of models to test (defaults to available models)\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with results from each model\n",
    "    \"\"\"\n",
    "    if models is None:\n",
    "        models = [\"gemini\", \"openai\", \"anthropic\"]\n",
    "    \n",
    "    benchmark_results = {\n",
    "        \"image_path\": image_path,\n",
    "        \"prompt\": prompt,\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"results\": {}\n",
    "    }\n",
    "    \n",
    "    for model_name in models:\n",
    "        print(f\"Testing {model_name}...\")\n",
    "        try:\n",
    "            # Create new OCR instance for each model\n",
    "            test_ocr = OCRBenchmark(model_name=model_name, temperature=0.1)\n",
    "            \n",
    "            # Check if model was initialized successfully\n",
    "            if test_ocr.model is None:\n",
    "                print(f\"‚ö†Ô∏è {model_name} - Skipped: API key not available\")\n",
    "                benchmark_results[\"results\"][model_name] = {\n",
    "                    \"success\": False,\n",
    "                    \"error\": f\"API key not available for {model_name}\",\n",
    "                    \"model_used\": model_name\n",
    "                }\n",
    "                continue\n",
    "            \n",
    "            result = test_ocr.extract_data_from_image(image_path, prompt)\n",
    "            benchmark_results[\"results\"][model_name] = result\n",
    "            \n",
    "            if result[\"success\"]:\n",
    "                print(f\"‚úÖ {model_name} - Success\")\n",
    "            else:\n",
    "                print(f\"‚ùå {model_name} - Failed: {result.get('error', 'Unknown error')}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå {model_name} - Error: {str(e)}\")\n",
    "            benchmark_results[\"results\"][model_name] = {\n",
    "                \"success\": False,\n",
    "                \"error\": str(e),\n",
    "                \"model_used\": model_name\n",
    "            }\n",
    "    \n",
    "    return benchmark_results\n",
    "\n",
    "# Utility function to compare and analyze results\n",
    "def analyze_benchmark_results(benchmark_results: Dict):\n",
    "    \"\"\"Analyze and display benchmark results.\"\"\"\n",
    "    print(\"\\nüìä BENCHMARK ANALYSIS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    results = benchmark_results[\"results\"]\n",
    "    successful_models = [model for model, result in results.items() if result.get(\"success\", False)]\n",
    "    failed_models = [model for model, result in results.items() if not result.get(\"success\", False)]\n",
    "    \n",
    "    print(f\"Image: {Path(benchmark_results['image_path']).name}\")\n",
    "    print(f\"Total models tested: {len(results)}\")\n",
    "    print(f\"Successful: {len(successful_models)} - {successful_models}\")\n",
    "    print(f\"Failed: {len(failed_models)} - {failed_models}\")\n",
    "    \n",
    "    if successful_models:\n",
    "        print(\"\\nüìù Extracted Data Comparison:\")\n",
    "        for model in successful_models:\n",
    "            print(f\"\\nü§ñ {model.upper()}:\")\n",
    "            extracted_data = results[model][\"extracted_data\"]\n",
    "            # Show first 200 characters\n",
    "            preview = extracted_data[:200] + \"...\" if len(extracted_data) > 200 else extracted_data\n",
    "            print(f\"   {preview}\")\n",
    "\n",
    "print(\"üöÄ Benchmarking utilities defined!\")\n",
    "print(\"Use benchmark_models() to compare multiple models on the same image.\")\n",
    "\n",
    "# Example usage (commented out):\n",
    "\"\"\"\n",
    "# Benchmark all models on a single image\n",
    "image_path = \"/path/to/your/image.png\"\n",
    "prompt = sample_prompts[\"payslip_extraction\"]\n",
    "\n",
    "benchmark_results = benchmark_models(image_path, prompt)\n",
    "analyze_benchmark_results(benchmark_results)\n",
    "\n",
    "# Save benchmark results\n",
    "with open(\"benchmark_results.json\", \"w\") as f:\n",
    "    json.dump(benchmark_results, f, indent=2)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6deb2eb7",
   "metadata": {},
   "source": [
    "# Quick Start Guide\n",
    "\n",
    "## Setup Instructions\n",
    "\n",
    "1. **Set up API Keys** - Add your API keys to the `.env` file:\n",
    "   ```bash\n",
    "   GOOGLE_API_KEY=your_actual_google_api_key\n",
    "   OPENAI_API_KEY=your_actual_openai_api_key  \n",
    "   ANTHROPIC_API_KEY=your_actual_anthropic_api_key\n",
    "   ```\n",
    "\n",
    "2. **Initialize the OCR System**:\n",
    "   ```python\n",
    "   ocr = OCRBenchmark(model_name=\"gemini\")  # Default: Gemini\n",
    "   ```\n",
    "\n",
    "3. **Process a Single Image**:\n",
    "   ```python\n",
    "   result = ocr.extract_data_from_image(\"path/to/image.png\", \"Extract all text\")\n",
    "   ```\n",
    "\n",
    "4. **Process All Images in Directory**:\n",
    "   ```python\n",
    "   results = ocr.process_directory(\"path/to/directory\", \"Extract payslip data\")\n",
    "   ```\n",
    "\n",
    "## Available Models\n",
    "- **gemini** - Google Gemini 1.5 Flash (Default)\n",
    "- **openai** - OpenAI GPT-4 Vision  \n",
    "- **anthropic** - Claude 3.5 Sonnet\n",
    "\n",
    "## Features\n",
    "‚úÖ **Model Agnostic** - Switch between models easily  \n",
    "‚úÖ **Batch Processing** - Process entire directories  \n",
    "‚úÖ **Custom Prompts** - Flexible prompt engineering  \n",
    "‚úÖ **JSON Output** - Save results to files  \n",
    "‚úÖ **Error Handling** - Robust error management  \n",
    "‚úÖ **Benchmarking** - Compare model performance  \n",
    "\n",
    "## Next Steps\n",
    "1. Set your API keys in `.env`\n",
    "2. Run the initialization cell\n",
    "3. Try processing a single image first\n",
    "4. Experiment with different prompts\n",
    "5. Compare results across models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befcc03f",
   "metadata": {},
   "source": [
    "### API Keys Status Check\n",
    "\n",
    "Verify which AI model API keys are available and properly configured in the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc6753b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîë API Keys Status Check\n",
      "==============================\n",
      "Available models:\n",
      "  gemini: ‚úÖ Available\n",
      "  openai: ‚ùå Missing API Key\n",
      "  anthropic: ‚ùå Missing API Key\n",
      "\n",
      "Recommended model: gemini\n",
      "\n",
      "‚úÖ 1 out of 3 models available!\n"
     ]
    }
   ],
   "source": [
    "# Check API Keys Availability\n",
    "print(\"üîë API Keys Status Check\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Create a temporary OCR instance to check API keys\n",
    "temp_ocr = OCRBenchmark()\n",
    "api_status = temp_ocr.check_api_keys()\n",
    "\n",
    "print(\"Available models:\")\n",
    "for model, available in api_status.items():\n",
    "    status = \"‚úÖ Available\" if available else \"‚ùå Missing API Key\"\n",
    "    print(f\"  {model}: {status}\")\n",
    "\n",
    "print(f\"\\nRecommended model: {next((model for model, available in api_status.items() if available), 'None available')}\")\n",
    "\n",
    "if not any(api_status.values()):\n",
    "    print(\"\\n‚ö†Ô∏è No API keys found!\")\n",
    "    print(\"Please set at least one API key in your .env file:\")\n",
    "    print(\"  GOOGLE_API_KEY=your_key_here\")\n",
    "    print(\"  OPENAI_API_KEY=your_key_here\") \n",
    "    print(\"  ANTHROPIC_API_KEY=your_key_here\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ {sum(api_status.values())} out of {len(api_status)} models available!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aeb89bc",
   "metadata": {},
   "source": [
    "### Testing Image Format Support\n",
    "\n",
    "Verify that JPG and other image formats are properly supported and test MIME type handling for different file extensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b0616b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test JPG file support and MIME type handling\n",
    "print(\"üîç Testing JPG File Support\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Show supported formats\n",
    "print(\"Supported image formats:\")\n",
    "for fmt in sorted(ocr.supported_formats):\n",
    "    print(f\"  {fmt}\")\n",
    "\n",
    "print(\"\\nMIME type mappings:\")\n",
    "for ext, mime in ocr.mime_type_map.items():\n",
    "    print(f\"  {ext} ‚Üí image/{mime}\")\n",
    "\n",
    "print(\"\\nüìÅ Checking for JPG files in your directories...\")\n",
    "\n",
    "# Check both directories for JPG files\n",
    "directories_to_check = [\n",
    "    \"/home/tanjim_noor/Work/AI OCR BenchMark/Pay Slip\",\n",
    "    \"/home/tanjim_noor/Work/AI OCR BenchMark/test png\"\n",
    "]\n",
    "\n",
    "for directory in directories_to_check:\n",
    "    if os.path.exists(directory):\n",
    "        print(f\"\\nüìÇ {directory}:\")\n",
    "        try:\n",
    "            all_files = list(Path(directory).iterdir())\n",
    "            jpg_files = [f for f in all_files if f.suffix.lower() in ['.jpg', '.jpeg']]\n",
    "            \n",
    "            if jpg_files:\n",
    "                print(f\"  Found {len(jpg_files)} JPG/JPEG file(s):\")\n",
    "                for jpg_file in jpg_files[:5]:  # Show first 5\n",
    "                    file_ext = jpg_file.suffix.lower()\n",
    "                    mime_type = ocr.mime_type_map.get(file_ext, file_ext[1:])\n",
    "                    print(f\"    {jpg_file.name} ({file_ext} ‚Üí image/{mime_type})\")\n",
    "                if len(jpg_files) > 5:\n",
    "                    print(f\"    ... and {len(jpg_files) - 5} more\")\n",
    "            else:\n",
    "                print(\"  No JPG files found\")\n",
    "                \n",
    "            # Show all supported image files\n",
    "            supported_images = ocr.get_images_from_directory(directory)\n",
    "            if supported_images:\n",
    "                print(f\"  Total supported images: {len(supported_images)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Error reading directory: {e}\")\n",
    "    else:\n",
    "        print(f\"\\nüìÇ {directory}: Directory not found\")\n",
    "\n",
    "print(f\"\\n‚úÖ JPG files are fully supported!\")\n",
    "print(\"The system will automatically convert .jpg extensions to 'image/jpeg' MIME type.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7120c7bd",
   "metadata": {},
   "source": [
    "### MIME Type Mapping Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd97e3fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Testing MIME Type Mapping\n",
      "==============================\n",
      ".jpg ‚Üí image/jpeg\n",
      ".jpeg ‚Üí image/jpeg\n",
      ".png ‚Üí image/png\n",
      ".webp ‚Üí image/webp\n",
      ".gif ‚Üí image/gif\n",
      "\n",
      "üìÅ Your file: IMG_20250826_154052.jpg\n",
      "Extension: .jpg\n",
      "MIME type: image/jpeg\n",
      "Should be: image/jpeg\n",
      "Mapping works: ‚úÖ\n"
     ]
    }
   ],
   "source": [
    "# Quick MIME type test\n",
    "print(\"üß™ Testing MIME Type Mapping\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "test_extensions = ['.jpg', '.jpeg', '.png', '.webp', '.gif']\n",
    "for ext in test_extensions:\n",
    "    mime_type = ocr.mime_type_map.get(ext, ext[1:])\n",
    "    print(f\"{ext} ‚Üí image/{mime_type}\")\n",
    "\n",
    "# Test with your specific file\n",
    "test_file = \"/home/tanjim_noor/Work/AI OCR BenchMark/Pay Slip/IMG_20250826_154052.jpg\"\n",
    "if os.path.exists(test_file):\n",
    "    file_ext = Path(test_file).suffix.lower()\n",
    "    mime_type = ocr.mime_type_map.get(file_ext, file_ext[1:])\n",
    "    print(f\"\\nüìÅ Your file: {Path(test_file).name}\")\n",
    "    print(f\"Extension: {file_ext}\")\n",
    "    print(f\"MIME type: image/{mime_type}\")\n",
    "    print(f\"Should be: image/jpeg\")\n",
    "    print(f\"Mapping works: {'‚úÖ' if mime_type == 'jpeg' else '‚ùå'}\")\n",
    "else:\n",
    "    print(f\"\\nüìÅ Test file not found: {test_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c043322d",
   "metadata": {},
   "source": [
    "### Image Diagnostics\n",
    "\n",
    "Analyze image files for size, dimensions, and optimization requirements to understand processing needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc2f17b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Image Diagnostics\n",
      "========================================\n",
      "Found 25 images. Analyzing first few...\n",
      "\n",
      "üìÅ 1. IMG_20250826_154052.jpg\n",
      "   File size: 2.98 MB\n",
      "   Dimensions: 4624x3472 pixels\n",
      "   Mode: RGB\n",
      "   Estimated base64 size: 3.96 MB\n",
      "   ‚ö†Ô∏è Large dimensions - will be resized\n",
      "\n",
      "üìÅ 2. IMG_20250826_154131.jpg\n",
      "   File size: 3.70 MB\n",
      "   Dimensions: 4624x3472 pixels\n",
      "   Mode: RGB\n",
      "   Estimated base64 size: 4.92 MB\n",
      "   ‚ö†Ô∏è Large dimensions - will be resized\n",
      "\n",
      "üìÅ 3. IMG_20250826_154154.jpg\n",
      "   File size: 3.52 MB\n",
      "   Dimensions: 4624x3472 pixels\n",
      "   Mode: RGB\n",
      "   Estimated base64 size: 4.69 MB\n",
      "   ‚ö†Ô∏è Large dimensions - will be resized\n"
     ]
    }
   ],
   "source": [
    "# Diagnostic: Check image file sizes and dimensions\n",
    "print(\"üîç Image Diagnostics\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "pay_slip_dir = \"/home/tanjim_noor/Work/AI OCR BenchMark/Pay Slip\"\n",
    "if os.path.exists(pay_slip_dir):\n",
    "    image_files = ocr.get_images_from_directory(pay_slip_dir)\n",
    "    \n",
    "    if image_files:\n",
    "        print(f\"Found {len(image_files)} images. Analyzing first few...\")\n",
    "        \n",
    "        for i, img_path in enumerate(image_files[:3]):\n",
    "            print(f\"\\nüìÅ {i+1}. {Path(img_path).name}\")\n",
    "            \n",
    "            # File size\n",
    "            file_size_mb = os.path.getsize(img_path) / (1024 * 1024)\n",
    "            print(f\"   File size: {file_size_mb:.2f} MB\")\n",
    "            \n",
    "            # Image dimensions\n",
    "            try:\n",
    "                with Image.open(img_path) as img:\n",
    "                    print(f\"   Dimensions: {img.size[0]}x{img.size[1]} pixels\")\n",
    "                    print(f\"   Mode: {img.mode}\")\n",
    "                    \n",
    "                    # Estimate base64 size\n",
    "                    estimated_base64_mb = file_size_mb * 1.33  # Base64 is ~33% larger\n",
    "                    print(f\"   Estimated base64 size: {estimated_base64_mb:.2f} MB\")\n",
    "                    \n",
    "                    if file_size_mb > 4:\n",
    "                        print(f\"   ‚ö†Ô∏è Large file - will be optimized\")\n",
    "                    if img.size[0] > 1024 or img.size[1] > 1024:\n",
    "                        print(f\"   ‚ö†Ô∏è Large dimensions - will be resized\")\n",
    "                        \n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ùå Error reading image: {e}\")\n",
    "    else:\n",
    "        print(\"No images found\")\n",
    "else:\n",
    "    print(\"Directory not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd9a005",
   "metadata": {},
   "source": [
    "### Image Optimization Configuration\n",
    "\n",
    "Utilities to configure global image optimization settings including resize ratios, quality settings, and file size limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3524c7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Optimization Configuration Utility\n",
    "print(\"üõ†Ô∏è Image Optimization Configuration\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "def update_global_settings(enable_optimization=None, resize_ratio=None, max_dimension=None, \n",
    "                          max_file_size_mb=None, jpeg_quality=None):\n",
    "    \"\"\"Update global image optimization settings.\"\"\"\n",
    "    global ENABLE_IMAGE_OPTIMIZATION, IMAGE_RESIZE_RATIO, MAX_IMAGE_DIMENSION\n",
    "    global MAX_FILE_SIZE_MB, JPEG_QUALITY\n",
    "    \n",
    "    if enable_optimization is not None:\n",
    "        ENABLE_IMAGE_OPTIMIZATION = enable_optimization\n",
    "        print(f\"‚úÖ Global optimization: {'Enabled' if enable_optimization else 'Disabled'}\")\n",
    "    \n",
    "    if resize_ratio is not None:\n",
    "        if 0.1 <= resize_ratio <= 1.0:\n",
    "            IMAGE_RESIZE_RATIO = resize_ratio\n",
    "            print(f\"‚úÖ Global resize ratio: {resize_ratio}\")\n",
    "        else:\n",
    "            print(f\"‚ùå Invalid resize ratio: {resize_ratio} (must be 0.1-1.0)\")\n",
    "    \n",
    "    if max_dimension is not None:\n",
    "        MAX_IMAGE_DIMENSION = max_dimension\n",
    "        print(f\"‚úÖ Global max dimension: {max_dimension}px\")\n",
    "    \n",
    "    if max_file_size_mb is not None:\n",
    "        MAX_FILE_SIZE_MB = max_file_size_mb\n",
    "        print(f\"‚úÖ Global max file size: {max_file_size_mb}MB\")\n",
    "    \n",
    "    if jpeg_quality is not None:\n",
    "        if 1 <= jpeg_quality <= 100:\n",
    "            JPEG_QUALITY = jpeg_quality\n",
    "            print(f\"‚úÖ Global JPEG quality: {jpeg_quality}\")\n",
    "        else:\n",
    "            print(f\"‚ùå Invalid JPEG quality: {jpeg_quality} (must be 1-100)\")\n",
    "\n",
    "def show_global_settings():\n",
    "    \"\"\"Display current global settings.\"\"\"\n",
    "    print(\"üåê Current Global Settings:\")\n",
    "    print(f\"  ENABLE_IMAGE_OPTIMIZATION = {ENABLE_IMAGE_OPTIMIZATION}\")\n",
    "    print(f\"  IMAGE_RESIZE_RATIO = {IMAGE_RESIZE_RATIO}\")\n",
    "    print(f\"  MAX_IMAGE_DIMENSION = {MAX_IMAGE_DIMENSION}\")\n",
    "    print(f\"  MAX_FILE_SIZE_MB = {MAX_FILE_SIZE_MB}\")\n",
    "    print(f\"  JPEG_QUALITY = {JPEG_QUALITY}\")\n",
    "\n",
    "# Show current settings\n",
    "show_global_settings()\n",
    "\n",
    "print(\"\\nüìñ Usage Examples:\")\n",
    "print(\"# Disable optimization completely:\")\n",
    "print(\"update_global_settings(enable_optimization=False)\")\n",
    "print(\"\\n# Resize images to 25% of original size:\")\n",
    "print(\"update_global_settings(resize_ratio=0.25)\")\n",
    "print(\"\\n# Set maximum dimension to 512px:\")\n",
    "print(\"update_global_settings(max_dimension=512)\")\n",
    "print(\"\\n# Multiple settings at once:\")\n",
    "print(\"update_global_settings(enable_optimization=True, resize_ratio=0.3, jpeg_quality=70)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a905d486",
   "metadata": {},
   "source": [
    "### Enhanced Processing with Formatting\n",
    "\n",
    "Execute enhanced batch processing with improved JSON formatting and structured data extraction for better analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8586660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced processing with better formatting\n",
    "try:\n",
    "    pay_slip_dir = \"/home/tanjim_noor/Work/AI OCR BenchMark/For benchmark\"\n",
    "    output_file = \"/home/tanjim_noor/Work/AI OCR BenchMark/OCR_Output/ocr_result_formatted.json\"\n",
    "    \n",
    "    print(\"üöÄ Starting enhanced batch processing with formatting...\")\n",
    "    print(f\"Input directory: {pay_slip_dir}\")\n",
    "    print(f\"Output file: {output_file}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Process all images with payslip extraction prompt\n",
    "    results = ocr.process_directory(\n",
    "        directory_path=pay_slip_dir,\n",
    "        prompt=sample_prompts[\"cash_expense_extraction_V2\"],\n",
    "        output_file=output_file,\n",
    "        save_formatted=True  # Enable enhanced formatting\n",
    "    )\n",
    "    \n",
    "    # Summary\n",
    "    successful = sum(1 for r in results if r[\"success\"])\n",
    "    failed = len(results) - successful\n",
    "    parsed_json = sum(1 for r in results if \"extracted_data_parsed\" in r)\n",
    "    \n",
    "    print(\"=\" * 50)\n",
    "    print(f\"üìä ENHANCED PROCESSING SUMMARY:\")\n",
    "    print(f\"Total images: {len(results)}\")\n",
    "    print(f\"Successful: {successful}\")\n",
    "    print(f\"Failed: {failed}\")\n",
    "    print(f\"Successfully parsed JSON: {parsed_json}\")\n",
    "    print(f\"üìÅ Files created:\")\n",
    "    print(f\"  ‚Ä¢ Main results: {output_file}\")\n",
    "    print(f\"  ‚Ä¢ Structured data only: {output_file.replace('.json', '_structured_only.json')}\")\n",
    "    \n",
    "    # Show sample of structured data\n",
    "    if results and \"extracted_data_parsed\" in results[0]:\n",
    "        print(f\"\\nüìã Sample structured data from first image:\")\n",
    "        sample_data = results[0][\"extracted_data_parsed\"]\n",
    "        print(json.dumps(sample_data, indent=2)[:500] + \"...\" if len(str(sample_data)) > 500 else json.dumps(sample_data, indent=2))\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error during enhanced batch processing: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e7c448",
   "metadata": {},
   "source": [
    "### Utility Function to view formatted results\n",
    "\n",
    "View the formatted results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3298b1aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Enhanced result viewing utility created!\n",
      "\n",
      "Usage examples:\n",
      "# View summary of formatted results:\n",
      "view_formatted_results('/home/tanjim_noor/Work/AI OCR BenchMark/OCR_Output/ocr_result_formatted.json')\n",
      "\n",
      "# View with full structured data:\n",
      "view_formatted_results('/home/tanjim_noor/Work/AI OCR BenchMark/OCR_Output/ocr_result_formatted.json', show_full_data=True)\n"
     ]
    }
   ],
   "source": [
    "# Utility function to view formatted results\n",
    "def view_formatted_results(results_file: str, show_full_data: bool = False):\n",
    "    \"\"\"\n",
    "    View formatted OCR results in a readable way.\n",
    "    \n",
    "    Args:\n",
    "        results_file: Path to the JSON results file\n",
    "        show_full_data: Whether to show complete structured data or just summary\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(results_file, 'r', encoding='utf-8') as f:\n",
    "            results = json.load(f)\n",
    "        \n",
    "        print(f\"üìä OCR Results Summary from {Path(results_file).name}\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        for i, result in enumerate(results, 1):\n",
    "            print(f\"\\nüñºÔ∏è Image {i}: {Path(result['image_path']).name}\")\n",
    "            print(f\"   Model: {result.get('model_used', 'Unknown')}\")\n",
    "            print(f\"   Status: {'‚úÖ Success' if result.get('success') else '‚ùå Failed'}\")\n",
    "            \n",
    "            if result.get('success'):\n",
    "                if 'structured_data' in result:\n",
    "                    structured = result['structured_data']\n",
    "                    print(f\"   Format: üìã Structured JSON\")\n",
    "                    \n",
    "                    # Show key information from structured data\n",
    "                    if isinstance(structured, dict):\n",
    "                        if 'payment_details' in structured:\n",
    "                            pd = structured['payment_details']\n",
    "                            print(f\"   Supplier: {pd.get('supplier', 'N/A')}\")\n",
    "                            print(f\"   Date: {pd.get('payment_date', 'N/A')}\")\n",
    "                            print(f\"   Total: {structured.get('totals', {}).get('total_amount', 'N/A')}\")\n",
    "                            print(f\"   Items: {len(structured.get('item_details', []))}\")\n",
    "                        \n",
    "                        if show_full_data:\n",
    "                            print(f\"\\n   üìã Full Structured Data:\")\n",
    "                            print(json.dumps(structured, indent=4))\n",
    "                    \n",
    "                elif 'extraction_format' in result:\n",
    "                    print(f\"   Format: üìù {result['extraction_format'].upper()}\")\n",
    "                    if 'extracted_data_preview' in result:\n",
    "                        print(f\"   Preview: {result['extracted_data_preview']}\")\n",
    "                elif 'extracted_data' in result:\n",
    "                    # Fallback for older format\n",
    "                    preview = result['extracted_data'][:100] + \"...\" if len(result['extracted_data']) > 100 else result['extracted_data']\n",
    "                    print(f\"   Data Preview: {preview}\")\n",
    "            else:\n",
    "                print(f\"   Error: {result.get('error', 'Unknown error')}\")\n",
    "        \n",
    "        print(f\"\\nüìà Summary: {sum(1 for r in results if r.get('success'))} successful out of {len(results)} total\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error reading results file: {e}\")\n",
    "\n",
    "print(\"üîç Enhanced result viewing utility created!\")\n",
    "print(\"\\nUsage examples:\")\n",
    "print(\"# View summary of formatted results:\")\n",
    "print(\"view_formatted_results('/home/tanjim_noor/Work/AI OCR BenchMark/OCR_Output/ocr_result_formatted.json')\")\n",
    "print(\"\\n# View with full structured data:\")\n",
    "print(\"view_formatted_results('/home/tanjim_noor/Work/AI OCR BenchMark/OCR_Output/ocr_result_formatted.json', show_full_data=True)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db0e8f5",
   "metadata": {},
   "source": [
    "# Comprehensive OCR Benchmark Function\n",
    "\n",
    "This section implements a comprehensive all-rounder function that combines batch processing, scoring, and reporting for multiple prompts and models. The function flow:\n",
    "\n",
    "1. **Model + Prompt + Image Directory** ‚Üí OCR Processing\n",
    "2. **OCR Results + Ground Truth** ‚Üí Batch Scoring  \n",
    "3. **Scores Analysis** ‚Üí Generate Comprehensive Report\n",
    "\n",
    "Features:\n",
    "- ‚úÖ Multi-prompt evaluation\n",
    "- ‚úÖ Batch OCR processing\n",
    "- ‚úÖ Automated scoring against ground truth\n",
    "- ‚úÖ Comprehensive performance reports\n",
    "- ‚úÖ Best prompt identification\n",
    "- ‚úÖ Model performance comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d48dccfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comprehensive_ocr_benchmark(\n",
    "    model_name: str,\n",
    "    prompts_dict: Dict[str, str],\n",
    "    image_directory: str,\n",
    "    ground_truth_file: str,\n",
    "    output_base_dir: str = \"/home/tanjim_noor/Work/AI OCR BenchMark/OCR_Output\",\n",
    "    scoring_config: Optional[List[Dict]] = None,\n",
    "    temperature: float = 0.1\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Comprehensive OCR benchmark function that processes images with multiple prompts,\n",
    "    scores results against ground truth, and generates performance reports.\n",
    "    \n",
    "    Args:\n",
    "        model_name: The AI model to use ('gemini', 'openai', 'anthropic')\n",
    "        prompts_dict: Dictionary of {prompt_name: prompt_text} to test\n",
    "        image_directory: Path to directory containing images to process\n",
    "        ground_truth_file: Path to JSON file with ground truth data\n",
    "        output_base_dir: Base directory for saving results\n",
    "        scoring_config: Configuration for scoring system (uses default if None)\n",
    "        temperature: Model temperature setting\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing comprehensive benchmark results including:\n",
    "        - Individual prompt performance\n",
    "        - Best performing prompt\n",
    "        - Detailed scoring breakdown\n",
    "        - Performance statistics\n",
    "    \"\"\"\n",
    "    \n",
    "    from datetime import datetime\n",
    "    import os\n",
    "    from pathlib import Path\n",
    "    \n",
    "    print(\"üöÄ COMPREHENSIVE OCR BENCHMARK\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"Image Directory: {image_directory}\")\n",
    "    print(f\"Ground Truth: {ground_truth_file}\")\n",
    "    print(f\"Prompts to test: {len(prompts_dict)}\")\n",
    "    print(f\"Output Directory: {output_base_dir}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(output_base_dir, exist_ok=True)\n",
    "    \n",
    "    # Initialize OCR system\n",
    "    try:\n",
    "        ocr_system = OCRBenchmark(model_name=model_name, temperature=temperature)\n",
    "        print(f\"‚úÖ OCR system initialized with {model_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to initialize OCR system: {e}\")\n",
    "        return {\"error\": f\"Failed to initialize OCR system: {e}\"}\n",
    "    \n",
    "    # Check if ground truth file exists\n",
    "    if not os.path.exists(ground_truth_file):\n",
    "        print(f\"‚ùå Ground truth file not found: {ground_truth_file}\")\n",
    "        return {\"error\": f\"Ground truth file not found: {ground_truth_file}\"}\n",
    "    \n",
    "    # Get list of images to process\n",
    "    try:\n",
    "        image_files = ocr_system.get_images_from_directory(image_directory)\n",
    "        print(f\"üìÇ Found {len(image_files)} images to process\")\n",
    "        if not image_files:\n",
    "            return {\"error\": \"No supported image files found in directory\"}\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error reading image directory: {e}\")\n",
    "        return {\"error\": f\"Error reading image directory: {e}\"}\n",
    "    \n",
    "    # Use default scoring config if none provided\n",
    "    if scoring_config is None:\n",
    "        scoring_config = [\n",
    "            {\"path\": \"payment_details.supplier\", \"weight\": 1, \"comparator\": \"exact\"},\n",
    "            {\"path\": \"payment_details.payment_date\", \"weight\": 1, \"comparator\": \"exact\"},\n",
    "            {\"path\": \"payment_details.payment_account\", \"weight\": 1, \"comparator\": \"embedding\"},\n",
    "            {\"path\": \"payment_details.payment_method\", \"weight\": 1, \"comparator\": \"exact\"},\n",
    "            {\"path\": \"payment_details.ref_no\", \"weight\": 1, \"comparator\": \"exact\"},\n",
    "            {\"path\": \"payment_details.tags\", \"weight\": 1, \"comparator\": \"semantic\"},\n",
    "            {\"path\": \"item_details.category\", \"weight\": 1, \"comparator\": \"exact\"},\n",
    "            {\"path\": \"item_details.description\", \"weight\": 1, \"comparator\": \"embedding\"},\n",
    "            {\"path\": \"item_details.quantity\", \"weight\": 1, \"comparator\": \"numeric\", \"tolerance\": 0.05},\n",
    "            {\"path\": \"item_details.unit_price\", \"weight\": 1, \"comparator\": \"numeric\", \"tolerance\": 0.1},\n",
    "            {\"path\": \"item_details.total\", \"weight\": 1, \"comparator\": \"numeric\", \"tolerance\": 0.1},\n",
    "            {\"path\": \"attachment\", \"weight\": 1, \"comparator\": \"exact\"},\n",
    "            {\"path\": \"memo\", \"weight\": 1, \"comparator\": \"embedding\"},\n",
    "            {\"path\": \"totals.sub_total\", \"weight\": 1, \"comparator\": \"numeric\", \"tolerance\": 0.1},\n",
    "            {\"path\": \"totals.sales_tax.is_percentage\", \"weight\": 1, \"comparator\": \"exact\"},\n",
    "            {\"path\": \"totals.sales_tax.tax\", \"weight\": 1, \"comparator\": \"numeric\", \"tolerance\": 0.1},\n",
    "            {\"path\": \"totals.sales_tax.amount\", \"weight\": 1, \"comparator\": \"numeric\", \"tolerance\": 0.1},\n",
    "            {\"path\": \"totals.discount.is_percentage\", \"weight\": 1, \"comparator\": \"exact\"},\n",
    "            {\"path\": \"totals.discount.discount\", \"weight\": 1, \"comparator\": \"numeric\", \"tolerance\": 0.1},\n",
    "            {\"path\": \"totals.discount.calculated_amount\", \"weight\": 1, \"comparator\": \"numeric\", \"tolerance\": 0.1},\n",
    "            {\"path\": \"totals.total_amount\", \"weight\": 1, \"comparator\": \"numeric\", \"tolerance\": 0.1},\n",
    "        ]\n",
    "    \n",
    "    # Store results for all prompts\n",
    "    prompt_results = {}\n",
    "    prompt_scores = {}\n",
    "    \n",
    "    # Process each prompt\n",
    "    for prompt_name, prompt_text in prompts_dict.items():\n",
    "        print(f\"\\nüîÑ Processing prompt: {prompt_name}\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Create output file path for this prompt\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        output_file = os.path.join(output_base_dir, f\"ocr_results_{prompt_name}_{model_name}_{timestamp}.json\")\n",
    "        \n",
    "        try:\n",
    "            # Step 1: Batch OCR Processing\n",
    "            print(f\"üìù Step 1: OCR Processing with {model_name}\")\n",
    "            ocr_results = ocr_system.process_directory(\n",
    "                directory_path=image_directory,\n",
    "                prompt=prompt_text,\n",
    "                output_file=output_file,\n",
    "                save_formatted=True\n",
    "            )\n",
    "            \n",
    "            # Check if processing was successful\n",
    "            successful_ocr = sum(1 for r in ocr_results if r.get(\"success\", False))\n",
    "            print(f\"‚úÖ OCR completed: {successful_ocr}/{len(ocr_results)} images processed successfully\")\n",
    "            \n",
    "            if successful_ocr == 0:\n",
    "                print(f\"‚ö†Ô∏è No images processed successfully for prompt: {prompt_name}\")\n",
    "                prompt_results[prompt_name] = {\n",
    "                    \"error\": \"No images processed successfully\",\n",
    "                    \"ocr_results\": ocr_results\n",
    "                }\n",
    "                continue\n",
    "            \n",
    "            # Step 2: Batch Scoring\n",
    "            print(f\"üìä Step 2: Scoring against ground truth\")\n",
    "            structured_results_file = output_file.replace('.json', '_structured_only.json')\n",
    "            \n",
    "            if os.path.exists(structured_results_file):\n",
    "                scoring_results = score_all_images(\n",
    "                    results_json_path=structured_results_file,\n",
    "                    actual_json_path=ground_truth_file,\n",
    "                    config=scoring_config\n",
    "                )\n",
    "                \n",
    "                if scoring_results:\n",
    "                    avg_score = scoring_results['average_score']\n",
    "                    print(f\"‚úÖ Scoring completed: Average score = {avg_score:.3f}\")\n",
    "                    \n",
    "                    # Store results\n",
    "                    prompt_results[prompt_name] = {\n",
    "                        \"ocr_results\": ocr_results,\n",
    "                        \"scoring_results\": scoring_results,\n",
    "                        \"output_files\": {\n",
    "                            \"ocr_results\": output_file,\n",
    "                            \"structured_only\": structured_results_file\n",
    "                        },\n",
    "                        \"statistics\": {\n",
    "                            \"total_images\": len(ocr_results),\n",
    "                            \"successful_ocr\": successful_ocr,\n",
    "                            \"processed_for_scoring\": scoring_results['processed_count'],\n",
    "                            \"average_score\": avg_score\n",
    "                        }\n",
    "                    }\n",
    "                    prompt_scores[prompt_name] = avg_score\n",
    "                else:\n",
    "                    print(f\"‚ùå Scoring failed for prompt: {prompt_name}\")\n",
    "                    prompt_results[prompt_name] = {\n",
    "                        \"error\": \"Scoring failed\",\n",
    "                        \"ocr_results\": ocr_results\n",
    "                    }\n",
    "            else:\n",
    "                print(f\"‚ùå Structured results file not found: {structured_results_file}\")\n",
    "                prompt_results[prompt_name] = {\n",
    "                    \"error\": \"Structured results file not found\",\n",
    "                    \"ocr_results\": ocr_results\n",
    "                }\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error processing prompt {prompt_name}: {e}\")\n",
    "            prompt_results[prompt_name] = {\n",
    "                \"error\": str(e),\n",
    "                \"ocr_results\": []\n",
    "            }\n",
    "    \n",
    "    # Step 3: Generate Comprehensive Report\n",
    "    print(f\"\\nüìà Step 3: Generating Comprehensive Report\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Initialize variables to avoid UnboundLocalError\n",
    "    sorted_prompts = []\n",
    "    best_prompt = None\n",
    "    \n",
    "    # Find best performing prompt\n",
    "    if prompt_scores:\n",
    "        best_prompt = max(prompt_scores.items(), key=lambda x: x[1])\n",
    "        worst_prompt = min(prompt_scores.items(), key=lambda x: x[1])\n",
    "        \n",
    "        print(f\"üèÜ PERFORMANCE RANKING:\")\n",
    "        sorted_prompts = sorted(prompt_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "        for i, (prompt_name, score) in enumerate(sorted_prompts, 1):\n",
    "            print(f\"  {i}. {prompt_name}: {score:.3f}\")\n",
    "        \n",
    "        print(f\"\\nü•á Best Prompt: {best_prompt[0]} (Score: {best_prompt[1]:.3f})\")\n",
    "        print(f\"ü•â Worst Prompt: {worst_prompt[0]} (Score: {worst_prompt[1]:.3f})\")\n",
    "        print(f\"üìä Score Range: {worst_prompt[1]:.3f} - {best_prompt[1]:.3f}\")\n",
    "        print(f\"üìà Score Improvement: {((best_prompt[1] - worst_prompt[1]) / worst_prompt[1] * 100):.1f}%\")\n",
    "    else:\n",
    "        print(\"‚ùå No successful prompt processing to rank\")\n",
    "    \n",
    "    # Create comprehensive results\n",
    "    comprehensive_results = {\n",
    "        \"benchmark_metadata\": {\n",
    "            \"model_name\": model_name,\n",
    "            \"image_directory\": image_directory,\n",
    "            \"ground_truth_file\": ground_truth_file,\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"total_prompts_tested\": len(prompts_dict),\n",
    "            \"successful_prompts\": len(prompt_scores),\n",
    "            \"total_images\": len(image_files)\n",
    "        },\n",
    "        \"prompt_results\": prompt_results,\n",
    "        \"performance_ranking\": sorted_prompts if prompt_scores else [],\n",
    "        \"best_prompt\": {\n",
    "            \"name\": best_prompt[0] if best_prompt else None,\n",
    "            \"score\": best_prompt[1] if best_prompt else None\n",
    "        } if best_prompt else None,\n",
    "        \"summary_statistics\": {\n",
    "            \"prompt_scores\": prompt_scores,\n",
    "            \"average_score_across_prompts\": sum(prompt_scores.values()) / len(prompt_scores) if prompt_scores else 0,\n",
    "            \"score_std_deviation\": float(np.std(list(prompt_scores.values()))) if prompt_scores else 0\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Save comprehensive results to file\n",
    "    comprehensive_output_file = os.path.join(output_base_dir, f\"comprehensive_benchmark_{model_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\")\n",
    "    with open(comprehensive_output_file, 'w') as f:\n",
    "        json.dump(comprehensive_results, f, indent=2)\n",
    "    \n",
    "    print(f\"üíæ Comprehensive results saved to: {comprehensive_output_file}\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"üéâ BENCHMARK COMPLETED!\")\n",
    "    \n",
    "    return comprehensive_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18bff22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üõ†Ô∏è Helper functions created!\n",
      "\n",
      "Usage Examples:\n",
      "# Quick test with core prompts and Gemini:\n",
      "results = quick_benchmark('gemini', 'core_set')\n",
      "display_benchmark_summary(results)\n",
      "\n",
      "# Full custom benchmark:\n",
      "my_prompts = {'test1': 'Extract all text', 'test2': 'Extract prices only'}\n",
      "results = comprehensive_ocr_benchmark('gemini', my_prompts, '/path/to/images', '/path/to/ground_truth.json')\n"
     ]
    }
   ],
   "source": [
    "# Helper function to create test prompt sets\n",
    "def create_test_prompt_sets():\n",
    "    \"\"\"Create different sets of prompts for testing various scenarios.\"\"\"\n",
    "    \n",
    "    # Core prompts from the existing sample_prompts\n",
    "    core_prompts = {\n",
    "        \"cash_expense_v2\": sample_prompts[\"cash_expense_extraction_V2\"],\n",
    "        \"cash_expense_v3\": sample_prompts[\"cash_expense_extraction_V3\"],\n",
    "    }\n",
    "    \n",
    "    # Experimental prompts for testing variations\n",
    "    experimental_prompts = {\n",
    "        \"detailed_extraction\": \"\"\"\n",
    "You are an expert document processing assistant. Extract ALL financial data from this invoice/receipt image.\n",
    "Focus on accuracy and completeness. Return data in structured JSON format with these sections:\n",
    "- payment_details (supplier, date, method, reference)\n",
    "- item_details (all line items with quantities, prices, totals)\n",
    "- totals (subtotal, tax, discount, final total)\n",
    "- additional_info (any other relevant data)\n",
    "\n",
    "Be extremely precise with numbers and calculations. If text is in Bengali/Bangla, transliterate to English.\n",
    "Return ONLY valid JSON, no additional text.\n",
    "        \"\"\",\n",
    "        \n",
    "        \"simplified_extraction\": \"\"\"\n",
    "Extract key information from this receipt/invoice:\n",
    "1. Vendor/Store name\n",
    "2. Date\n",
    "3. Total amount\n",
    "4. Payment method\n",
    "5. List of items purchased\n",
    "\n",
    "Format as clean JSON. Focus on the most important information.\n",
    "        \"\"\",\n",
    "        \n",
    "        \"validation_focused\": \"\"\"\n",
    "Extract data from this financial document. Pay special attention to:\n",
    "- Mathematical accuracy (ensure totals match item sums)\n",
    "- Proper data types (numbers as numbers, dates as dates)\n",
    "- Complete item extraction (don't miss any line items)\n",
    "- Tax and discount calculations\n",
    "\n",
    "Return as structured JSON following this exact schema:\n",
    "{payment_details: {}, item_details: [], totals: {}}\n",
    "        \"\"\"\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        \"core_set\": core_prompts,\n",
    "        \"experimental_set\": experimental_prompts,\n",
    "        \"combined_set\": {**core_prompts, **experimental_prompts}\n",
    "    }\n",
    "\n",
    "# Helper function to run quick benchmark\n",
    "def quick_benchmark(model_name: str = \"gemini\", prompt_set: str = \"core_set\"):\n",
    "    \"\"\"\n",
    "    Run a quick benchmark with predefined settings.\n",
    "    \n",
    "    Args:\n",
    "        model_name: Model to use ('gemini', 'openai', 'anthropic')\n",
    "        prompt_set: Which prompt set to use ('core_set', 'experimental_set', 'combined_set')\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get prompts\n",
    "    prompt_sets = create_test_prompt_sets()\n",
    "    prompts_to_test = prompt_sets.get(prompt_set, prompt_sets[\"core_set\"])\n",
    "    \n",
    "    # Default paths\n",
    "    image_directory = \"/home/tanjim_noor/Work/AI OCR BenchMark/For benchmark\"\n",
    "    ground_truth_file = \"/home/tanjim_noor/Work/AI OCR BenchMark/OCR_Output/ocr_actual_1_structured_only.json\"\n",
    "    \n",
    "    print(f\"üöÄ Quick Benchmark: {model_name} with {prompt_set}\")\n",
    "    print(f\"Testing {len(prompts_to_test)} prompts on images from: {image_directory}\")\n",
    "    \n",
    "    # Run comprehensive benchmark\n",
    "    results = comprehensive_ocr_benchmark(\n",
    "        model_name=model_name,\n",
    "        prompts_dict=prompts_to_test,\n",
    "        image_directory=image_directory,\n",
    "        ground_truth_file=ground_truth_file\n",
    "    )\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Helper function to display benchmark results nicely\n",
    "def display_benchmark_summary(results: Dict[str, Any]):\n",
    "    \"\"\"Display a nice summary of benchmark results.\"\"\"\n",
    "    \n",
    "    if \"error\" in results:\n",
    "        print(f\"‚ùå Benchmark failed: {results['error']}\")\n",
    "        return\n",
    "    \n",
    "    metadata = results[\"benchmark_metadata\"]\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"üèÜ COMPREHENSIVE BENCHMARK SUMMARY\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    print(f\"üìä Model: {metadata['model_name']}\")\n",
    "    print(f\"üìÇ Images: {metadata['total_images']} total\")\n",
    "    print(f\"üìù Prompts: {metadata['total_prompts_tested']} tested, {metadata['successful_prompts']} successful\")\n",
    "    print(f\"‚è∞ Timestamp: {metadata['timestamp']}\")\n",
    "    \n",
    "    if results[\"best_prompt\"]:\n",
    "        print(f\"\\nü•á Best Performing Prompt:\")\n",
    "        print(f\"   Name: {results['best_prompt']['name']}\")\n",
    "        print(f\"   Score: {results['best_prompt']['score']:.3f}\")\n",
    "    \n",
    "    print(f\"\\nüìà Performance Ranking:\")\n",
    "    for i, (prompt_name, score) in enumerate(results[\"performance_ranking\"], 1):\n",
    "        print(f\"   {i}. {prompt_name}: {score:.3f}\")\n",
    "    \n",
    "    stats = results[\"summary_statistics\"]\n",
    "    print(f\"\\nüìä Summary Statistics:\")\n",
    "    print(f\"   Average Score: {stats['average_score_across_prompts']:.3f}\")\n",
    "    print(f\"   Score Std Dev: {stats['score_std_deviation']:.3f}\")\n",
    "    \n",
    "    print(f\"\\nüíæ Results Location: Check OCR_Output directory for detailed files\")\n",
    "\n",
    "print(\"üõ†Ô∏è Helper functions created!\")\n",
    "print(\"\\nUsage Examples:\")\n",
    "print(\"# Quick test with core prompts and Gemini:\")\n",
    "print(\"results = quick_benchmark('gemini', 'core_set')\")\n",
    "print(\"display_benchmark_summary(results)\")\n",
    "print(\"\\n# Full custom benchmark:\")\n",
    "print(\"my_prompts = {'test1': 'Extract all text', 'test2': 'Extract prices only'}\")\n",
    "print(\"results = comprehensive_ocr_benchmark('gemini', my_prompts, '/path/to/images', '/path/to/ground_truth.json')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d7d9a2",
   "metadata": {},
   "source": [
    "## üéØ Demo: Comprehensive Benchmark Usage\n",
    "\n",
    "This section demonstrates how to use the comprehensive benchmark function to test multiple prompts and identify the best performing one. The demo will:\n",
    "\n",
    "1. **Load multiple prompts** from predefined sets\n",
    "2. **Process all images** in the benchmark directory\n",
    "3. **Score results** against ground truth data\n",
    "4. **Generate a report** showing which prompt performs best\n",
    "\n",
    "Uncomment the code below to run a comprehensive benchmark test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89dccff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Running Quick Benchmark with Core Prompts...\n",
      "üöÄ Quick Benchmark: huggingface with core_set\n",
      "Testing 4 prompts on images from: /home/tanjim_noor/Work/AI OCR BenchMark/For benchmark\n",
      "üöÄ COMPREHENSIVE OCR BENCHMARK\n",
      "============================================================\n",
      "Model: huggingface\n",
      "Image Directory: /home/tanjim_noor/Work/AI OCR BenchMark/For benchmark\n",
      "Ground Truth: /home/tanjim_noor/Work/AI OCR BenchMark/OCR_Output/ocr_actual_1_structured_only.json\n",
      "Prompts to test: 4\n",
      "Output Directory: /home/tanjim_noor/Work/AI OCR BenchMark/OCR_Output\n",
      "============================================================\n",
      "‚úÖ OCR system initialized with huggingface\n",
      "üìÇ Found 10 images to process\n",
      "\n",
      "üîÑ Processing prompt: cash_expense_v1\n",
      "----------------------------------------\n",
      "üìù Step 1: OCR Processing with huggingface\n",
      "Found 10 image(s) to process...\n",
      "Processing image 1/10: IMG_20250826_154308~2.jpg\n",
      "üöÄ Starting image processing...\n",
      "üîç File extension: .jpg ‚Üí MIME type: image/jpeg\n",
      "üñºÔ∏è Optimizing image...\n",
      "üìè Original file size: 1.33 MB\n",
      "üìê Original dimensions: 3472x4624\n",
      "üîß HuggingFace aggressive resize by ratio 0.5: 1736x2312\n",
      "üíæ HuggingFace JPEG quality: 50\n",
      "‚úÖ HuggingFace optimized size: 0.17 MB\n",
      "üìù Encoding to base64...\n",
      "üìä Base64 size: 0.23 MB\n",
      "ü§ñ Sending to HUGGINGFACE...\n",
      "‚è±Ô∏è API call completed in 11.28 seconds\n",
      "‚úÖ Successfully processed IMG_20250826_154308~2.jpg\n",
      "üìä Extracted structured data format: json\n",
      "Processing image 2/10: IMG_20250826_154831.jpg\n",
      "üöÄ Starting image processing...\n",
      "üîç File extension: .jpg ‚Üí MIME type: image/jpeg\n",
      "üñºÔ∏è Optimizing image...\n",
      "üìè Original file size: 4.62 MB\n",
      "üìê Original dimensions: 4624x3472\n",
      "üîß HuggingFace aggressive resize by ratio 0.5: 2312x1736\n",
      "üíæ HuggingFace JPEG quality: 50\n",
      "‚úÖ HuggingFace optimized size: 0.20 MB\n",
      "üìù Encoding to base64...\n",
      "üìä Base64 size: 0.26 MB\n",
      "ü§ñ Sending to HUGGINGFACE...\n",
      "‚è±Ô∏è API call completed in 9.77 seconds\n",
      "‚úÖ Successfully processed IMG_20250826_154831.jpg\n",
      "üìä Extracted structured data format: json\n",
      "Processing image 3/10: IMG_20250826_154929.jpg\n",
      "üöÄ Starting image processing...\n",
      "üîç File extension: .jpg ‚Üí MIME type: image/jpeg\n",
      "üñºÔ∏è Optimizing image...\n",
      "üìè Original file size: 4.69 MB\n",
      "üìê Original dimensions: 4624x3472\n",
      "üîß HuggingFace aggressive resize by ratio 0.5: 2312x1736\n",
      "üíæ HuggingFace JPEG quality: 50\n",
      "‚úÖ HuggingFace optimized size: 0.40 MB\n",
      "üìù Encoding to base64...\n",
      "üìä Base64 size: 0.53 MB\n",
      "ü§ñ Sending to HUGGINGFACE...\n",
      "‚ùå Error during processing: Error code: 413 - {'error': 'request entity too large'}\n",
      "‚ùå Failed to process IMG_20250826_154929.jpg: Error code: 413 - {'error': 'request entity too large'}\n",
      "Processing image 4/10: IMG_20250826_155237.jpg\n",
      "üöÄ Starting image processing...\n",
      "üîç File extension: .jpg ‚Üí MIME type: image/jpeg\n",
      "üñºÔ∏è Optimizing image...\n",
      "üìè Original file size: 4.21 MB\n",
      "üìê Original dimensions: 4624x3472\n",
      "üîß HuggingFace aggressive resize by ratio 0.5: 2312x1736\n",
      "üíæ HuggingFace JPEG quality: 50\n",
      "‚úÖ HuggingFace optimized size: 0.32 MB\n",
      "üìù Encoding to base64...\n",
      "üìä Base64 size: 0.42 MB\n",
      "ü§ñ Sending to HUGGINGFACE...\n",
      "‚è±Ô∏è API call completed in 32.81 seconds\n",
      "‚úÖ Successfully processed IMG_20250826_155237.jpg\n",
      "üìä Extracted structured data format: json\n",
      "Processing image 5/10: IMG_20250826_155754.jpg\n",
      "üöÄ Starting image processing...\n",
      "üîç File extension: .jpg ‚Üí MIME type: image/jpeg\n",
      "üñºÔ∏è Optimizing image...\n",
      "üìè Original file size: 3.26 MB\n",
      "üìê Original dimensions: 4624x3472\n",
      "üîß HuggingFace aggressive resize by ratio 0.5: 2312x1736\n",
      "üíæ HuggingFace JPEG quality: 50\n",
      "‚úÖ HuggingFace optimized size: 0.19 MB\n",
      "üìù Encoding to base64...\n",
      "üìä Base64 size: 0.25 MB\n",
      "ü§ñ Sending to HUGGINGFACE...\n",
      "‚è±Ô∏è API call completed in 16.35 seconds\n",
      "‚úÖ Successfully processed IMG_20250826_155754.jpg\n",
      "üìä Extracted structured data format: json\n",
      "Processing image 6/10: IMG_20250826_160041.jpg\n",
      "üöÄ Starting image processing...\n",
      "üîç File extension: .jpg ‚Üí MIME type: image/jpeg\n",
      "üñºÔ∏è Optimizing image...\n",
      "üìè Original file size: 2.93 MB\n",
      "üìê Original dimensions: 4624x3472\n",
      "üîß HuggingFace aggressive resize by ratio 0.5: 2312x1736\n",
      "üíæ HuggingFace JPEG quality: 50\n",
      "‚úÖ HuggingFace optimized size: 0.18 MB\n",
      "üìù Encoding to base64...\n",
      "üìä Base64 size: 0.25 MB\n",
      "ü§ñ Sending to HUGGINGFACE...\n",
      "‚è±Ô∏è API call completed in 9.59 seconds\n",
      "‚úÖ Successfully processed IMG_20250826_160041.jpg\n",
      "üìä Extracted structured data format: json\n",
      "Processing image 7/10: IMG_20250826_160529.jpg\n",
      "üöÄ Starting image processing...\n",
      "üîç File extension: .jpg ‚Üí MIME type: image/jpeg\n",
      "üñºÔ∏è Optimizing image...\n",
      "üìè Original file size: 4.22 MB\n",
      "üìê Original dimensions: 4624x3472\n",
      "üîß HuggingFace aggressive resize by ratio 0.5: 2312x1736\n",
      "üíæ HuggingFace JPEG quality: 50\n",
      "‚úÖ HuggingFace optimized size: 0.32 MB\n",
      "üìù Encoding to base64...\n",
      "üìä Base64 size: 0.43 MB\n",
      "ü§ñ Sending to HUGGINGFACE...\n",
      "‚è±Ô∏è API call completed in 13.16 seconds\n",
      "‚úÖ Successfully processed IMG_20250826_160529.jpg\n",
      "üìä Extracted structured data format: json\n",
      "Processing image 8/10: IMG_20250826_160644.jpg\n",
      "üöÄ Starting image processing...\n",
      "üîç File extension: .jpg ‚Üí MIME type: image/jpeg\n",
      "üñºÔ∏è Optimizing image...\n",
      "üìè Original file size: 4.54 MB\n",
      "üìê Original dimensions: 4624x3472\n",
      "üîß HuggingFace aggressive resize by ratio 0.5: 2312x1736\n",
      "üíæ HuggingFace JPEG quality: 50\n",
      "‚úÖ HuggingFace optimized size: 0.22 MB\n",
      "üìù Encoding to base64...\n",
      "üìä Base64 size: 0.30 MB\n",
      "ü§ñ Sending to HUGGINGFACE...\n",
      "‚è±Ô∏è API call completed in 11.95 seconds\n",
      "‚úÖ Successfully processed IMG_20250826_160644.jpg\n",
      "üìä Extracted structured data format: json\n",
      "Processing image 9/10: IMG_20250826_160928.jpg\n",
      "üöÄ Starting image processing...\n",
      "üîç File extension: .jpg ‚Üí MIME type: image/jpeg\n",
      "üñºÔ∏è Optimizing image...\n",
      "üìè Original file size: 5.80 MB\n",
      "üìê Original dimensions: 4624x3472\n",
      "üîß HuggingFace aggressive resize by ratio 0.5: 2312x1736\n",
      "üíæ HuggingFace JPEG quality: 50\n",
      "‚úÖ HuggingFace optimized size: 0.33 MB\n",
      "üìù Encoding to base64...\n",
      "üìä Base64 size: 0.44 MB\n",
      "ü§ñ Sending to HUGGINGFACE...\n",
      "‚è±Ô∏è API call completed in 41.30 seconds\n",
      "‚úÖ Successfully processed IMG_20250826_160928.jpg\n",
      "üìä Extracted structured data format: json\n",
      "Processing image 10/10: IMG_20250826_160949.jpg\n",
      "üöÄ Starting image processing...\n",
      "üîç File extension: .jpg ‚Üí MIME type: image/jpeg\n",
      "üñºÔ∏è Optimizing image...\n",
      "üìè Original file size: 4.73 MB\n",
      "üìê Original dimensions: 4624x3472\n",
      "üîß HuggingFace aggressive resize by ratio 0.5: 2312x1736\n",
      "üíæ HuggingFace JPEG quality: 50\n",
      "‚úÖ HuggingFace optimized size: 0.24 MB\n",
      "üìù Encoding to base64...\n",
      "üìä Base64 size: 0.32 MB\n",
      "ü§ñ Sending to HUGGINGFACE...\n",
      "‚è±Ô∏è API call completed in 9.88 seconds\n",
      "‚úÖ Successfully processed IMG_20250826_160949.jpg\n",
      "üìä Extracted structured data format: json\n",
      "üìÑ Formatted results saved to: /home/tanjim_noor/Work/AI OCR BenchMark/OCR_Output/ocr_results_cash_expense_v1_huggingface_20250902_144006.json\n",
      "üìã Structured data summary saved to: /home/tanjim_noor/Work/AI OCR BenchMark/OCR_Output/ocr_results_cash_expense_v1_huggingface_20250902_144006_structured_only.json\n",
      "Results saved to /home/tanjim_noor/Work/AI OCR BenchMark/OCR_Output/ocr_results_cash_expense_v1_huggingface_20250902_144006.json\n",
      "‚úÖ OCR completed: 9/10 images processed successfully\n",
      "üìä Step 2: Scoring against ground truth\n",
      "‚ùå Error processing prompt cash_expense_v1: name 'score_all_images' is not defined\n",
      "\n",
      "üîÑ Processing prompt: cash_expense_v2\n",
      "----------------------------------------\n",
      "üìù Step 1: OCR Processing with huggingface\n",
      "Found 10 image(s) to process...\n",
      "Processing image 1/10: IMG_20250826_154308~2.jpg\n",
      "üöÄ Starting image processing...\n",
      "üîç File extension: .jpg ‚Üí MIME type: image/jpeg\n",
      "üñºÔ∏è Optimizing image...\n",
      "üìè Original file size: 1.33 MB\n",
      "üìê Original dimensions: 3472x4624\n",
      "üîß HuggingFace aggressive resize by ratio 0.5: 1736x2312\n",
      "üíæ HuggingFace JPEG quality: 50\n",
      "‚úÖ HuggingFace optimized size: 0.17 MB\n",
      "üìù Encoding to base64...\n",
      "üìä Base64 size: 0.23 MB\n",
      "ü§ñ Sending to HUGGINGFACE...\n",
      "‚è±Ô∏è API call completed in 12.77 seconds\n",
      "‚úÖ Successfully processed IMG_20250826_154308~2.jpg\n",
      "üìä Extracted structured data format: json\n",
      "Processing image 2/10: IMG_20250826_154831.jpg\n",
      "üöÄ Starting image processing...\n",
      "üîç File extension: .jpg ‚Üí MIME type: image/jpeg\n",
      "üñºÔ∏è Optimizing image...\n",
      "üìè Original file size: 4.62 MB\n",
      "üìê Original dimensions: 4624x3472\n",
      "üîß HuggingFace aggressive resize by ratio 0.5: 2312x1736\n",
      "üíæ HuggingFace JPEG quality: 50\n",
      "‚úÖ HuggingFace optimized size: 0.20 MB\n",
      "üìù Encoding to base64...\n",
      "üìä Base64 size: 0.26 MB\n",
      "ü§ñ Sending to HUGGINGFACE...\n",
      "‚è±Ô∏è API call completed in 9.27 seconds\n",
      "‚úÖ Successfully processed IMG_20250826_154831.jpg\n",
      "üìä Extracted structured data format: json\n",
      "Processing image 3/10: IMG_20250826_154929.jpg\n",
      "üöÄ Starting image processing...\n",
      "üîç File extension: .jpg ‚Üí MIME type: image/jpeg\n",
      "üñºÔ∏è Optimizing image...\n",
      "üìè Original file size: 4.69 MB\n",
      "üìê Original dimensions: 4624x3472\n",
      "üîß HuggingFace aggressive resize by ratio 0.5: 2312x1736\n",
      "üíæ HuggingFace JPEG quality: 50\n",
      "‚úÖ HuggingFace optimized size: 0.40 MB\n",
      "üìù Encoding to base64...\n",
      "üìä Base64 size: 0.53 MB\n",
      "ü§ñ Sending to HUGGINGFACE...\n",
      "‚ùå Error during processing: Error code: 413 - {'error': 'request entity too large'}\n",
      "‚ùå Failed to process IMG_20250826_154929.jpg: Error code: 413 - {'error': 'request entity too large'}\n",
      "Processing image 4/10: IMG_20250826_155237.jpg\n",
      "üöÄ Starting image processing...\n",
      "üîç File extension: .jpg ‚Üí MIME type: image/jpeg\n",
      "üñºÔ∏è Optimizing image...\n",
      "üìè Original file size: 4.21 MB\n",
      "üìê Original dimensions: 4624x3472\n",
      "üîß HuggingFace aggressive resize by ratio 0.5: 2312x1736\n",
      "üíæ HuggingFace JPEG quality: 50\n",
      "‚úÖ HuggingFace optimized size: 0.32 MB\n",
      "üìù Encoding to base64...\n",
      "üìä Base64 size: 0.42 MB\n",
      "ü§ñ Sending to HUGGINGFACE...\n",
      "‚è±Ô∏è API call completed in 34.11 seconds\n",
      "‚úÖ Successfully processed IMG_20250826_155237.jpg\n",
      "üìä Extracted structured data format: json\n",
      "Processing image 5/10: IMG_20250826_155754.jpg\n",
      "üöÄ Starting image processing...\n",
      "üîç File extension: .jpg ‚Üí MIME type: image/jpeg\n",
      "üñºÔ∏è Optimizing image...\n",
      "üìè Original file size: 3.26 MB\n",
      "üìê Original dimensions: 4624x3472\n",
      "üîß HuggingFace aggressive resize by ratio 0.5: 2312x1736\n",
      "üíæ HuggingFace JPEG quality: 50\n",
      "‚úÖ HuggingFace optimized size: 0.19 MB\n",
      "üìù Encoding to base64...\n",
      "üìä Base64 size: 0.25 MB\n",
      "ü§ñ Sending to HUGGINGFACE...\n",
      "‚è±Ô∏è API call completed in 15.22 seconds\n",
      "‚úÖ Successfully processed IMG_20250826_155754.jpg\n",
      "üìä Extracted structured data format: json\n",
      "Processing image 6/10: IMG_20250826_160041.jpg\n",
      "üöÄ Starting image processing...\n",
      "üîç File extension: .jpg ‚Üí MIME type: image/jpeg\n",
      "üñºÔ∏è Optimizing image...\n",
      "üìè Original file size: 2.93 MB\n",
      "üìê Original dimensions: 4624x3472\n",
      "üîß HuggingFace aggressive resize by ratio 0.5: 2312x1736\n",
      "üíæ HuggingFace JPEG quality: 50\n",
      "‚úÖ HuggingFace optimized size: 0.18 MB\n",
      "üìù Encoding to base64...\n",
      "üìä Base64 size: 0.25 MB\n",
      "ü§ñ Sending to HUGGINGFACE...\n",
      "‚è±Ô∏è API call completed in 10.31 seconds\n",
      "‚úÖ Successfully processed IMG_20250826_160041.jpg\n",
      "üìä Extracted structured data format: json\n",
      "Processing image 7/10: IMG_20250826_160529.jpg\n",
      "üöÄ Starting image processing...\n",
      "üîç File extension: .jpg ‚Üí MIME type: image/jpeg\n",
      "üñºÔ∏è Optimizing image...\n",
      "üìè Original file size: 4.22 MB\n",
      "üìê Original dimensions: 4624x3472\n",
      "üîß HuggingFace aggressive resize by ratio 0.5: 2312x1736\n",
      "üíæ HuggingFace JPEG quality: 50\n",
      "‚úÖ HuggingFace optimized size: 0.32 MB\n",
      "üìù Encoding to base64...\n",
      "üìä Base64 size: 0.43 MB\n",
      "ü§ñ Sending to HUGGINGFACE...\n",
      "‚è±Ô∏è API call completed in 14.50 seconds\n",
      "‚úÖ Successfully processed IMG_20250826_160529.jpg\n",
      "üìä Extracted structured data format: json\n",
      "Processing image 8/10: IMG_20250826_160644.jpg\n",
      "üöÄ Starting image processing...\n",
      "üîç File extension: .jpg ‚Üí MIME type: image/jpeg\n",
      "üñºÔ∏è Optimizing image...\n",
      "üìè Original file size: 4.54 MB\n",
      "üìê Original dimensions: 4624x3472\n",
      "üîß HuggingFace aggressive resize by ratio 0.5: 2312x1736\n",
      "üíæ HuggingFace JPEG quality: 50\n",
      "‚úÖ HuggingFace optimized size: 0.22 MB\n",
      "üìù Encoding to base64...\n",
      "üìä Base64 size: 0.30 MB\n",
      "ü§ñ Sending to HUGGINGFACE...\n",
      "‚è±Ô∏è API call completed in 10.82 seconds\n",
      "‚úÖ Successfully processed IMG_20250826_160644.jpg\n",
      "üìä Extracted structured data format: json\n",
      "Processing image 9/10: IMG_20250826_160928.jpg\n",
      "üöÄ Starting image processing...\n",
      "üîç File extension: .jpg ‚Üí MIME type: image/jpeg\n",
      "üñºÔ∏è Optimizing image...\n",
      "üìè Original file size: 5.80 MB\n",
      "üìê Original dimensions: 4624x3472\n",
      "üîß HuggingFace aggressive resize by ratio 0.5: 2312x1736\n",
      "üíæ HuggingFace JPEG quality: 50\n",
      "‚úÖ HuggingFace optimized size: 0.33 MB\n",
      "üìù Encoding to base64...\n",
      "üìä Base64 size: 0.44 MB\n",
      "ü§ñ Sending to HUGGINGFACE...\n",
      "‚è±Ô∏è API call completed in 39.99 seconds\n",
      "‚úÖ Successfully processed IMG_20250826_160928.jpg\n",
      "üìä Extracted structured data format: json\n",
      "Processing image 10/10: IMG_20250826_160949.jpg\n",
      "üöÄ Starting image processing...\n",
      "üîç File extension: .jpg ‚Üí MIME type: image/jpeg\n",
      "üñºÔ∏è Optimizing image...\n",
      "üìè Original file size: 4.73 MB\n",
      "üìê Original dimensions: 4624x3472\n",
      "üîß HuggingFace aggressive resize by ratio 0.5: 2312x1736\n",
      "üíæ HuggingFace JPEG quality: 50\n",
      "‚úÖ HuggingFace optimized size: 0.24 MB\n",
      "üìù Encoding to base64...\n",
      "üìä Base64 size: 0.32 MB\n",
      "ü§ñ Sending to HUGGINGFACE...\n",
      "‚è±Ô∏è API call completed in 10.90 seconds\n",
      "‚úÖ Successfully processed IMG_20250826_160949.jpg\n",
      "üìä Extracted structured data format: json\n",
      "üìÑ Formatted results saved to: /home/tanjim_noor/Work/AI OCR BenchMark/OCR_Output/ocr_results_cash_expense_v2_huggingface_20250902_144246.json\n",
      "üìã Structured data summary saved to: /home/tanjim_noor/Work/AI OCR BenchMark/OCR_Output/ocr_results_cash_expense_v2_huggingface_20250902_144246_structured_only.json\n",
      "Results saved to /home/tanjim_noor/Work/AI OCR BenchMark/OCR_Output/ocr_results_cash_expense_v2_huggingface_20250902_144246.json\n",
      "‚úÖ OCR completed: 9/10 images processed successfully\n",
      "üìä Step 2: Scoring against ground truth\n",
      "‚ùå Error processing prompt cash_expense_v2: name 'score_all_images' is not defined\n",
      "\n",
      "üîÑ Processing prompt: cash_expense_v3\n",
      "----------------------------------------\n",
      "üìù Step 1: OCR Processing with huggingface\n",
      "Found 10 image(s) to process...\n",
      "Processing image 1/10: IMG_20250826_154308~2.jpg\n",
      "üöÄ Starting image processing...\n",
      "üîç File extension: .jpg ‚Üí MIME type: image/jpeg\n",
      "üñºÔ∏è Optimizing image...\n",
      "üìè Original file size: 1.33 MB\n",
      "üìê Original dimensions: 3472x4624\n",
      "üîß HuggingFace aggressive resize by ratio 0.5: 1736x2312\n",
      "üíæ HuggingFace JPEG quality: 50\n",
      "‚úÖ HuggingFace optimized size: 0.17 MB\n",
      "üìù Encoding to base64...\n",
      "üìä Base64 size: 0.23 MB\n",
      "ü§ñ Sending to HUGGINGFACE...\n",
      "‚è±Ô∏è API call completed in 11.44 seconds\n",
      "‚úÖ Successfully processed IMG_20250826_154308~2.jpg\n",
      "üìä Extracted structured data format: json\n",
      "Processing image 2/10: IMG_20250826_154831.jpg\n",
      "üöÄ Starting image processing...\n",
      "üîç File extension: .jpg ‚Üí MIME type: image/jpeg\n",
      "üñºÔ∏è Optimizing image...\n",
      "üìè Original file size: 4.62 MB\n",
      "üìê Original dimensions: 4624x3472\n",
      "üîß HuggingFace aggressive resize by ratio 0.5: 2312x1736\n",
      "üíæ HuggingFace JPEG quality: 50\n",
      "‚úÖ HuggingFace optimized size: 0.20 MB\n",
      "üìù Encoding to base64...\n",
      "üìä Base64 size: 0.26 MB\n",
      "ü§ñ Sending to HUGGINGFACE...\n",
      "‚è±Ô∏è API call completed in 9.58 seconds\n",
      "‚úÖ Successfully processed IMG_20250826_154831.jpg\n",
      "üìä Extracted structured data format: json\n",
      "Processing image 3/10: IMG_20250826_154929.jpg\n",
      "üöÄ Starting image processing...\n",
      "üîç File extension: .jpg ‚Üí MIME type: image/jpeg\n",
      "üñºÔ∏è Optimizing image...\n",
      "üìè Original file size: 4.69 MB\n",
      "üìê Original dimensions: 4624x3472\n",
      "üîß HuggingFace aggressive resize by ratio 0.5: 2312x1736\n",
      "üíæ HuggingFace JPEG quality: 50\n",
      "‚úÖ HuggingFace optimized size: 0.40 MB\n",
      "üìù Encoding to base64...\n",
      "üìä Base64 size: 0.53 MB\n",
      "ü§ñ Sending to HUGGINGFACE...\n",
      "‚ùå Error during processing: Error code: 413 - {'error': 'request entity too large'}\n",
      "‚ùå Failed to process IMG_20250826_154929.jpg: Error code: 413 - {'error': 'request entity too large'}\n",
      "Processing image 4/10: IMG_20250826_155237.jpg\n",
      "üöÄ Starting image processing...\n",
      "üîç File extension: .jpg ‚Üí MIME type: image/jpeg\n",
      "üñºÔ∏è Optimizing image...\n",
      "üìè Original file size: 4.21 MB\n",
      "üìê Original dimensions: 4624x3472\n",
      "üîß HuggingFace aggressive resize by ratio 0.5: 2312x1736\n",
      "üíæ HuggingFace JPEG quality: 50\n",
      "‚úÖ HuggingFace optimized size: 0.32 MB\n",
      "üìù Encoding to base64...\n",
      "üìä Base64 size: 0.42 MB\n",
      "ü§ñ Sending to HUGGINGFACE...\n",
      "‚è±Ô∏è API call completed in 34.69 seconds\n",
      "‚úÖ Successfully processed IMG_20250826_155237.jpg\n",
      "üìä Extracted structured data format: json\n",
      "Processing image 5/10: IMG_20250826_155754.jpg\n",
      "üöÄ Starting image processing...\n",
      "üîç File extension: .jpg ‚Üí MIME type: image/jpeg\n",
      "üñºÔ∏è Optimizing image...\n",
      "üìè Original file size: 3.26 MB\n",
      "üìê Original dimensions: 4624x3472\n",
      "üîß HuggingFace aggressive resize by ratio 0.5: 2312x1736\n",
      "üíæ HuggingFace JPEG quality: 50\n",
      "‚úÖ HuggingFace optimized size: 0.19 MB\n",
      "üìù Encoding to base64...\n",
      "üìä Base64 size: 0.25 MB\n",
      "ü§ñ Sending to HUGGINGFACE...\n",
      "‚è±Ô∏è API call completed in 15.53 seconds\n",
      "‚úÖ Successfully processed IMG_20250826_155754.jpg\n",
      "üìä Extracted structured data format: json\n",
      "Processing image 6/10: IMG_20250826_160041.jpg\n",
      "üöÄ Starting image processing...\n",
      "üîç File extension: .jpg ‚Üí MIME type: image/jpeg\n",
      "üñºÔ∏è Optimizing image...\n",
      "üìè Original file size: 2.93 MB\n",
      "üìê Original dimensions: 4624x3472\n",
      "üîß HuggingFace aggressive resize by ratio 0.5: 2312x1736\n",
      "üíæ HuggingFace JPEG quality: 50\n",
      "‚úÖ HuggingFace optimized size: 0.18 MB\n",
      "üìù Encoding to base64...\n",
      "üìä Base64 size: 0.25 MB\n",
      "ü§ñ Sending to HUGGINGFACE...\n",
      "‚è±Ô∏è API call completed in 9.19 seconds\n",
      "‚úÖ Successfully processed IMG_20250826_160041.jpg\n",
      "üìä Extracted structured data format: json\n",
      "Processing image 7/10: IMG_20250826_160529.jpg\n",
      "üöÄ Starting image processing...\n",
      "üîç File extension: .jpg ‚Üí MIME type: image/jpeg\n",
      "üñºÔ∏è Optimizing image...\n",
      "üìè Original file size: 4.22 MB\n",
      "üìê Original dimensions: 4624x3472\n",
      "üîß HuggingFace aggressive resize by ratio 0.5: 2312x1736\n",
      "üíæ HuggingFace JPEG quality: 50\n",
      "‚úÖ HuggingFace optimized size: 0.32 MB\n",
      "üìù Encoding to base64...\n",
      "üìä Base64 size: 0.43 MB\n",
      "ü§ñ Sending to HUGGINGFACE...\n",
      "‚è±Ô∏è API call completed in 13.77 seconds\n",
      "‚úÖ Successfully processed IMG_20250826_160529.jpg\n",
      "üìä Extracted structured data format: json\n",
      "Processing image 8/10: IMG_20250826_160644.jpg\n",
      "üöÄ Starting image processing...\n",
      "üîç File extension: .jpg ‚Üí MIME type: image/jpeg\n",
      "üñºÔ∏è Optimizing image...\n",
      "üìè Original file size: 4.54 MB\n",
      "üìê Original dimensions: 4624x3472\n",
      "üîß HuggingFace aggressive resize by ratio 0.5: 2312x1736\n",
      "üíæ HuggingFace JPEG quality: 50\n",
      "‚úÖ HuggingFace optimized size: 0.22 MB\n",
      "üìù Encoding to base64...\n",
      "üìä Base64 size: 0.30 MB\n",
      "ü§ñ Sending to HUGGINGFACE...\n",
      "‚è±Ô∏è API call completed in 10.81 seconds\n",
      "‚úÖ Successfully processed IMG_20250826_160644.jpg\n",
      "üìä Extracted structured data format: json\n",
      "Processing image 9/10: IMG_20250826_160928.jpg\n",
      "üöÄ Starting image processing...\n",
      "üîç File extension: .jpg ‚Üí MIME type: image/jpeg\n",
      "üñºÔ∏è Optimizing image...\n",
      "üìè Original file size: 5.80 MB\n",
      "üìê Original dimensions: 4624x3472\n",
      "üîß HuggingFace aggressive resize by ratio 0.5: 2312x1736\n",
      "üíæ HuggingFace JPEG quality: 50\n",
      "‚úÖ HuggingFace optimized size: 0.33 MB\n",
      "üìù Encoding to base64...\n",
      "üìä Base64 size: 0.44 MB\n",
      "ü§ñ Sending to HUGGINGFACE...\n",
      "‚è±Ô∏è API call completed in 41.63 seconds\n",
      "‚úÖ Successfully processed IMG_20250826_160928.jpg\n",
      "üìä Extracted structured data format: json\n",
      "Processing image 10/10: IMG_20250826_160949.jpg\n",
      "üöÄ Starting image processing...\n",
      "üîç File extension: .jpg ‚Üí MIME type: image/jpeg\n",
      "üñºÔ∏è Optimizing image...\n",
      "üìè Original file size: 4.73 MB\n",
      "üìê Original dimensions: 4624x3472\n",
      "üîß HuggingFace aggressive resize by ratio 0.5: 2312x1736\n",
      "üíæ HuggingFace JPEG quality: 50\n",
      "‚úÖ HuggingFace optimized size: 0.24 MB\n",
      "üìù Encoding to base64...\n",
      "üìä Base64 size: 0.32 MB\n",
      "ü§ñ Sending to HUGGINGFACE...\n",
      "‚è±Ô∏è API call completed in 9.27 seconds\n",
      "‚úÖ Successfully processed IMG_20250826_160949.jpg\n",
      "üìä Extracted structured data format: json\n",
      "üìÑ Formatted results saved to: /home/tanjim_noor/Work/AI OCR BenchMark/OCR_Output/ocr_results_cash_expense_v3_huggingface_20250902_144527.json\n",
      "üìã Structured data summary saved to: /home/tanjim_noor/Work/AI OCR BenchMark/OCR_Output/ocr_results_cash_expense_v3_huggingface_20250902_144527_structured_only.json\n",
      "Results saved to /home/tanjim_noor/Work/AI OCR BenchMark/OCR_Output/ocr_results_cash_expense_v3_huggingface_20250902_144527.json\n",
      "‚úÖ OCR completed: 9/10 images processed successfully\n",
      "üìä Step 2: Scoring against ground truth\n",
      "‚ùå Error processing prompt cash_expense_v3: name 'score_all_images' is not defined\n",
      "\n",
      "üîÑ Processing prompt: cash_expense_v4\n",
      "----------------------------------------\n",
      "üìù Step 1: OCR Processing with huggingface\n",
      "Found 10 image(s) to process...\n",
      "Processing image 1/10: IMG_20250826_154308~2.jpg\n",
      "üöÄ Starting image processing...\n",
      "üîç File extension: .jpg ‚Üí MIME type: image/jpeg\n",
      "üñºÔ∏è Optimizing image...\n",
      "üìè Original file size: 1.33 MB\n",
      "üìê Original dimensions: 3472x4624\n",
      "üîß HuggingFace aggressive resize by ratio 0.5: 1736x2312\n",
      "üíæ HuggingFace JPEG quality: 50\n",
      "‚úÖ HuggingFace optimized size: 0.17 MB\n",
      "üìù Encoding to base64...\n",
      "üìä Base64 size: 0.23 MB\n",
      "ü§ñ Sending to HUGGINGFACE...\n",
      "‚è±Ô∏è API call completed in 18.41 seconds\n",
      "‚úÖ Successfully processed IMG_20250826_154308~2.jpg\n",
      "üìä Extracted structured data format: json\n",
      "Processing image 2/10: IMG_20250826_154831.jpg\n",
      "üöÄ Starting image processing...\n",
      "üîç File extension: .jpg ‚Üí MIME type: image/jpeg\n",
      "üñºÔ∏è Optimizing image...\n",
      "üìè Original file size: 4.62 MB\n",
      "üìê Original dimensions: 4624x3472\n",
      "üîß HuggingFace aggressive resize by ratio 0.5: 2312x1736\n",
      "üíæ HuggingFace JPEG quality: 50\n",
      "‚úÖ HuggingFace optimized size: 0.20 MB\n",
      "üìù Encoding to base64...\n",
      "üìä Base64 size: 0.26 MB\n",
      "ü§ñ Sending to HUGGINGFACE...\n",
      "‚è±Ô∏è API call completed in 8.46 seconds\n",
      "‚úÖ Successfully processed IMG_20250826_154831.jpg\n",
      "üìä Extracted structured data format: json\n",
      "Processing image 3/10: IMG_20250826_154929.jpg\n",
      "üöÄ Starting image processing...\n",
      "üîç File extension: .jpg ‚Üí MIME type: image/jpeg\n",
      "üñºÔ∏è Optimizing image...\n",
      "üìè Original file size: 4.69 MB\n",
      "üìê Original dimensions: 4624x3472\n",
      "üîß HuggingFace aggressive resize by ratio 0.5: 2312x1736\n",
      "üíæ HuggingFace JPEG quality: 50\n",
      "‚úÖ HuggingFace optimized size: 0.40 MB\n",
      "üìù Encoding to base64...\n",
      "üìä Base64 size: 0.53 MB\n",
      "ü§ñ Sending to HUGGINGFACE...\n",
      "‚ùå Error during processing: Error code: 413 - {'error': 'request entity too large'}\n",
      "‚ùå Failed to process IMG_20250826_154929.jpg: Error code: 413 - {'error': 'request entity too large'}\n",
      "Processing image 4/10: IMG_20250826_155237.jpg\n",
      "üöÄ Starting image processing...\n",
      "üîç File extension: .jpg ‚Üí MIME type: image/jpeg\n",
      "üñºÔ∏è Optimizing image...\n",
      "üìè Original file size: 4.21 MB\n",
      "üìê Original dimensions: 4624x3472\n",
      "üîß HuggingFace aggressive resize by ratio 0.5: 2312x1736\n",
      "üíæ HuggingFace JPEG quality: 50\n",
      "‚úÖ HuggingFace optimized size: 0.32 MB\n",
      "üìù Encoding to base64...\n",
      "üìä Base64 size: 0.42 MB\n",
      "ü§ñ Sending to HUGGINGFACE...\n",
      "‚è±Ô∏è API call completed in 32.50 seconds\n",
      "‚úÖ Successfully processed IMG_20250826_155237.jpg\n",
      "üìä Extracted structured data format: json\n",
      "Processing image 5/10: IMG_20250826_155754.jpg\n",
      "üöÄ Starting image processing...\n",
      "üîç File extension: .jpg ‚Üí MIME type: image/jpeg\n",
      "üñºÔ∏è Optimizing image...\n",
      "üìè Original file size: 3.26 MB\n",
      "üìê Original dimensions: 4624x3472\n",
      "üîß HuggingFace aggressive resize by ratio 0.5: 2312x1736\n",
      "üíæ HuggingFace JPEG quality: 50\n",
      "‚úÖ HuggingFace optimized size: 0.19 MB\n",
      "üìù Encoding to base64...\n",
      "üìä Base64 size: 0.25 MB\n",
      "ü§ñ Sending to HUGGINGFACE...\n",
      "‚è±Ô∏è API call completed in 23.98 seconds\n",
      "‚úÖ Successfully processed IMG_20250826_155754.jpg\n",
      "üìä Extracted structured data format: json\n",
      "Processing image 6/10: IMG_20250826_160041.jpg\n",
      "üöÄ Starting image processing...\n",
      "üîç File extension: .jpg ‚Üí MIME type: image/jpeg\n",
      "üñºÔ∏è Optimizing image...\n",
      "üìè Original file size: 2.93 MB\n",
      "üìê Original dimensions: 4624x3472\n",
      "üîß HuggingFace aggressive resize by ratio 0.5: 2312x1736\n",
      "üíæ HuggingFace JPEG quality: 50\n",
      "‚úÖ HuggingFace optimized size: 0.18 MB\n",
      "üìù Encoding to base64...\n",
      "üìä Base64 size: 0.25 MB\n",
      "ü§ñ Sending to HUGGINGFACE...\n",
      "‚è±Ô∏è API call completed in 8.83 seconds\n",
      "‚úÖ Successfully processed IMG_20250826_160041.jpg\n",
      "üìä Extracted structured data format: json\n",
      "Processing image 7/10: IMG_20250826_160529.jpg\n",
      "üöÄ Starting image processing...\n",
      "üîç File extension: .jpg ‚Üí MIME type: image/jpeg\n",
      "üñºÔ∏è Optimizing image...\n",
      "üìè Original file size: 4.22 MB\n",
      "üìê Original dimensions: 4624x3472\n",
      "üîß HuggingFace aggressive resize by ratio 0.5: 2312x1736\n",
      "üíæ HuggingFace JPEG quality: 50\n",
      "‚úÖ HuggingFace optimized size: 0.32 MB\n",
      "üìù Encoding to base64...\n",
      "üìä Base64 size: 0.43 MB\n",
      "ü§ñ Sending to HUGGINGFACE...\n",
      "‚è±Ô∏è API call completed in 13.57 seconds\n",
      "‚úÖ Successfully processed IMG_20250826_160529.jpg\n",
      "üìä Extracted structured data format: json\n",
      "Processing image 8/10: IMG_20250826_160644.jpg\n",
      "üöÄ Starting image processing...\n",
      "üîç File extension: .jpg ‚Üí MIME type: image/jpeg\n",
      "üñºÔ∏è Optimizing image...\n",
      "üìè Original file size: 4.54 MB\n",
      "üìê Original dimensions: 4624x3472\n",
      "üîß HuggingFace aggressive resize by ratio 0.5: 2312x1736\n",
      "üíæ HuggingFace JPEG quality: 50\n",
      "‚úÖ HuggingFace optimized size: 0.22 MB\n",
      "üìù Encoding to base64...\n",
      "üìä Base64 size: 0.30 MB\n",
      "ü§ñ Sending to HUGGINGFACE...\n",
      "‚ùå Error during processing: Error code: 402 - {'error': 'You have exceeded your monthly included credits for Inference Providers. Subscribe to PRO to get 20x more monthly included credits.'}\n",
      "‚ùå Failed to process IMG_20250826_160644.jpg: Error code: 402 - {'error': 'You have exceeded your monthly included credits for Inference Providers. Subscribe to PRO to get 20x more monthly included credits.'}\n",
      "Processing image 9/10: IMG_20250826_160928.jpg\n",
      "üöÄ Starting image processing...\n",
      "üîç File extension: .jpg ‚Üí MIME type: image/jpeg\n",
      "üñºÔ∏è Optimizing image...\n",
      "üìè Original file size: 5.80 MB\n",
      "üìê Original dimensions: 4624x3472\n",
      "üîß HuggingFace aggressive resize by ratio 0.5: 2312x1736\n",
      "üíæ HuggingFace JPEG quality: 50\n",
      "‚úÖ HuggingFace optimized size: 0.33 MB\n",
      "üìù Encoding to base64...\n",
      "üìä Base64 size: 0.44 MB\n",
      "ü§ñ Sending to HUGGINGFACE...\n",
      "‚ùå Error during processing: Error code: 402 - {'error': 'You have exceeded your monthly included credits for Inference Providers. Subscribe to PRO to get 20x more monthly included credits.'}\n",
      "‚ùå Failed to process IMG_20250826_160928.jpg: Error code: 402 - {'error': 'You have exceeded your monthly included credits for Inference Providers. Subscribe to PRO to get 20x more monthly included credits.'}\n",
      "Processing image 10/10: IMG_20250826_160949.jpg\n",
      "üöÄ Starting image processing...\n",
      "üîç File extension: .jpg ‚Üí MIME type: image/jpeg\n",
      "üñºÔ∏è Optimizing image...\n",
      "üìè Original file size: 4.73 MB\n",
      "üìê Original dimensions: 4624x3472\n",
      "üîß HuggingFace aggressive resize by ratio 0.5: 2312x1736\n",
      "üíæ HuggingFace JPEG quality: 50\n",
      "‚úÖ HuggingFace optimized size: 0.24 MB\n",
      "üìù Encoding to base64...\n",
      "üìä Base64 size: 0.32 MB\n",
      "ü§ñ Sending to HUGGINGFACE...\n",
      "‚ùå Error during processing: Error code: 402 - {'error': 'You have exceeded your monthly included credits for Inference Providers. Subscribe to PRO to get 20x more monthly included credits.'}\n",
      "‚ùå Failed to process IMG_20250826_160949.jpg: Error code: 402 - {'error': 'You have exceeded your monthly included credits for Inference Providers. Subscribe to PRO to get 20x more monthly included credits.'}\n",
      "üìÑ Formatted results saved to: /home/tanjim_noor/Work/AI OCR BenchMark/OCR_Output/ocr_results_cash_expense_v4_huggingface_20250902_144807.json\n",
      "üìã Structured data summary saved to: /home/tanjim_noor/Work/AI OCR BenchMark/OCR_Output/ocr_results_cash_expense_v4_huggingface_20250902_144807_structured_only.json\n",
      "Results saved to /home/tanjim_noor/Work/AI OCR BenchMark/OCR_Output/ocr_results_cash_expense_v4_huggingface_20250902_144807.json\n",
      "‚úÖ OCR completed: 6/10 images processed successfully\n",
      "üìä Step 2: Scoring against ground truth\n",
      "‚ùå Error processing prompt cash_expense_v4: name 'score_all_images' is not defined\n",
      "\n",
      "üìà Step 3: Generating Comprehensive Report\n",
      "============================================================\n",
      "‚ùå No successful prompt processing to rank\n",
      "üíæ Comprehensive results saved to: /home/tanjim_noor/Work/AI OCR BenchMark/OCR_Output/comprehensive_benchmark_huggingface_20250902_145000.json\n",
      "============================================================\n",
      "üéâ BENCHMARK COMPLETED!\n",
      "\n",
      "================================================================================\n",
      "üèÜ COMPREHENSIVE BENCHMARK SUMMARY\n",
      "================================================================================\n",
      "üìä Model: huggingface\n",
      "üìÇ Images: 10 total\n",
      "üìù Prompts: 4 tested, 0 successful\n",
      "‚è∞ Timestamp: 2025-09-02T14:50:00.212975\n",
      "\n",
      "üìà Performance Ranking:\n",
      "\n",
      "üìä Summary Statistics:\n",
      "   Average Score: 0.000\n",
      "   Score Std Dev: 0.000\n",
      "\n",
      "üíæ Results Location: Check OCR_Output directory for detailed files\n",
      "üîç Demo code ready!\n",
      "Uncomment the sections above to run different types of benchmarks:\n",
      "  - Example 1: Quick test with predefined prompts\n",
      "  - Example 2: Custom comprehensive benchmark\n",
      "  - Example 3: Multi-model comparison\n",
      "\n",
      "‚ö†Ô∏è Note: Each benchmark processes all images and makes API calls.\n",
      "Make sure you have sufficient API credits before running extensive tests.\n"
     ]
    }
   ],
   "source": [
    "# Demo: Comprehensive OCR Benchmark\n",
    "# Uncomment the code below to run a full benchmark test\n",
    "\n",
    "\n",
    "# Example 1: Quick benchmark with core prompts\n",
    "print(\"üöÄ Running Quick Benchmark with Core Prompts...\")\n",
    "results = quick_benchmark(model_name=\"huggingface\", prompt_set=\"core_set\")\n",
    "display_benchmark_summary(results)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# Example 2: Custom comprehensive benchmark\n",
    "print(\"üîß Running Custom Comprehensive Benchmark...\")\n",
    "\n",
    "# Define custom prompts to test\n",
    "test_prompts = {\n",
    "    \"v1_basic\": sample_prompts[\"cash_expense_extraction\"],\n",
    "    \"v2_enhanced\": sample_prompts[\"cash_expense_extraction_V2\"],\n",
    "    \"general\": sample_prompts[\"general_ocr\"],\n",
    "    \"structured\": sample_prompts[\"structured_data\"]\n",
    "}\n",
    "\n",
    "# Set paths\n",
    "image_dir = \"/home/tanjim_noor/Work/AI OCR BenchMark/For benchmark\"\n",
    "ground_truth = \"/home/tanjim_noor/Work/AI OCR BenchMark/OCR_Output/ocr_actual_1_structured_only.json\"\n",
    "\n",
    "# Run comprehensive benchmark\n",
    "results = comprehensive_ocr_benchmark(\n",
    "    model_name=\"gemini\",\n",
    "    prompts_dict=test_prompts,\n",
    "    image_directory=image_dir,\n",
    "    ground_truth_file=ground_truth,\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "# Display results\n",
    "display_benchmark_summary(results)\n",
    "\n",
    "# Optionally, access detailed results\n",
    "if \"prompt_results\" in results:\n",
    "    print(\"\\nüìä Detailed Results:\")\n",
    "    for prompt_name, prompt_data in results[\"prompt_results\"].items():\n",
    "        if \"statistics\" in prompt_data:\n",
    "            stats = prompt_data[\"statistics\"]\n",
    "            print(f\"  {prompt_name}:\")\n",
    "            print(f\"    OCR Success: {stats['successful_ocr']}/{stats['total_images']}\")\n",
    "            print(f\"    Scored Images: {stats['processed_for_scoring']}\")\n",
    "            print(f\"    Average Score: {stats['average_score']:.3f}\")\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "# Example 3: Compare multiple models on same prompts\n",
    "print(\"üî¨ Multi-Model Comparison...\")\n",
    "\n",
    "models_to_test = [\"gemini\"]  # Add \"openai\", \"anthropic\" if you have API keys\n",
    "test_prompts = {\n",
    "    \"cash_v2\": sample_prompts[\"cash_expense_extraction_V2\"],\n",
    "    \"general\": sample_prompts[\"general_ocr\"]\n",
    "}\n",
    "\n",
    "model_comparison = {}\n",
    "\n",
    "for model in models_to_test:\n",
    "    print(f\"\\nü§ñ Testing {model}...\")\n",
    "    try:\n",
    "        model_results = comprehensive_ocr_benchmark(\n",
    "            model_name=model,\n",
    "            prompts_dict=test_prompts,\n",
    "            image_directory=\"/home/tanjim_noor/Work/AI OCR BenchMark/For benchmark\",\n",
    "            ground_truth_file=\"/home/tanjim_noor/Work/AI OCR BenchMark/OCR_Output/ocr_actual_1_structured_only.json\"\n",
    "        )\n",
    "        model_comparison[model] = model_results\n",
    "        \n",
    "        if model_results.get(\"best_prompt\"):\n",
    "            best = model_results[\"best_prompt\"]\n",
    "            print(f\"   Best prompt for {model}: {best['name']} (Score: {best['score']:.3f})\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå {model} failed: {e}\")\n",
    "        model_comparison[model] = {\"error\": str(e)}\n",
    "\n",
    "# Summary across models\n",
    "print(\"\\nüèÜ MULTI-MODEL SUMMARY:\")\n",
    "for model, results in model_comparison.items():\n",
    "    if \"error\" not in results and results.get(\"best_prompt\"):\n",
    "        best = results[\"best_prompt\"]\n",
    "        avg_score = results[\"summary_statistics\"][\"average_score_across_prompts\"]\n",
    "        print(f\"  {model}: Best = {best['name']} ({best['score']:.3f}), Avg = {avg_score:.3f}\")\n",
    "    else:\n",
    "        print(f\"  {model}: Failed or no results\")\n",
    "\"\"\"\n",
    "\n",
    "print(\"üîç Demo code ready!\")\n",
    "print(\"Uncomment the sections above to run different types of benchmarks:\")\n",
    "print(\"  - Example 1: Quick test with predefined prompts\")\n",
    "print(\"  - Example 2: Custom comprehensive benchmark\") \n",
    "print(\"  - Example 3: Multi-model comparison\")\n",
    "print(\"\\n‚ö†Ô∏è Note: Each benchmark processes all images and makes API calls.\")\n",
    "print(\"Make sure you have sufficient API credits before running extensive tests.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bebc5ab",
   "metadata": {},
   "source": [
    "#  Implementation Summary & Quick Reference\n",
    "\n",
    "## ‚úÖ **What We've Implemented**\n",
    "\n",
    "###  **Comprehensive OCR Benchmark Function**\n",
    "A complete all-rounder function that implements your requested workflow:\n",
    "\n",
    "**Model + Prompts + Image Directory ‚Üí OCR Processing ‚Üí Scoring ‚Üí Report Generation**\n",
    "\n",
    "### üîß **Key Features Delivered:**\n",
    "\n",
    "1. **`comprehensive_ocr_benchmark()`** - Main function that:\n",
    "   - ‚úÖ Takes model, prompts dict, and image directory as input\n",
    "   - ‚úÖ Processes all images in batches with each prompt\n",
    "   - ‚úÖ Outputs results to JSON files (structured format)\n",
    "   - ‚úÖ Automatically scores against ground truth data\n",
    "   - ‚úÖ Generates comprehensive performance reports\n",
    "   - ‚úÖ Identifies best performing prompt\n",
    "\n",
    "2. **Helper Functions:**\n",
    "   - ‚úÖ `quick_benchmark()` - Easy testing with predefined prompts\n",
    "   - ‚úÖ `create_test_prompt_sets()` - Predefined prompt collections\n",
    "   - ‚úÖ `display_benchmark_summary()` - Pretty result display\n",
    "\n",
    "3. **Multi-Prompt Testing:**\n",
    "   - ‚úÖ Core prompts (existing cash_expense_extraction variants)\n",
    "   - ‚úÖ Experimental prompts (detailed, simplified, validation-focused)\n",
    "   - ‚úÖ Custom prompt support\n",
    "\n",
    "## **Quick Usage Guide**\n",
    "\n",
    "### **Simple Test:**\n",
    "```python\n",
    "# Quick test with predefined prompts\n",
    "results = quick_benchmark(\"gemini\", \"core_set\")\n",
    "display_benchmark_summary(results)\n",
    "```\n",
    "\n",
    "### **Custom Comprehensive Test:**\n",
    "```python\n",
    "# Define your prompts\n",
    "my_prompts = {\n",
    "    \"prompt1\": \"Extract all invoice data...\",\n",
    "    \"prompt2\": \"Focus on total amounts...\",\n",
    "    \"prompt3\": \"Extract supplier info...\"\n",
    "}\n",
    "\n",
    "# Run comprehensive benchmark\n",
    "results = comprehensive_ocr_benchmark(\n",
    "    model_name=\"gemini\",\n",
    "    prompts_dict=my_prompts,\n",
    "    image_directory=\"/path/to/images\",\n",
    "    ground_truth_file=\"/path/to/ground_truth.json\"\n",
    ")\n",
    "\n",
    "# View results\n",
    "display_benchmark_summary(results)\n",
    "```\n",
    "\n",
    "### **Multi-Model Comparison:**\n",
    "```python\n",
    "models = [\"gemini\", \"openai\", \"anthropic\"]\n",
    "for model in models:\n",
    "    results = comprehensive_ocr_benchmark(model, my_prompts, ...)\n",
    "    print(f\"{model} best: {results['best_prompt']['name']}\")\n",
    "```\n",
    "\n",
    "## üìä **Output Files Generated:**\n",
    "- `ocr_results_{prompt}_{model}_{timestamp}.json` - Full OCR results\n",
    "- `ocr_results_{prompt}_{model}_{timestamp}_structured_only.json` - Structured data only\n",
    "- `comprehensive_benchmark_{model}_{timestamp}.json` - Complete benchmark report\n",
    "\n",
    "## üèÜ **Report Contents:**\n",
    "- **Performance Ranking** - Prompts sorted by score\n",
    "- **Best Prompt Identification** - Highest scoring prompt\n",
    "- **Statistical Analysis** - Average scores, standard deviation\n",
    "- **Detailed Breakdown** - Per-image, per-field scoring\n",
    "- **Success Rates** - OCR processing and scoring success rates\n",
    "\n",
    "## üîÑ **Complete Workflow Implemented:**\n",
    "1. **Input:** Model + Prompt Set + Image Directory\n",
    "2. **Processing:** Batch OCR for each prompt\n",
    "3. **Scoring:** Compare with ground truth using multiple metrics\n",
    "4. **Analysis:** Statistical analysis and ranking\n",
    "5. **Output:** Comprehensive report identifying best prompt\n",
    "\n",
    "**‚úÖ Your requested flow has been fully implemented and is ready to use!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
