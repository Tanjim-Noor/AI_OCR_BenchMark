{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "04918598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All required libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import base64\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional, Union\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "# LangChain imports\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "# For image processing\n",
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "print(\"All required libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "938319a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OCRBenchmark class defined successfully!\n",
      "üîß Global settings: Optimization=False, Resize ratio=0.5\n"
     ]
    }
   ],
   "source": [
    "# Global Configuration Variables for Image Processing\n",
    "# Set to False to disable image resizing/optimization\n",
    "ENABLE_IMAGE_OPTIMIZATION = False\n",
    "# Resize ratio (0.5 = 50% of original size, 1.0 = no resize)\n",
    "IMAGE_RESIZE_RATIO = 0.5\n",
    "MAX_IMAGE_DIMENSION = 1024  # Maximum width or height in pixels\n",
    "MAX_FILE_SIZE_MB = 4  # Maximum file size in MB before optimization\n",
    "JPEG_QUALITY = 85  # JPEG compression quality (1-100)\n",
    "\n",
    "# JSON Parsing and Formatting Utilities\n",
    "def parse_json_from_text(text: str):\n",
    "    \"\"\"\n",
    "    Extract and parse JSON from text that might contain markdown code blocks or other formatting.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # First try to parse directly\n",
    "        return json.loads(text)\n",
    "    except json.JSONDecodeError:\n",
    "        pass\n",
    "    \n",
    "    # Try to extract JSON from markdown code blocks\n",
    "    json_patterns = [\n",
    "        r'```json\\s*(\\{.*?\\})\\s*```',  # ```json ... ```\n",
    "        r'```\\s*(\\{.*?\\})\\s*```',      # ``` ... ```\n",
    "        r'(\\{.*?\\})',                   # Just the JSON object\n",
    "    ]\n",
    "    \n",
    "    for pattern in json_patterns:\n",
    "        matches = re.findall(pattern, text, re.DOTALL)\n",
    "        for match in matches:\n",
    "            try:\n",
    "                return json.loads(match.strip())\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "    \n",
    "    # If all parsing fails, return the original text\n",
    "    return text\n",
    "\n",
    "def format_extraction_result(result: Dict) -> Dict:\n",
    "    \"\"\"\n",
    "    Format the extraction result with parsed JSON data for better readability.\n",
    "    \"\"\"\n",
    "    if result.get(\"success\") and \"extracted_data\" in result:\n",
    "        try:\n",
    "            # Try to parse the extracted data as JSON\n",
    "            parsed_data = parse_json_from_text(result[\"extracted_data\"])\n",
    "            \n",
    "            # If successfully parsed, add it as a separate field\n",
    "            if isinstance(parsed_data, (dict, list)):\n",
    "                result[\"extracted_data_parsed\"] = parsed_data\n",
    "                result[\"extraction_format\"] = \"json\"\n",
    "            else:\n",
    "                result[\"extraction_format\"] = \"text\"\n",
    "                \n",
    "        except Exception as e:\n",
    "            result[\"extraction_format\"] = \"text\"\n",
    "            result[\"parsing_error\"] = str(e)\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "class OCRBenchmark:\n",
    "    \"\"\"\n",
    "    A model-agnostic OCR system using LangChain that can extract data from images\n",
    "    using different AI models (Gemini, OpenAI GPT-4V, Claude).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_name: str = \"gemini\", temperature: float = 0.1):\n",
    "        \"\"\"\n",
    "        Initialize the OCR Benchmark system.\n",
    "\n",
    "        Args:\n",
    "            model_name: The model to use (\"gemini\", \"openai\", \"anthropic\")\n",
    "            temperature: Temperature for model generation\n",
    "        \"\"\"\n",
    "        # Load environment variables\n",
    "        load_dotenv()\n",
    "\n",
    "        self.model_name = model_name\n",
    "        self.temperature = temperature\n",
    "        self.model = None\n",
    "\n",
    "        # Supported image formats\n",
    "        self.supported_formats = {'.png', '.jpg', '.jpeg', '.webp', '.gif'}\n",
    "\n",
    "        # MIME type mapping for correct image format handling\n",
    "        self.mime_type_map = {\n",
    "            '.jpg': 'jpeg',\n",
    "            '.jpeg': 'jpeg',\n",
    "            '.png': 'png',\n",
    "            '.webp': 'webp',\n",
    "            '.gif': 'gif'\n",
    "        }\n",
    "\n",
    "        # Image optimization settings (now using global variables)\n",
    "        self.enable_optimization = ENABLE_IMAGE_OPTIMIZATION\n",
    "        self.resize_ratio = IMAGE_RESIZE_RATIO\n",
    "        self.max_image_size = (MAX_IMAGE_DIMENSION, MAX_IMAGE_DIMENSION)\n",
    "        self.max_file_size_mb = MAX_FILE_SIZE_MB\n",
    "        self.jpeg_quality = JPEG_QUALITY\n",
    "\n",
    "        # Try to initialize model, but don't fail if API key is missing\n",
    "        try:\n",
    "            self.model = self._initialize_model()\n",
    "        except ValueError as e:\n",
    "            print(f\"‚ö†Ô∏è Warning: {e}\")\n",
    "            print(\n",
    "                f\"Model '{model_name}' will be initialized when needed if API key becomes available.\")\n",
    "\n",
    "    def _initialize_model(self):\n",
    "        \"\"\"Initialize the specified model.\"\"\"\n",
    "        if self.model_name.lower() == \"gemini\":\n",
    "            api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "            if not api_key or api_key == \"your_google_api_key_here\":\n",
    "                raise ValueError(\n",
    "                    \"GOOGLE_API_KEY not found or not set. Please set your GOOGLE_API_KEY in the .env file\")\n",
    "            return ChatGoogleGenerativeAI(\n",
    "                model=\"gemini-2.5-flash\",  # Updated to newer model\n",
    "                temperature=self.temperature,\n",
    "                google_api_key=api_key,\n",
    "                timeout=60  # Set reasonable timeout\n",
    "            )\n",
    "\n",
    "        elif self.model_name.lower() == \"openai\":\n",
    "            api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "            if not api_key or api_key == \"your_openai_api_key_here\":\n",
    "                raise ValueError(\n",
    "                    \"OPENAI_API_KEY not found or not set. Please set your OPENAI_API_KEY in the .env file\")\n",
    "            return ChatOpenAI(\n",
    "                model=\"gpt-4o\",\n",
    "                temperature=self.temperature,\n",
    "                timeout=60\n",
    "            )\n",
    "\n",
    "        elif self.model_name.lower() == \"anthropic\":\n",
    "            return ChatAnthropic(\n",
    "                model_name=\"claude-3-5-sonnet-20241022\",\n",
    "                temperature=self.temperature,\n",
    "                timeout=60,\n",
    "                stop=None\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported model: {self.model_name}\")\n",
    "\n",
    "    def _optimize_image(self, image_path: str) -> bytes:\n",
    "        \"\"\"Optimize image size and quality for API processing.\"\"\"\n",
    "        try:\n",
    "            # Check if optimization is enabled\n",
    "            if not self.enable_optimization:\n",
    "                print(f\"üîß Image optimization disabled - using original file\")\n",
    "                with open(image_path, \"rb\") as f:\n",
    "                    return f.read()\n",
    "\n",
    "            # Check file size first\n",
    "            file_size_mb = os.path.getsize(image_path) / (1024 * 1024)\n",
    "            print(f\"üìè Original file size: {file_size_mb:.2f} MB\")\n",
    "\n",
    "            with Image.open(image_path) as img:\n",
    "                # Convert to RGB if necessary\n",
    "                if img.mode in ('RGBA', 'P'):\n",
    "                    img = img.convert('RGB')\n",
    "\n",
    "                original_size = img.size\n",
    "                print(\n",
    "                    f\"üìê Original dimensions: {original_size[0]}x{original_size[1]}\")\n",
    "\n",
    "                # Apply resize ratio if specified\n",
    "                if self.resize_ratio != 1.0:\n",
    "                    new_width = int(img.size[0] * self.resize_ratio)\n",
    "                    new_height = int(img.size[1] * self.resize_ratio)\n",
    "                    img = img.resize((new_width, new_height),\n",
    "                                     Image.Resampling.LANCZOS)\n",
    "                    print(\n",
    "                        f\"üîß Resized by ratio {self.resize_ratio}: {img.size[0]}x{img.size[1]}\")\n",
    "\n",
    "                # Additional resize if still too large\n",
    "                elif (img.size[0] > self.max_image_size[0] or\n",
    "                      img.size[1] > self.max_image_size[1] or\n",
    "                      file_size_mb > self.max_file_size_mb):\n",
    "\n",
    "                    img.thumbnail(self.max_image_size,\n",
    "                                  Image.Resampling.LANCZOS)\n",
    "                    print(\n",
    "                        f\"üîß Thumbnail resize to: {img.size[0]}x{img.size[1]}\")\n",
    "\n",
    "                # Save to bytes with optimized quality\n",
    "                img_byte_arr = io.BytesIO()\n",
    "\n",
    "                # Choose format and quality based on original\n",
    "                if Path(image_path).suffix.lower() in ['.jpg', '.jpeg']:\n",
    "                    img.save(img_byte_arr, format='JPEG',\n",
    "                             quality=self.jpeg_quality, optimize=True)\n",
    "                    print(f\"üíæ JPEG quality: {self.jpeg_quality}\")\n",
    "                else:\n",
    "                    img.save(img_byte_arr, format='PNG', optimize=True)\n",
    "                    print(f\"üíæ PNG optimized\")\n",
    "\n",
    "                optimized_data = img_byte_arr.getvalue()\n",
    "                optimized_size_mb = len(optimized_data) / (1024 * 1024)\n",
    "                print(f\"‚úÖ Final optimized size: {optimized_size_mb:.2f} MB\")\n",
    "\n",
    "                return optimized_data\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Image optimization failed, using original: {e}\")\n",
    "            # Fallback to original file\n",
    "            with open(image_path, \"rb\") as f:\n",
    "                return f.read()\n",
    "\n",
    "    def _ensure_model_initialized(self):\n",
    "        \"\"\"Ensure model is initialized before use.\"\"\"\n",
    "        if self.model is None:\n",
    "            try:\n",
    "                self.model = self._initialize_model()\n",
    "            except ValueError as e:\n",
    "                raise ValueError(\n",
    "                    f\"Cannot initialize model '{self.model_name}': {e}\")\n",
    "\n",
    "    def encode_image(self, image_path: str) -> str:\n",
    "        \"\"\"Encode image to base64 string.\"\"\"\n",
    "        with open(image_path, \"rb\") as image_file:\n",
    "            return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "    def extract_data_from_image(self, image_path: str, prompt: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Extract data from a single image using the specified prompt.\n",
    "\n",
    "        Args:\n",
    "            image_path: Path to the image file\n",
    "            prompt: Custom prompt for data extraction\n",
    "\n",
    "        Returns:\n",
    "            Dictionary containing the extracted data and metadata\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(f\"üöÄ Starting image processing...\")\n",
    "\n",
    "            # Ensure model is initialized\n",
    "            self._ensure_model_initialized()\n",
    "\n",
    "            # Validate image file\n",
    "            if not os.path.exists(image_path):\n",
    "                return {\n",
    "                    \"error\": f\"Image file not found: {image_path}\",\n",
    "                    \"success\": False\n",
    "                }\n",
    "\n",
    "            # Check file extension\n",
    "            file_ext = Path(image_path).suffix.lower()\n",
    "            if file_ext not in self.supported_formats:\n",
    "                return {\n",
    "                    \"error\": f\"Unsupported image format: {file_ext}\",\n",
    "                    \"success\": False\n",
    "                }\n",
    "\n",
    "            # Get correct MIME type\n",
    "            mime_type = self.mime_type_map.get(file_ext, file_ext[1:])\n",
    "            print(\n",
    "                f\"üîç File extension: {file_ext} ‚Üí MIME type: image/{mime_type}\")\n",
    "\n",
    "            # Optimize image\n",
    "            print(f\"üñºÔ∏è Optimizing image...\")\n",
    "            image_data = self._optimize_image(image_path)\n",
    "\n",
    "            # Encode to base64\n",
    "            print(f\"üìù Encoding to base64...\")\n",
    "            base64_image = base64.b64encode(image_data).decode()\n",
    "            base64_size_mb = len(base64_image) / (1024 * 1024)\n",
    "            print(f\"üìä Base64 size: {base64_size_mb:.2f} MB\")\n",
    "\n",
    "            # Construct data URL\n",
    "            data_url = f\"data:image/{mime_type};base64,{base64_image}\"\n",
    "\n",
    "            # Prepare message\n",
    "            print(f\"ü§ñ Sending to {self.model_name.upper()}...\")\n",
    "            message = HumanMessage(\n",
    "                content=[\n",
    "                    {\"type\": \"text\", \"text\": prompt},\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\"url\": data_url}\n",
    "                    }\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            # Get response with timeout handling\n",
    "            if self.model is not None:\n",
    "                import time\n",
    "                start_time = time.time()\n",
    "                response = self.model.invoke([message])\n",
    "                end_time = time.time()\n",
    "                print(f\"‚è±Ô∏è API call completed in {end_time - start_time:.2f} seconds\")\n",
    "            else:\n",
    "                raise ValueError(\"Model not initialized\")\n",
    "\n",
    "            result = {\n",
    "                \"image_path\": image_path,\n",
    "                \"model_used\": self.model_name,\n",
    "                \"prompt\": prompt,\n",
    "                \"extracted_data\": response.content,\n",
    "                \"success\": True,\n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "                \"processing_time_seconds\": end_time - start_time if 'end_time' in locals() else None\n",
    "            }\n",
    "            \n",
    "            # Format the result with parsed JSON data\n",
    "            return format_extraction_result(result)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error during processing: {str(e)}\")\n",
    "            return {\n",
    "                \"image_path\": image_path,\n",
    "                \"error\": str(e),\n",
    "                \"success\": False,\n",
    "                \"timestamp\": datetime.now().isoformat()\n",
    "            }\n",
    "\n",
    "    def get_images_from_directory(self, directory_path: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Get all supported image files from a directory.\n",
    "\n",
    "        Args:\n",
    "            directory_path: Path to the directory containing images\n",
    "\n",
    "        Returns:\n",
    "            List of image file paths\n",
    "        \"\"\"\n",
    "        directory = Path(directory_path)\n",
    "        if not directory.exists():\n",
    "            raise ValueError(f\"Directory not found: {directory_path}\")\n",
    "\n",
    "        image_files = []\n",
    "        for file_path in directory.iterdir():\n",
    "            if file_path.is_file() and file_path.suffix.lower() in self.supported_formats:\n",
    "                image_files.append(str(file_path))\n",
    "\n",
    "        return sorted(image_files)\n",
    "\n",
    "    def save_results(self, results: List[Dict], output_file: str):\n",
    "        \"\"\"Save results to a JSON file.\"\"\"\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    def save_results_formatted(self, results: List[Dict], output_file: str, \n",
    "                             include_parsed_data: bool = True):\n",
    "        \"\"\"\n",
    "        Save results with enhanced formatting and optional separation of parsed data.\n",
    "        \n",
    "        Args:\n",
    "            results: List of OCR results\n",
    "            output_file: Path to save the formatted results\n",
    "            include_parsed_data: Whether to include parsed JSON data separately\n",
    "        \"\"\"\n",
    "        # Create formatted results\n",
    "        formatted_results = []\n",
    "        \n",
    "        for result in results:\n",
    "            formatted_result = result.copy()\n",
    "            \n",
    "            # If we have parsed JSON data, optionally restructure\n",
    "            if include_parsed_data and \"extracted_data_parsed\" in result:\n",
    "                # Move parsed data to top level for easier access\n",
    "                formatted_result[\"structured_data\"] = result[\"extracted_data_parsed\"]\n",
    "                \n",
    "                # Keep original text for reference but make it shorter in display\n",
    "                original_text = result[\"extracted_data\"]\n",
    "                if len(original_text) > 200:\n",
    "                    formatted_result[\"extracted_data_preview\"] = original_text[:200] + \"...\"\n",
    "                    formatted_result[\"extracted_data_full\"] = original_text\n",
    "                    del formatted_result[\"extracted_data\"]\n",
    "                \n",
    "            formatted_results.append(formatted_result)\n",
    "        \n",
    "        # Save with proper formatting\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(formatted_results, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        print(f\"üìÑ Formatted results saved to: {output_file}\")\n",
    "        \n",
    "        # Also create a summary file with just the structured data\n",
    "        if include_parsed_data:\n",
    "            summary_file = output_file.replace('.json', '_structured_only.json')\n",
    "            structured_only = []\n",
    "            \n",
    "            for result in formatted_results:\n",
    "                if \"structured_data\" in result:\n",
    "                    structured_only.append({\n",
    "                        \"image_path\": result[\"image_path\"],\n",
    "                        \"model_used\": result[\"model_used\"],\n",
    "                        \"success\": result[\"success\"],\n",
    "                        \"structured_data\": result[\"structured_data\"],\n",
    "                        \"timestamp\": result[\"timestamp\"]\n",
    "                    })\n",
    "            \n",
    "            with open(summary_file, 'w', encoding='utf-8') as f:\n",
    "                json.dump(structured_only, f, indent=2, ensure_ascii=False)\n",
    "            \n",
    "            print(f\"üìã Structured data summary saved to: {summary_file}\")\n",
    "\n",
    "    def process_directory(self, directory_path: str, prompt: str,\n",
    "                          output_file: Optional[str] = None,\n",
    "                          save_formatted: bool = True) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Process all images in a directory with enhanced result formatting.\n",
    "        \"\"\"\n",
    "        image_files = self.get_images_from_directory(directory_path)\n",
    "\n",
    "        if not image_files:\n",
    "            print(f\"No supported image files found in {directory_path}\")\n",
    "            return []\n",
    "\n",
    "        print(f\"Found {len(image_files)} image(s) to process...\")\n",
    "\n",
    "        results = []\n",
    "        for i, image_path in enumerate(image_files, 1):\n",
    "            print(f\"Processing image {i}/{len(image_files)}: {Path(image_path).name}\")\n",
    "            result = self.extract_data_from_image(image_path, prompt)\n",
    "            results.append(result)\n",
    "\n",
    "            if result[\"success\"]:\n",
    "                print(f\"‚úÖ Successfully processed {Path(image_path).name}\")\n",
    "                # Show preview of structured data if available\n",
    "                if \"extracted_data_parsed\" in result:\n",
    "                    print(f\"üìä Extracted structured data format: {result.get('extraction_format', 'unknown')}\")\n",
    "            else:\n",
    "                print(f\"‚ùå Failed to process {Path(image_path).name}: {result.get('error', 'Unknown error')}\")\n",
    "\n",
    "        # Save results with enhanced formatting\n",
    "        if output_file:\n",
    "            if save_formatted:\n",
    "                self.save_results_formatted(results, output_file, include_parsed_data=True)\n",
    "            else:\n",
    "                self.save_results(results, output_file)\n",
    "            print(f\"Results saved to {output_file}\")\n",
    "\n",
    "        return results\n",
    "\n",
    "    def change_model(self, model_name: str):\n",
    "        \"\"\"Change the model being used.\"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.model = None  # Reset model\n",
    "        try:\n",
    "            self.model = self._initialize_model()\n",
    "            print(f\"‚úÖ Model changed to: {model_name}\")\n",
    "        except ValueError as e:\n",
    "            print(f\"‚ö†Ô∏è Warning: Failed to initialize {model_name}: {e}\")\n",
    "            print(f\"Model will be initialized when needed if API key becomes available.\")\n",
    "\n",
    "    def check_api_keys(self) -> Dict[str, bool]:\n",
    "        \"\"\"Check which API keys are available.\"\"\"\n",
    "        keys_status = {}\n",
    "\n",
    "        google_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "        keys_status[\"gemini\"] = bool(\n",
    "            google_key and google_key != \"your_google_api_key_here\")\n",
    "\n",
    "        openai_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "        keys_status[\"openai\"] = bool(\n",
    "            openai_key and openai_key != \"your_openai_api_key_here\")\n",
    "\n",
    "        anthropic_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "        keys_status[\"anthropic\"] = bool(\n",
    "            anthropic_key and anthropic_key != \"your_anthropic_api_key_here\")\n",
    "\n",
    "        return keys_status\n",
    "\n",
    "\n",
    "print(\"OCRBenchmark class defined successfully!\")\n",
    "print(\n",
    "    f\"üîß Global settings: Optimization={ENABLE_IMAGE_OPTIMIZATION}, Resize ratio={IMAGE_RESIZE_RATIO}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b66c3c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prompts created:\n",
      "\n",
      "üìù GENERAL_OCR:\n",
      "   Extract all text from this image and format it nicely. Preserve the structure and layout as much as possible....\n",
      "\n",
      "üìù PAYSLIP_EXTRACTION:\n",
      "   \n",
      "        This is a payslip/salary slip image. Please extract the following information in JSON format:\n",
      "        {\n",
      "          \"employee_name\": \"\",\n",
      "          \"employee_id\": \"\",\n",
      "          \"pay_period\": \"\",\n",
      "          \"gross_salary\": \"\",\n",
      "          \"net_salary\": \"\",\n",
      "          \"deductions\": [],\n",
      "          \"company_name\": \"\"\n",
      "        }\n",
      "        If any field is not found, use null.\n",
      "        ...\n",
      "\n",
      "üìù STRUCTURED_DATA:\n",
      "   \n",
      "        Extract all structured data from this image and organize it in a clear, readable format. \n",
      "        Identify tables, forms, or any structured information and present it clearly.\n",
      "        ...\n",
      "\n",
      "üìù KEY_VALUE_PAIRS:\n",
      "   \n",
      "        Extract all key-value pairs from this image. Present them as:\n",
      "        Key: Value\n",
      "        Key: Value\n",
      "        etc.\n",
      "        ...\n",
      "\n",
      "üìù CASH_EXPENSE_EXTRACTION:\n",
      "   \n",
      "You are an expert document processing assistant. Your task is to extract structured data from invoice/receipt images and return the information in a specific JSON format.\n",
      "\n",
      "## Instructions:\n",
      "1. Analyze the provided image carefully to identify all relevant financial information\n",
      "2. Extract the data according to the JSON schema provided below\n",
      "3. If any required field is not visible or unclear in the image, use reasonable defaults or null values\n",
      "4. For monetary amounts, use numbers without currency symbols\n",
      "5. For dates, use the format found in the document or convert to YYYY-MM-DD if possible\n",
      "6. Be precise with calculations - ensure totals match the extracted item details\n",
      "7. The document may contain text in both English and Bangla (Bengali). For Bangla text, provide English transliteration (not translation) - write the Bangla words using English letters to show how they sound. For Bangla numbers convert them to\n",
      "    English numerals (e.g., \"‡ßß‡ß®‡ß©\" becomes \"123\").\n",
      "## Required JSON Schema:\n",
      "Retur...\n",
      "\n",
      "üìù CASH_EXPENSE_EXTRACTION_V2:\n",
      "   \n",
      "You are an expert document processing assistant. Your task is to extract structured data from invoice/receipt images and return the information in a specific JSON format.\n",
      "\n",
      "## Instructions:\n",
      "1. Analyze the provided image carefully to identify all relevant financial information\n",
      "2. Extract the data according to the JSON schema provided below\n",
      "3. If any required field is not visible or unclear in the image, use reasonable defaults or null values\n",
      "4. For monetary amounts, use numbers without currency symbols\n",
      "5. For dates, use the format found in the document or convert to YYYY-MM-DD if possible\n",
      "6. Be precise with calculations - ensure totals match the extracted item details\n",
      "7. The document may contain text in both English and Bangla (Bengali). For Bangla text, provide English transliteration (not translation) - write the Bangla words using English letters to show how they sound. For Bangla numbers convert them to\n",
      "    English numerals (e.g., \"‡ßß‡ß®‡ß©\" becomes \"123\").\n",
      "## Required JSON Schema:\n",
      "Retur...\n",
      "\n",
      "Ready to use! Check the next cells for usage examples.\n"
     ]
    }
   ],
   "source": [
    "# Example usage and demonstration\n",
    "def create_sample_prompts():\n",
    "    \"\"\"Create sample prompts for different OCR tasks.\"\"\"\n",
    "    return {\n",
    "        \"general_ocr\": \"Extract all text from this image and format it nicely. Preserve the structure and layout as much as possible.\",\n",
    "\n",
    "        \"payslip_extraction\": \"\"\"\n",
    "        This is a payslip/salary slip image. Please extract the following information in JSON format:\n",
    "        {\n",
    "          \"employee_name\": \"\",\n",
    "          \"employee_id\": \"\",\n",
    "          \"pay_period\": \"\",\n",
    "          \"gross_salary\": \"\",\n",
    "          \"net_salary\": \"\",\n",
    "          \"deductions\": [],\n",
    "          \"company_name\": \"\"\n",
    "        }\n",
    "        If any field is not found, use null.\n",
    "        \"\"\",\n",
    "\n",
    "        \"structured_data\": \"\"\"\n",
    "        Extract all structured data from this image and organize it in a clear, readable format. \n",
    "        Identify tables, forms, or any structured information and present it clearly.\n",
    "        \"\"\",\n",
    "\n",
    "        \"key_value_pairs\": \"\"\"\n",
    "        Extract all key-value pairs from this image. Present them as:\n",
    "        Key: Value\n",
    "        Key: Value\n",
    "        etc.\n",
    "        \"\"\",\n",
    "\n",
    "        \"cash_expense_extraction\": \"\"\"\n",
    "You are an expert document processing assistant. Your task is to extract structured data from invoice/receipt images and return the information in a specific JSON format.\n",
    "\n",
    "## Instructions:\n",
    "1. Analyze the provided image carefully to identify all relevant financial information\n",
    "2. Extract the data according to the JSON schema provided below\n",
    "3. If any required field is not visible or unclear in the image, use reasonable defaults or null values\n",
    "4. For monetary amounts, use numbers without currency symbols\n",
    "5. For dates, use the format found in the document or convert to YYYY-MM-DD if possible\n",
    "6. Be precise with calculations - ensure totals match the extracted item details\n",
    "7. The document may contain text in both English and Bangla (Bengali). For Bangla text, provide English transliteration (not translation) - write the Bangla words using English letters to show how they sound. For Bangla numbers convert them to\n",
    "    English numerals (e.g., \"‡ßß‡ß®‡ß©\" becomes \"123\").\n",
    "## Required JSON Schema:\n",
    "Return your response as a valid JSON object following this exact structure:\n",
    "\n",
    "{\n",
    "  \"payment_details\": {\n",
    "    \"supplier\": \"string - Name of the supplier/vendor\",\n",
    "    \"payment_account\": \"string - Account used for payment\",\n",
    "    \"payment_date\": \"string - Date of payment\",\n",
    "    \"payment_method\": \"string - Mode of payment (cash, card, bank transfer, etc.)\",\n",
    "    \"ref_no\": \"string - Reference/invoice number\",\n",
    "    \"tags\": [\"array of relevant tags\"]\n",
    "  },\n",
    "  \"item_details\": [\n",
    "    {\n",
    "      \"category\": \"string - Expense category\",\n",
    "      \"description\": \"string - Item description (optional)\",\n",
    "      \"quantity\": \"number - Quantity of items\",\n",
    "      \"unit_price\": \"number - Price per unit\",\n",
    "      \"total\": \"number - Total amount for this item\"\n",
    "    },\n",
    "    {\n",
    "      \"category\": \"string - Expense category for item 2\",\n",
    "      \"description\": \"string - Item description (optional)\",\n",
    "      \"quantity\": \"number - Quantity of items\",\n",
    "      \"unit_price\": \"number - Price per unit\", \n",
    "      \"total\": \"number - Total amount for this item\"\n",
    "    }\n",
    "    // ... additional items as found in the document\n",
    "  ],\n",
    "  \"attachment\": \"string - File reference if mentioned\",\n",
    "  \"memo\": \"string - Any additional notes or memo\",\n",
    "  \"totals\": {\n",
    "    \"sub_total\": \"number - Subtotal before tax and discount\",\n",
    "    \"sales_tax\": {\n",
    "      \"is_percentage\": \"boolean - true if tax is %, false if fixed amount\",\n",
    "      \"tax\": \"number - Tax rate / VAT rate (%)\",\n",
    "      \"amount\": \"number - Calculated tax amount\"\n",
    "    },\n",
    "    \"discount\": {\n",
    "      \"is_percentage\": \"boolean - true if discount is %, false if fixed amount\", \n",
    "      \"discount\": \"number - Discount rate (%)\",\n",
    "      \"calculated_amount\": \"number - Calculated discount amount\"\n",
    "    },\n",
    "    \"total_amount\": \"number - Final total amount\"\n",
    "  }\n",
    "}\n",
    "\n",
    "## Extraction Guidelines:\n",
    "- **Supplier**: Look for business name, vendor name, or \"Bill To\" information\n",
    "- **Payment Account**: Extract account numbers, card details, or payment method info\n",
    "- **Payment Date**: Find transaction date, invoice date, or payment date\n",
    "- **Payment Method**: Identify if paid by cash, card, check, bank transfer, etc.\n",
    "- **Reference Number**: Look for invoice #, receipt #, transaction ID, or reference number\n",
    "- **Tags**: Generate relevant tags based on the business type or expense category\n",
    "- **Item Details**: Extract ALL line items from the document - create a separate object for each item/product/service listed\n",
    "- **Categories**: Classify expenses (office supplies, travel, meals, equipment, etc.)\n",
    "- **Calculations**: Verify that item totals sum to subtotal, and final calculations are accurate\n",
    "- **Language Handling**: If you encounter Bangla/Bengali text, transliterate it into English letters (e.g., \"‡¶ü‡¶æ‡¶ï‡¶æ\" becomes \"taka\", \"‡¶®‡¶æ‡¶Æ\" becomes \"naam\"). Do not translate the meaning, just write how the Bangla words sound in English.\n",
    "\n",
    "## Important Notes:\n",
    "- Return ONLY the JSON object, no additional text or explanations\n",
    "- If information is missing, use null for strings and 0 for numbers\n",
    "- Ensure all numbers are numeric values, not strings\n",
    "- **Extract every single line item** - the item_details array should contain one object for each product/service listed in the document\n",
    "- Double-check mathematical accuracy of totals and ensure all item totals sum to the subtotal\n",
    "- VAT and tax should be considered same.\n",
    "- If no tax/VAT or discount is present, set the respective amounts to 0\n",
    "\n",
    "Now please analyze the provided image and extract the data according to this schema.\n",
    "        \"\"\",\n",
    "        \"payslip_extraction\": \"\"\"\n",
    "        This is a payslip/salary slip image. Please extract the following information in JSON format:\n",
    "        {\n",
    "          \"employee_name\": \"\",\n",
    "          \"employee_id\": \"\",\n",
    "          \"pay_period\": \"\",\n",
    "          \"gross_salary\": \"\",\n",
    "          \"net_salary\": \"\",\n",
    "          \"deductions\": [],\n",
    "          \"company_name\": \"\"\n",
    "        }\n",
    "        If any field is not found, use null.\n",
    "        \"\"\",\n",
    "\n",
    "        \"structured_data\": \"\"\"\n",
    "        Extract all structured data from this image and organize it in a clear, readable format. \n",
    "        Identify tables, forms, or any structured information and present it clearly.\n",
    "        \"\"\",\n",
    "\n",
    "        \"key_value_pairs\": \"\"\"\n",
    "        Extract all key-value pairs from this image. Present them as:\n",
    "        Key: Value\n",
    "        Key: Value\n",
    "        etc.\n",
    "        \"\"\",\n",
    "\n",
    "        \"cash_expense_extraction_V2\": \"\"\"\n",
    "You are an expert document processing assistant. Your task is to extract structured data from invoice/receipt images and return the information in a specific JSON format.\n",
    "\n",
    "## Instructions:\n",
    "1. Analyze the provided image carefully to identify all relevant financial information\n",
    "2. Extract the data according to the JSON schema provided below\n",
    "3. If any required field is not visible or unclear in the image, use reasonable defaults or null values\n",
    "4. For monetary amounts, use numbers without currency symbols\n",
    "5. For dates, use the format found in the document or convert to YYYY-MM-DD if possible\n",
    "6. Be precise with calculations - ensure totals match the extracted item details\n",
    "7. The document may contain text in both English and Bangla (Bengali). For Bangla text, provide English transliteration (not translation) - write the Bangla words using English letters to show how they sound. For Bangla numbers convert them to\n",
    "    English numerals (e.g., \"‡ßß‡ß®‡ß©\" becomes \"123\").\n",
    "## Required JSON Schema:\n",
    "Return your response as a valid JSON object following this exact structure:\n",
    "\n",
    "{\n",
    "  \"payment_details\": {\n",
    "    \"supplier\": \"string - Name of the supplier/vendor\",\n",
    "    \"payment_account\": \"string - Account used for payment\",\n",
    "    \"payment_date\": \"string - Date of payment\",\n",
    "    \"payment_method\": \"string - Mode of payment (cash, card, bank transfer, etc.)\",\n",
    "    \"ref_no\": \"string - Reference/invoice number\",\n",
    "    \"tags\": [\"array of relevant tags\"]\n",
    "  },\n",
    "  \"item_details\": [\n",
    "    {\n",
    "      \"category\": \"string - Expense category\",\n",
    "      \"description\": \"string - Item description (optional)\",\n",
    "      \"quantity\": \"number - Quantity of items\",\n",
    "      \"unit_price\": \"number - Price per unit\",\n",
    "      \"total\": \"number - Total amount for this item\"\n",
    "    },\n",
    "    {\n",
    "      \"category\": \"string - Expense category for item 2\",\n",
    "      \"description\": \"string - Item description (optional)\",\n",
    "      \"quantity\": \"number - Quantity of items\",\n",
    "      \"unit_price\": \"number - Price per unit\", \n",
    "      \"total\": \"number - Total amount for this item\"\n",
    "    }\n",
    "    // ... additional items as found in the document\n",
    "  ],\n",
    "  \"attachment\": \"string - File reference if mentioned\",\n",
    "  \"memo\": \"string - Any additional notes or memo\",\n",
    "  \"totals\": {\n",
    "    \"sub_total\": \"number - Subtotal before tax and discount\",\n",
    "    \"sales_tax\": {\n",
    "      \"is_percentage\": \"boolean - true if tax/VAT is %, false if fixed amount\",\n",
    "      \"tax\": \"number - Tax/VAT rate (%)\", \n",
    "      \"amount\": \"number - Calculated tax/VAT amount\"\n",
    "    },\n",
    "    \"discount\": {\n",
    "      \"is_percentage\": \"boolean - true if discount is %, false if fixed amount\", \n",
    "      \"discount\": \"number - Discount rate (%)\",\n",
    "      \"calculated_amount\": \"number - Calculated discount amount\"\n",
    "    },\n",
    "    \"total_amount\": \"number - Final total amount\"\n",
    "  }\n",
    "}\n",
    "\n",
    "## Extraction Guidelines:\n",
    "- **Supplier**: Look for business name, vendor name, or \"Bill To\" information\n",
    "- **Payment Account**: Extract account numbers, card details, or payment method info\n",
    "- **Payment Date**: Find transaction date, invoice date, or payment date\n",
    "- **Payment Method**: Identify if paid by cash, card, check, bank transfer, etc.\n",
    "- **Reference Number**: Look for invoice #, receipt #, transaction ID, or reference number\n",
    "- **Tags**: Generate relevant tags based on the business type or expense category\n",
    "- **Item Details**: Extract ALL line items from the document - create a separate object for each item/product/service listed\n",
    "- **Categories**: Classify expenses (office supplies, travel, meals, equipment, etc.)\n",
    "- **Calculations**: Verify that item totals sum to subtotal, and final calculations are accurate\n",
    "- **Language Handling**: If you encounter Bangla/Bengali text, transliterate it into English letters (e.g., \"‡¶ü‡¶æ‡¶ï‡¶æ\" becomes \"taka\", \"‡¶®‡¶æ‡¶Æ\" becomes \"naam\"). Do not translate the meaning, just write how the Bangla words sound in English.\n",
    "- **Tax/VAT Recognition**: Look for any tax-related terms including \"Tax\", \"VAT\", \"Sales Tax\", \"Service Tax\", \"GST\", or similar terms in both English and Bangla. Treat all of these as sales tax in the JSON structure.\n",
    "\n",
    "\n",
    "## Important Notes:\n",
    "- Return ONLY the JSON object, no additional text or explanations\n",
    "- If information is missing, use null for strings and 0 for numbers\n",
    "- Ensure all numbers are numeric values, not strings\n",
    "- **Extract every single line item** - the item_details array should contain one object for each product/service listed in the document\n",
    "- Double-check mathematical accuracy of totals and ensure all item totals sum to the subtotal\n",
    "- VAT and tax should be considered same.\n",
    "- If no tax/VAT or discount is present, set the respective amounts to 0\n",
    "\n",
    "Now please analyze the provided image and extract the data according to this schema.\n",
    "        \"\"\"\n",
    "    }\n",
    "\n",
    "\n",
    "# Create sample prompts\n",
    "sample_prompts = create_sample_prompts()\n",
    "\n",
    "print(\"Sample prompts created:\")\n",
    "for name, prompt in sample_prompts.items():\n",
    "    print(f\"\\nüìù {name.upper()}:\")\n",
    "    print(f\"   {prompt[:1000]}...\")\n",
    "\n",
    "print(\"\\nReady to use! Check the next cells for usage examples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "004fd257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ OCR Benchmark system initialized successfully with Gemini!\n",
      "Current model: gemini\n",
      "Found 11 image(s) in Pay Slip directory\n",
      "Sample images:\n",
      "  1. IMG_20250826_154715.jpg\n",
      "  2. IMG_20250826_154715_1.jpg\n",
      "  3. IMG_20250826_154929.jpg\n",
      "  4. IMG_20250826_155237.jpg\n",
      "  5. IMG_20250826_155355.jpg\n",
      "  6. IMG_20250826_155901.jpg\n",
      "  7. IMG_20250826_160504.jpg\n",
      "  8. IMG_20250826_160529.jpg\n",
      "  9. IMG_20250826_160544.jpg\n",
      "  10. IMG_20250826_160721.jpg\n",
      "  11. IMG_20250826_160928.jpg\n"
     ]
    }
   ],
   "source": [
    "# Initialize the OCR Benchmark system\n",
    "# By default, it uses Gemini model\n",
    "\n",
    "try:\n",
    "    # Initialize with Gemini (default)\n",
    "    ocr = OCRBenchmark(model_name=\"gemini\", temperature=0.1)\n",
    "    print(\"‚úÖ OCR Benchmark system initialized successfully with Gemini!\")\n",
    "    print(f\"Current model: {ocr.model_name}\")\n",
    "    \n",
    "    # Check if we have images to process\n",
    "    pay_slip_dir = \"/home/tanjim_noor/Work/AI OCR BenchMark/Good Pictures\"\n",
    "    if os.path.exists(pay_slip_dir):\n",
    "        image_files = ocr.get_images_from_directory(pay_slip_dir)\n",
    "        print(f\"Found {len(image_files)} image(s) in Pay Slip directory\")\n",
    "        if image_files:\n",
    "            print(\"Sample images:\")\n",
    "            for i, img in enumerate(image_files):\n",
    "                print(f\"  {i+1}. {Path(img).name}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error initializing OCR system: {e}\")\n",
    "    print(\"Please make sure to set your API keys in the .env file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e0daa950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: IMG_20250826_154715.jpg\n",
      "Using general OCR prompt...\n",
      "üöÄ Starting image processing...\n",
      "üîç File extension: .jpg ‚Üí MIME type: image/jpeg\n",
      "üñºÔ∏è Optimizing image...\n",
      "üîß Image optimization disabled - using original file\n",
      "üìù Encoding to base64...\n",
      "üìä Base64 size: 5.75 MB\n",
      "ü§ñ Sending to GEMINI...\n",
      "‚è±Ô∏è API call completed in 24.77 seconds\n",
      "\n",
      "‚úÖ SUCCESS!\n",
      "Model used: gemini\n",
      "Extracted data:\n",
      "```json\n",
      "{\n",
      "  \"payment_details\": {\n",
      "    \"supplier\": \"SHWAPNO\",\n",
      "    \"payment_account\": null,\n",
      "    \"payment_date\": \"2025-06-26\",\n",
      "    \"payment_method\": \"City Bank\",\n",
      "    \"ref_no\": \"D0612506260221\",\n",
      "    \"tags\": [\n",
      "      \"grocery\",\n",
      "      \"supermarket\",\n",
      "      \"food\",\n",
      "      \"retail\"\n",
      "    ]\n",
      "  },\n",
      "  \"item_details\": [\n",
      "    {\n",
      "      \"category\": \"Groceries\",\n",
      "      \"description\": \"Fresh Soyabean Oil 5 Ltr\",\n",
      "      \"quantity\": 1,\n",
      "      \"unit_price\": 922.00,\n",
      "      \"total\": 922.00\n",
      "    },\n",
      "    {\n",
      "      \"category\": \"Dairy & Refrigerated\",\n",
      "      \"description\": \"ULTRA Sweet Curd 500Gm\",\n",
      "      \"quantity\": 1,\n",
      "      \"unit_price\": 150.00,\n",
      "      \"total\": 150.00\n",
      "    }\n",
      "  ],\n",
      "  \"attachment\": null,\n",
      "  \"memo\": \"Earned points will expire within 6 months from the date of the transaction if not redeemed. Purchase of defected item must be exchanged by 24 hours with invoice.\",\n",
      "  \"totals\": {\n",
      "    \"sub_total\": 932.17,\n",
      "    \"sales_tax\": {\n",
      "      \"is_percentage\": false,\n",
      "      \"tax\": 0,\n",
      "      \"amount\": 139.83\n",
      "    },\n",
      "    \"discount\": {\n",
      "      \"is_percentage\": false,\n",
      "      \"discount\": 0,\n",
      "      \"calculated_amount\": 0.00\n",
      "    },\n",
      "    \"total_amount\": 1072.00\n",
      "  }\n",
      "}\n",
      "```\n",
      "‚è±Ô∏è API call completed in 24.77 seconds\n",
      "\n",
      "‚úÖ SUCCESS!\n",
      "Model used: gemini\n",
      "Extracted data:\n",
      "```json\n",
      "{\n",
      "  \"payment_details\": {\n",
      "    \"supplier\": \"SHWAPNO\",\n",
      "    \"payment_account\": null,\n",
      "    \"payment_date\": \"2025-06-26\",\n",
      "    \"payment_method\": \"City Bank\",\n",
      "    \"ref_no\": \"D0612506260221\",\n",
      "    \"tags\": [\n",
      "      \"grocery\",\n",
      "      \"supermarket\",\n",
      "      \"food\",\n",
      "      \"retail\"\n",
      "    ]\n",
      "  },\n",
      "  \"item_details\": [\n",
      "    {\n",
      "      \"category\": \"Groceries\",\n",
      "      \"description\": \"Fresh Soyabean Oil 5 Ltr\",\n",
      "      \"quantity\": 1,\n",
      "      \"unit_price\": 922.00,\n",
      "      \"total\": 922.00\n",
      "    },\n",
      "    {\n",
      "      \"category\": \"Dairy & Refrigerated\",\n",
      "      \"description\": \"ULTRA Sweet Curd 500Gm\",\n",
      "      \"quantity\": 1,\n",
      "      \"unit_price\": 150.00,\n",
      "      \"total\": 150.00\n",
      "    }\n",
      "  ],\n",
      "  \"attachment\": null,\n",
      "  \"memo\": \"Earned points will expire within 6 months from the date of the transaction if not redeemed. Purchase of defected item must be exchanged by 24 hours with invoice.\",\n",
      "  \"totals\": {\n",
      "    \"sub_total\": 932.17,\n",
      "    \"sales_tax\": {\n",
      "      \"is_percentage\": false,\n",
      "      \"tax\": 0,\n",
      "      \"amount\": 139.83\n",
      "    },\n",
      "    \"discount\": {\n",
      "      \"is_percentage\": false,\n",
      "      \"discount\": 0,\n",
      "      \"calculated_amount\": 0.00\n",
      "    },\n",
      "    \"total_amount\": 1072.00\n",
      "  }\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# Example 1: Process a single image\n",
    "# Let's try to process one image from the Pay Slip directory\n",
    "\n",
    "try:\n",
    "    pay_slip_dir = \"/home/tanjim_noor/Work/AI OCR BenchMark/Good Pictures\"\n",
    "    if os.path.exists(pay_slip_dir):\n",
    "        image_files = ocr.get_images_from_directory(pay_slip_dir)\n",
    "        \n",
    "        if image_files:\n",
    "            # Process the first image with payslip extraction prompt\n",
    "            first_image = image_files[0]\n",
    "            print(f\"Processing: {Path(first_image).name}\")\n",
    "            print(\"Using general OCR prompt...\")\n",
    "\n",
    "            result = ocr.extract_data_from_image(first_image, sample_prompts[\"cash_expense_extraction_V2\"])\n",
    "            \n",
    "            if result[\"success\"]:\n",
    "                print(\"\\n‚úÖ SUCCESS!\")\n",
    "                print(f\"Model used: {result['model_used']}\")\n",
    "                print(f\"Extracted data:\\n{result['extracted_data']}\")\n",
    "            else:\n",
    "                print(f\"\\n‚ùå FAILED: {result.get('error', 'Unknown error')}\")\n",
    "        else:\n",
    "            print(\"No images found in the directory\")\n",
    "    else:\n",
    "        print(\"Pay Slip directory not found\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"Make sure your API key is set correctly in the .env file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e460ee7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting batch processing...\n",
      "Input directory: /home/tanjim_noor/Work/AI OCR BenchMark/For benchmark\n",
      "Output file: /home/tanjim_noor/Work/AI OCR BenchMark/OCR_Output/ocr_result_1.json\n",
      "==================================================\n",
      "Found 2 image(s) to process...\n",
      "Processing image 1/2: IMG_20250826_154308~2.jpg\n",
      "üöÄ Starting image processing...\n",
      "üîç File extension: .jpg ‚Üí MIME type: image/jpeg\n",
      "üñºÔ∏è Optimizing image...\n",
      "üîß Image optimization disabled - using original file\n",
      "üìù Encoding to base64...\n",
      "üìä Base64 size: 1.77 MB\n",
      "ü§ñ Sending to GEMINI...\n",
      "‚è±Ô∏è API call completed in 12.05 seconds\n",
      "‚úÖ Successfully processed IMG_20250826_154308~2.jpg\n",
      "üìä Extracted structured data format: json\n",
      "Processing image 2/2: IMG_20250826_155754.jpg\n",
      "üöÄ Starting image processing...\n",
      "üîç File extension: .jpg ‚Üí MIME type: image/jpeg\n",
      "üñºÔ∏è Optimizing image...\n",
      "üîß Image optimization disabled - using original file\n",
      "üìù Encoding to base64...\n",
      "üìä Base64 size: 4.35 MB\n",
      "ü§ñ Sending to GEMINI...\n",
      "‚è±Ô∏è API call completed in 12.05 seconds\n",
      "‚úÖ Successfully processed IMG_20250826_154308~2.jpg\n",
      "üìä Extracted structured data format: json\n",
      "Processing image 2/2: IMG_20250826_155754.jpg\n",
      "üöÄ Starting image processing...\n",
      "üîç File extension: .jpg ‚Üí MIME type: image/jpeg\n",
      "üñºÔ∏è Optimizing image...\n",
      "üîß Image optimization disabled - using original file\n",
      "üìù Encoding to base64...\n",
      "üìä Base64 size: 4.35 MB\n",
      "ü§ñ Sending to GEMINI...\n",
      "‚è±Ô∏è API call completed in 10.54 seconds\n",
      "‚úÖ Successfully processed IMG_20250826_155754.jpg\n",
      "üìä Extracted structured data format: json\n",
      "üìÑ Formatted results saved to: /home/tanjim_noor/Work/AI OCR BenchMark/OCR_Output/ocr_result_1.json\n",
      "üìã Structured data summary saved to: /home/tanjim_noor/Work/AI OCR BenchMark/OCR_Output/ocr_result_1_structured_only.json\n",
      "Results saved to /home/tanjim_noor/Work/AI OCR BenchMark/OCR_Output/ocr_result_1.json\n",
      "==================================================\n",
      "üìä PROCESSING SUMMARY:\n",
      "Total images: 2\n",
      "Successful: 2\n",
      "Failed: 0\n",
      "Results saved to: /home/tanjim_noor/Work/AI OCR BenchMark/OCR_Output/ocr_result_1.json\n",
      "Batch processing code is commented out.\n",
      "‚è±Ô∏è API call completed in 10.54 seconds\n",
      "‚úÖ Successfully processed IMG_20250826_155754.jpg\n",
      "üìä Extracted structured data format: json\n",
      "üìÑ Formatted results saved to: /home/tanjim_noor/Work/AI OCR BenchMark/OCR_Output/ocr_result_1.json\n",
      "üìã Structured data summary saved to: /home/tanjim_noor/Work/AI OCR BenchMark/OCR_Output/ocr_result_1_structured_only.json\n",
      "Results saved to /home/tanjim_noor/Work/AI OCR BenchMark/OCR_Output/ocr_result_1.json\n",
      "==================================================\n",
      "üìä PROCESSING SUMMARY:\n",
      "Total images: 2\n",
      "Successful: 2\n",
      "Failed: 0\n",
      "Results saved to: /home/tanjim_noor/Work/AI OCR BenchMark/OCR_Output/ocr_result_1.json\n",
      "Batch processing code is commented out.\n"
     ]
    }
   ],
   "source": [
    "# Example 2: Process all images in directory (Batch Processing)\n",
    "# This will process all images in the Pay Slip directory\n",
    "\n",
    "# Uncomment the following code to process all images:\n",
    "\n",
    "try:\n",
    "    pay_slip_dir = \"/home/tanjim_noor/Work/AI OCR BenchMark/For benchmark\"\n",
    "    output_file = \"/home/tanjim_noor/Work/AI OCR BenchMark/OCR_Output/ocr_result_1.json\"\n",
    "    \n",
    "    print(\"Starting batch processing...\")\n",
    "    print(f\"Input directory: {pay_slip_dir}\")\n",
    "    print(f\"Output file: {output_file}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Process all images with payslip extraction prompt\n",
    "    results = ocr.process_directory(\n",
    "        directory_path=pay_slip_dir,\n",
    "        prompt=sample_prompts[\"cash_expense_extraction_V2\"],\n",
    "        output_file=output_file\n",
    "    )\n",
    "    \n",
    "    # Summary\n",
    "    successful = sum(1 for r in results if r[\"success\"])\n",
    "    failed = len(results) - successful\n",
    "    \n",
    "    print(\"=\" * 50)\n",
    "    print(f\"üìä PROCESSING SUMMARY:\")\n",
    "    print(f\"Total images: {len(results)}\")\n",
    "    print(f\"Successful: {successful}\")\n",
    "    print(f\"Failed: {failed}\")\n",
    "    print(f\"Results saved to: {output_file}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error during batch processing: {e}\")\n",
    "\n",
    "\n",
    "print(\"Batch processing code is commented out.\")\n",
    "#print(\"Uncomment the code above to process all images in the directory.\")\n",
    "#print(\"Note: This will use API calls for each image, so be mindful of costs!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cbedb491",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_json = {\n",
    "    \"payment_details\": {\n",
    "        \"supplier\": \"Maa Fruits & Departmental Store\",\n",
    "        \"payment_account\": None,\n",
    "        \"payment_date\": \"2020-06-20\",\n",
    "        \"payment_method\": \"Cash\",\n",
    "        \"ref_no\": None,\n",
    "        \"tags\": [\n",
    "            \"Fruits\",\n",
    "            \"Departmental Store\",\n",
    "            \"Retail\",\n",
    "            \"Wholesale\"\n",
    "        ]\n",
    "    },\n",
    "    \"item_details\": [\n",
    "        {\n",
    "            \"category\": \"Fruits\",\n",
    "            \"description\": \"Tomatoes\",\n",
    "            \"quantity\": 80,\n",
    "            \"unit_price\": 7.5,\n",
    "            \"total\": 600\n",
    "        }\n",
    "    ],\n",
    "    \"attachment\": None,\n",
    "    \"memo\": \"Soles goods are not returnable. Welcome come again.\",\n",
    "    \"totals\": {\n",
    "        \"sub_total\": 600,\n",
    "        \"sales_tax\": {\n",
    "            \"is_percentage\": False,\n",
    "            \"tax\": 0,\n",
    "            \"amount\": 0\n",
    "        },\n",
    "        \"discount\": {\n",
    "            \"is_percentage\": False,\n",
    "            \"discount\": 0,\n",
    "            \"calculated_amount\": 0\n",
    "        },\n",
    "        \"total_amount\": 600\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "actual_json = {\n",
    "    \"payment_details\": {\n",
    "        \"supplier\": \"Maa Fruits & Departmental Store\",\n",
    "        \"payment_account\": None,\n",
    "        \"payment_date\": \"2020-06-20\",\n",
    "        \"payment_method\": \"Cash\",\n",
    "        \"ref_no\": None,\n",
    "        \"tags\": [\n",
    "            \"Fruits\",\n",
    "            \"Departmental Store\",\n",
    "            \"Retail\",\n",
    "            \"Wholesale\"\n",
    "        ]\n",
    "    },\n",
    "    \"item_details\": [\n",
    "        {\n",
    "            \"category\": \"Fruits\",\n",
    "            \"description\": \"Tomatoes\",\n",
    "            \"quantity\": 80,\n",
    "            \"unit_price\": 10,\n",
    "            \"total\": 800\n",
    "        }\n",
    "    ],\n",
    "    \"attachment\": None,\n",
    "    \"memo\": \"Soled goods are not returnable. Welcome come again.\",\n",
    "    \"totals\": {\n",
    "        \"sub_total\": 800,\n",
    "        \"sales_tax\": {\n",
    "            \"is_percentage\": False,\n",
    "            \"tax\": 0,\n",
    "            \"amount\": 0\n",
    "        },\n",
    "        \"discount\": {\n",
    "            \"is_percentage\": False,\n",
    "            \"discount\": 0,\n",
    "            \"calculated_amount\": 0\n",
    "        },\n",
    "        \"total_amount\": 800\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "output_json_2 = {\n",
    "  \"payment_details\": {\n",
    "    \"supplier\": \"SHWAPNO (ACI Logistics Limited)\",\n",
    "    \"payment_account\": \"City Bank\",\n",
    "    \"payment_date\": \"2025-06-26\",\n",
    "    \"payment_method\": \"Bank Transfer/Card (implied by City Bank)\",\n",
    "    \"ref_no\": \"D0612506260068\",\n",
    "    \"tags\": [\"Groceries\", \"Supermarket\", \"Retail\", \"Food\"]\n",
    "  },\n",
    "  \"item_details\": [\n",
    "    {\n",
    "      \"category\": \"Groceries\",\n",
    "      \"description\": \"Beef Premium Cube kg\",\n",
    "      \"quantity\": 6.33,\n",
    "      \"unit_price\": 775.00,\n",
    "      \"total\": 4901.88\n",
    "    },\n",
    "    {\n",
    "      \"category\": \"Groceries\",\n",
    "      \"description\": \"Broiler Chicken Breast Bon\",\n",
    "      \"quantity\": 0.50,\n",
    "      \"unit_price\": 534.00,\n",
    "      \"total\": 267.00\n",
    "    },\n",
    "    {\n",
    "      \"category\": \"Groceries\",\n",
    "      \"description\": \"Capsicum Green kg\",\n",
    "      \"quantity\": 1.01,\n",
    "      \"unit_price\": 320.00,\n",
    "      \"total\": 321.60\n",
    "    },\n",
    "    {\n",
    "      \"category\": \"Groceries\",\n",
    "      \"description\": \"Carrot China (China Gajor) K\",\n",
    "      \"quantity\": 1.40,\n",
    "      \"unit_price\": 160.00,\n",
    "      \"total\": 224.00\n",
    "    },\n",
    "    {\n",
    "      \"category\": \"Groceries\",\n",
    "      \"description\": \"Cucumber (Shosha) kg\",\n",
    "      \"quantity\": 2.69,\n",
    "      \"unit_price\": 55.00,\n",
    "      \"total\": 147.68\n",
    "    },\n",
    "    {\n",
    "      \"category\": \"Groceries\",\n",
    "      \"description\": \"Green Chili (Kacha Morich)\",\n",
    "      \"quantity\": 0.67,\n",
    "      \"unit_price\": 100.00,\n",
    "      \"total\": 66.50\n",
    "    },\n",
    "    {\n",
    "      \"category\": \"Groceries\",\n",
    "      \"description\": \"Green Papaya (Kacha Pepe)\",\n",
    "      \"quantity\": 3.00,\n",
    "      \"unit_price\": 45.00,\n",
    "      \"total\": 134.78\n",
    "    },\n",
    "    {\n",
    "      \"category\": \"Groceries\",\n",
    "      \"description\": \"Lemon Long (Lomba Lebu) PC\",\n",
    "      \"quantity\": 8.00,\n",
    "      \"unit_price\": 6.00,\n",
    "      \"total\": 48.00\n",
    "    },\n",
    "    {\n",
    "      \"category\": \"Groceries\",\n",
    "      \"description\": \"Long Bean (Boroboti) KG\",\n",
    "      \"quantity\": 1.83,\n",
    "      \"unit_price\": 90.00,\n",
    "      \"total\": 164.70\n",
    "    },\n",
    "    {\n",
    "      \"category\": \"Groceries\",\n",
    "      \"description\": \"Piyaj Deshi Loose kg\",\n",
    "      \"quantity\": 2.66,\n",
    "      \"unit_price\": 58.00,\n",
    "      \"total\": 154.28\n",
    "    },\n",
    "    {\n",
    "      \"category\": \"Groceries\",\n",
    "      \"description\": \"Shonalika Dressed Classic\",\n",
    "      \"quantity\": 1.65,\n",
    "      \"unit_price\": 667.00,\n",
    "      \"total\": 1097.22\n",
    "    },\n",
    "    {\n",
    "      \"category\": \"Groceries\",\n",
    "      \"description\": \"ACI Pure Corn Flour 150g\",\n",
    "      \"quantity\": 1.00,\n",
    "      \"unit_price\": 60.00,\n",
    "      \"total\": 60.00\n",
    "    },\n",
    "    {\n",
    "      \"category\": \"Groceries\",\n",
    "      \"description\": \"Shwapno Black Pepper Powde\",\n",
    "      \"quantity\": 1.00,\n",
    "      \"unit_price\": 129.00,\n",
    "      \"total\": 129.00\n",
    "    }\n",
    "  ],\n",
    "  \"attachment\": False,\n",
    "  \"memo\": \"Thank you for shopping with SHWAPNO. Please visit www.shwapno.com for home delivery. Purchase of defected item must be exchanged by 24 hours with invoice. For any queries, suggestions or complaints, please call 16469 (9:00 AM - 6:00 PM). Earned points will expire within 6 months from the date of the transaction if not redeemed. VAT against this challan is payable through central registration. Prices inclusive of standard VAT except exempted items, VAT Payable TK. : 7.17.\",\n",
    "  \"totals\": {\n",
    "    \"sub_total\": 7716.64,\n",
    "    \"sales_tax\": {\n",
    "      \"is_percentage\": False,\n",
    "      \"tax\": 0,\n",
    "      \"amount\": 0\n",
    "    },\n",
    "    \"discount\": {\n",
    "      \"is_percentage\": False,\n",
    "      \"discount\": 0,\n",
    "      \"calculated_amount\": 0\n",
    "    },\n",
    "    \"total_amount\": 7716.64\n",
    "  }\n",
    "}\n",
    "\n",
    "actual_json_2 = {\"payment_details\": {\n",
    "    \"supplier\": \"SHWAPNO (ACI Logistics Limited)\",\n",
    "    \"payment_account\": \"City Bank\",\n",
    "    \"payment_date\": \"2025-06-26\",\n",
    "    \"payment_method\": \"Bank Transfer/Card (implied by City Bank)\",\n",
    "    \"ref_no\": \"D0612506260068\",\n",
    "    \"tags\": [\"Groceries\", \"Supermarket\", \"Retail\", \"Food\"]\n",
    "  },\n",
    "  \"item_details\": [\n",
    "    {\n",
    "      \"category\": \"Groceries\",\n",
    "      \"description\": \"Beef Premium Cube kg\",\n",
    "      \"quantity\": 6.33,\n",
    "      \"unit_price\": 775.00,\n",
    "      \"total\": 4901.88\n",
    "    },\n",
    "    {\n",
    "      \"category\": \"Groceries\",\n",
    "      \"description\": \"Broiler Chicken Breast Bon\",\n",
    "      \"quantity\": 0.50,\n",
    "      \"unit_price\": 534.00,\n",
    "      \"total\": 267.00\n",
    "    },\n",
    "    {\n",
    "      \"category\": \"Groceries\",\n",
    "      \"description\": \"Capsicum Green kg\",\n",
    "      \"quantity\": 1.01,\n",
    "      \"unit_price\": 320.00,\n",
    "      \"total\": 321.60\n",
    "    },\n",
    "    {\n",
    "      \"category\": \"Groceries\",\n",
    "      \"description\": \"Carrot China (China Gajor) K\",\n",
    "      \"quantity\": 1.40,\n",
    "      \"unit_price\": 160.00,\n",
    "      \"total\": 224.00\n",
    "    },\n",
    "    {\n",
    "      \"category\": \"Groceries\",\n",
    "      \"description\": \"Cucumber (Shosha) kg\",\n",
    "      \"quantity\": 2.69,\n",
    "      \"unit_price\": 55.00,\n",
    "      \"total\": 147.68\n",
    "    },\n",
    "    {\n",
    "      \"category\": \"Groceries\",\n",
    "      \"description\": \"Green Chili (Kacha Morich)\",\n",
    "      \"quantity\": 0.67,\n",
    "      \"unit_price\": 100.00,\n",
    "      \"total\": 66.50\n",
    "    },\n",
    "    {\n",
    "      \"category\": \"Groceries\",\n",
    "      \"description\": \"Green Papaya (Kacha Pepe)\",\n",
    "      \"quantity\": 3.00,\n",
    "      \"unit_price\": 45.00,\n",
    "      \"total\": 134.78\n",
    "    },\n",
    "    {\n",
    "      \"category\": \"Groceries\",\n",
    "      \"description\": \"Lemon Long (Lomba Lebu) PC\",\n",
    "      \"quantity\": 8.00,\n",
    "      \"unit_price\": 6.00,\n",
    "      \"total\": 48.00\n",
    "    },\n",
    "    {\n",
    "      \"category\": \"Groceries\",\n",
    "      \"description\": \"Long Bean (Boroboti) KG\",\n",
    "      \"quantity\": 1.83,\n",
    "      \"unit_price\": 90.00,\n",
    "      \"total\": 164.70\n",
    "    },\n",
    "    {\n",
    "      \"category\": \"Groceries\",\n",
    "      \"description\": \"Piyaj Deshi Loose kg\",\n",
    "      \"quantity\": 2.66,\n",
    "      \"unit_price\": 58.00,\n",
    "      \"total\": 154.28\n",
    "    },\n",
    "    {\n",
    "      \"category\": \"Groceries\",\n",
    "      \"description\": \"Shonalika Dressed Classic\",\n",
    "      \"quantity\": 1.65,\n",
    "      \"unit_price\": 667.00,\n",
    "      \"total\": 1097.22\n",
    "    },\n",
    "    {\n",
    "      \"category\": \"Groceries\",\n",
    "      \"description\": \"ACI Pure Corn Flour 150g\",\n",
    "      \"quantity\": 1.00,\n",
    "      \"unit_price\": 60.00,\n",
    "      \"total\": 60.00\n",
    "    },\n",
    "    {\n",
    "      \"category\": \"Groceries\",\n",
    "      \"description\": \"Shwapno Black Pepper Powde\",\n",
    "      \"quantity\": 1.00,\n",
    "      \"unit_price\": 129.00,\n",
    "      \"total\": 129.00\n",
    "    }\n",
    "  ],\n",
    "  \"attachment\": False,\n",
    "  \"memo\": \"Thank you for shopping with SHWAPNO. Please visit www.shwapno.com for home delivery. Purchase of defected item must be exchanged by 24 hours with invoice. For any queries, suggestions or complaints, please call 16469 (9:00 AM - 6:00 PM). Earned points will expire within 6 months from the date of the transaction if not redeemed. VAT against this challan is payable through central registration. Prices inclusive of standard VAT except exempted items, VAT Payable TK. : 7.17.\",\n",
    "  \"totals\": {\n",
    "    \"sub_total\": 7716.64,\n",
    "    \"sales_tax\": {\n",
    "      \"is_percentage\": False,\n",
    "      \"tax\": 0,\n",
    "      \"amount\": 7.17\n",
    "    },\n",
    "    \"discount\": {\n",
    "      \"is_percentage\": False,\n",
    "      \"discount\": 0,\n",
    "      \"calculated_amount\": 30.80\n",
    "    },\n",
    "    \"total_amount\": 7686.00\n",
    "  }\n",
    "}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "28ec01ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence-transformers\n",
      "  Downloading sentence_transformers-5.1.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting transformers<5.0.0,>=4.41.0 (from sentence-transformers)\n",
      "  Downloading transformers-4.55.4-py3-none-any.whl.metadata (41 kB)\n",
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.12/site-packages (from sentence-transformers) (4.67.1)\n",
      "Collecting torch>=1.11.0 (from sentence-transformers)\n",
      "  Downloading torch-2.8.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (30 kB)\n",
      "Collecting scikit-learn (from sentence-transformers)\n",
      "  Downloading scikit_learn-1.7.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n",
      "Collecting scipy (from sentence-transformers)\n",
      "  Downloading scipy-1.16.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (61 kB)\n",
      "Collecting huggingface-hub>=0.20.0 (from sentence-transformers)\n",
      "  Downloading huggingface_hub-0.34.4-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: Pillow in ./.venv/lib/python3.12/site-packages (from sentence-transformers) (11.3.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in ./.venv/lib/python3.12/site-packages (from sentence-transformers) (4.15.0)\n",
      "Collecting filelock (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading filelock-3.19.1-py3-none-any.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.3.2)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.7.34)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.32.5)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Downloading fsspec-2025.7.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Downloading hf_xet-1.1.8-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (703 bytes)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (80.9.0)\n",
      "Collecting sympy>=1.13.3 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.8.93 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.8.90 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.8.90 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.10.2.21 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cublas-cu12==12.8.4.1 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cufft-cu12==11.3.3.83 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.9.90 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.7.3.90 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.5.8.93 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.7.1 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl.metadata (7.0 kB)\n",
      "Collecting nvidia-nccl-cu12==2.27.3 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.8.90 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.8.93 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cufile-cu12==1.13.1.3 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==3.4.0 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading triton-3.4.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.12/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.8.3)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn->sentence-transformers)\n",
      "  Downloading joblib-1.5.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn->sentence-transformers)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading sentence_transformers-5.1.0-py3-none-any.whl (483 kB)\n",
      "Downloading transformers-4.55.4-py3-none-any.whl (11.3 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.34.4-py3-none-any.whl (561 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m561.5/561.5 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading hf_xet-1.1.8-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2025.7.0-py3-none-any.whl (199 kB)\n",
      "Downloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (485 kB)\n",
      "Downloading torch-2.8.0-cp312-cp312-manylinux_2_28_x86_64.whl (887.9 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m887.9/887.9 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m  \u001b[33m0:01:13\u001b[0mm0:00:01\u001b[0m00:03\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (594.3 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m594.3/594.3 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m  \u001b[33m0:01:11\u001b[0mm0:00:01\u001b[0m00:03\u001b[0mm\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0meta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m88.0/88.0 MB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m  \u001b[33m0:00:05\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m954.8/954.8 kB\u001b[0m \u001b[31m45.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl (706.8 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m706.8/706.8 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m  \u001b[33m0:00:58\u001b[0mm0:00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (193.1 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m193.1/193.1 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m  \u001b[33m0:00:17\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (63.6 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m63.6/63.6 MB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m  \u001b[33m0:00:04\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (267.5 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m267.5/267.5 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m  \u001b[33m0:00:20\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (288.2 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m288.2/288.2 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m  \u001b[33m0:00:26\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl (287.2 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m287.2/287.2 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m  \u001b[33m0:00:22\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (322.4 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m322.4/322.4 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m  \u001b[33m0:00:24\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
      "Downloading triton-3.4.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (155.6 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m155.6/155.6 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m  \u001b[33m0:00:12\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading filelock-3.19.1-py3-none-any.whl (15 kB)\n",
      "Downloading networkx-3.5-py3-none-any.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading scikit_learn-1.7.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (9.5 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.5.1-py3-none-any.whl (307 kB)\n",
      "Downloading scipy-1.16.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (35.2 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m35.2/35.2 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: nvidia-cusparselt-cu12, mpmath, triton, threadpoolctl, sympy, scipy, safetensors, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, networkx, joblib, hf-xet, fsspec, filelock, scikit-learn, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, huggingface-hub, tokenizers, nvidia-cusolver-cu12, transformers, torch, sentence-transformers\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m31/31\u001b[0m [sentence-transformers]ence-transformers]]\n",
      "\u001b[1A\u001b[2KSuccessfully installed filelock-3.19.1 fsspec-2025.7.0 hf-xet-1.1.8 huggingface-hub-0.34.4 joblib-1.5.1 mpmath-1.3.0 networkx-3.5 nvidia-cublas-cu12-12.8.4.1 nvidia-cuda-cupti-cu12-12.8.90 nvidia-cuda-nvrtc-cu12-12.8.93 nvidia-cuda-runtime-cu12-12.8.90 nvidia-cudnn-cu12-9.10.2.21 nvidia-cufft-cu12-11.3.3.83 nvidia-cufile-cu12-1.13.1.3 nvidia-curand-cu12-10.3.9.90 nvidia-cusolver-cu12-11.7.3.90 nvidia-cusparse-cu12-12.5.8.93 nvidia-cusparselt-cu12-0.7.1 nvidia-nccl-cu12-2.27.3 nvidia-nvjitlink-cu12-12.8.93 nvidia-nvtx-cu12-12.8.90 safetensors-0.6.2 scikit-learn-1.7.1 scipy-1.16.1 sentence-transformers-5.1.0 sympy-1.14.0 threadpoolctl-3.6.0 tokenizers-0.21.4 torch-2.8.0 transformers-4.55.4 triton-3.4.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# %pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9521d70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import Any, Dict, List\n",
    "from difflib import SequenceMatcher\n",
    "import numpy as np\n",
    "\n",
    "# Optional: install if not already\n",
    "# pip install sentence-transformers\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load a small model once (fast + lightweight)\n",
    "embed_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "\n",
    "# --- helpers ---\n",
    "def get_by_path(data: dict, path: str):\n",
    "    \"\"\"\n",
    "    Supports:\n",
    "    - normal dot paths (\"totals.sub_total\")\n",
    "    - indexed paths (\"item_details[0].quantity\")\n",
    "    - wildcard array paths (\"item_details.quantity\") -> returns list of values\n",
    "    \"\"\"\n",
    "    keys = path.replace(\"]\", \"\").split(\".\")\n",
    "    val = data\n",
    "    for k in keys:\n",
    "        if isinstance(val, list):\n",
    "            # Apply to each element if list\n",
    "            if k.isdigit():\n",
    "                val = val[int(k)]\n",
    "            else:\n",
    "                val = [v[k] for v in val]\n",
    "        else:\n",
    "            if \"[\" in k:  # explicit index\n",
    "                field, idx = k.split(\"[\")\n",
    "                val = val[field][int(idx)]\n",
    "            else:\n",
    "                val = val[k]\n",
    "    return val\n",
    "\n",
    "\n",
    "# --- comparators ---\n",
    "def exact_match(a, b) -> float:\n",
    "    return 1.0 if str(a) == str(b) else 0.0\n",
    "\n",
    "def numeric_tolerance(a, b, tolerance: float = 0.1) -> float:\n",
    "    try:\n",
    "        a, b = float(a), float(b)\n",
    "    except (ValueError, TypeError):\n",
    "        return 0.0\n",
    "    if a == b:\n",
    "        return 1.0\n",
    "    rel_error = abs(a - b) / (abs(b) + 1e-8)\n",
    "    return max(0.0, 1 - rel_error / tolerance)\n",
    "\n",
    "def semantic_similarity(a, b) -> float:\n",
    "    \"\"\"Cheap semantic: edit similarity.\"\"\"\n",
    "    if not a or not b:\n",
    "        return 0.0\n",
    "    return SequenceMatcher(None, str(a).lower(), str(b).lower()).ratio()\n",
    "\n",
    "def embedding_similarity(a, b) -> float:\n",
    "    \"\"\"Semantic similarity using embeddings + cosine similarity.\"\"\"\n",
    "    if not a or not b:\n",
    "        return 0.0\n",
    "    emb_a = embed_model.encode(str(a), convert_to_numpy=True)\n",
    "    emb_b = embed_model.encode(str(b), convert_to_numpy=True)\n",
    "    cos_sim = np.dot(emb_a, emb_b) / (np.linalg.norm(emb_a) * np.linalg.norm(emb_b))\n",
    "    # map cosine [-1,1] to [0,1]\n",
    "    return float((cos_sim + 1) / 2)\n",
    "\n",
    "\n",
    "COMPARATORS = {\n",
    "    \"exact\": exact_match,\n",
    "    \"numeric\": numeric_tolerance,\n",
    "    \"semantic\": semantic_similarity,   # lightweight string-based\n",
    "    \"embedding\": embedding_similarity, # heavy but meaningful\n",
    "}\n",
    "\n",
    "\n",
    "# --- main scoring ---\n",
    "def score_json(pred: dict, actual: dict, config: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "    total_weight = sum(field[\"weight\"] for field in config)\n",
    "    breakdown = []\n",
    "    weighted_sum = 0.0\n",
    "\n",
    "    for field in config:\n",
    "        path = field[\"path\"]\n",
    "        weight = field.get(\"weight\", 1.0)\n",
    "        comparator = COMPARATORS[field[\"comparator\"]]\n",
    "        tolerance = field.get(\"tolerance\", 0.1)\n",
    "\n",
    "        pred_val = get_by_path(pred, path)\n",
    "        actual_val = get_by_path(actual, path)\n",
    "\n",
    "        # Case 1: scalar\n",
    "        if not isinstance(pred_val, list):\n",
    "            pred_val, actual_val = [pred_val], [actual_val]\n",
    "\n",
    "        # If lengths differ, penalize missing items\n",
    "        max_len = max(len(pred_val), len(actual_val))\n",
    "        scores = []\n",
    "        for i in range(max_len):\n",
    "            pv = pred_val[i] if i < len(pred_val) else None\n",
    "            av = actual_val[i] if i < len(actual_val) else None\n",
    "\n",
    "            if field[\"comparator\"] == \"numeric\":\n",
    "                s = comparator(pv, av, tolerance)\n",
    "            else:\n",
    "                s = comparator(pv, av)\n",
    "            scores.append(s)\n",
    "\n",
    "            breakdown.append({\n",
    "                \"path\": f\"{path}[{i}]\",\n",
    "                \"pred\": pv,\n",
    "                \"actual\": av,\n",
    "                \"comparator\": field[\"comparator\"],\n",
    "                \"score\": round(s, 3),\n",
    "                \"weight\": weight,\n",
    "                \"weighted\": round(weight * s, 3)\n",
    "            })\n",
    "\n",
    "        avg_score = sum(scores) / max_len if max_len > 0 else 0\n",
    "        weighted_sum += weight * avg_score\n",
    "\n",
    "    final_score = weighted_sum / total_weight if total_weight > 0 else 0\n",
    "    return {\n",
    "        \"final_score\": round(final_score, 3),\n",
    "        \"breakdown\": breakdown\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1a4e69e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Score: 0.952\n",
      "{'path': 'payment_details.supplier[0]', 'pred': 'SHWAPNO (ACI Logistics Limited)', 'actual': 'SHWAPNO (ACI Logistics Limited)', 'comparator': 'exact', 'score': 1.0, 'weight': 1, 'weighted': 1.0}\n",
      "{'path': 'payment_details.payment_date[0]', 'pred': '2025-06-26', 'actual': '2025-06-26', 'comparator': 'exact', 'score': 1.0, 'weight': 1, 'weighted': 1.0}\n",
      "{'path': 'payment_details.payment_account[0]', 'pred': 'City Bank', 'actual': 'City Bank', 'comparator': 'embedding', 'score': 1.0, 'weight': 1, 'weighted': 1.0}\n",
      "{'path': 'payment_details.payment_method[0]', 'pred': 'Bank Transfer/Card (implied by City Bank)', 'actual': 'Bank Transfer/Card (implied by City Bank)', 'comparator': 'exact', 'score': 1.0, 'weight': 1, 'weighted': 1.0}\n",
      "{'path': 'payment_details.ref_no[0]', 'pred': 'D0612506260068', 'actual': 'D0612506260068', 'comparator': 'exact', 'score': 1.0, 'weight': 1, 'weighted': 1.0}\n",
      "{'path': 'payment_details.tags[0]', 'pred': 'Groceries', 'actual': 'Groceries', 'comparator': 'semantic', 'score': 1.0, 'weight': 1, 'weighted': 1.0}\n",
      "{'path': 'payment_details.tags[1]', 'pred': 'Supermarket', 'actual': 'Supermarket', 'comparator': 'semantic', 'score': 1.0, 'weight': 1, 'weighted': 1.0}\n",
      "{'path': 'payment_details.tags[2]', 'pred': 'Retail', 'actual': 'Retail', 'comparator': 'semantic', 'score': 1.0, 'weight': 1, 'weighted': 1.0}\n",
      "{'path': 'payment_details.tags[3]', 'pred': 'Food', 'actual': 'Food', 'comparator': 'semantic', 'score': 1.0, 'weight': 1, 'weighted': 1.0}\n",
      "{'path': 'item_details.category[0]', 'pred': 'Groceries', 'actual': 'Groceries', 'comparator': 'exact', 'score': 1.0, 'weight': 1, 'weighted': 1.0}\n",
      "{'path': 'item_details.category[1]', 'pred': 'Groceries', 'actual': 'Groceries', 'comparator': 'exact', 'score': 1.0, 'weight': 1, 'weighted': 1.0}\n",
      "{'path': 'item_details.category[2]', 'pred': 'Groceries', 'actual': 'Groceries', 'comparator': 'exact', 'score': 1.0, 'weight': 1, 'weighted': 1.0}\n",
      "{'path': 'item_details.category[3]', 'pred': 'Groceries', 'actual': 'Groceries', 'comparator': 'exact', 'score': 1.0, 'weight': 1, 'weighted': 1.0}\n",
      "{'path': 'item_details.category[4]', 'pred': 'Groceries', 'actual': 'Groceries', 'comparator': 'exact', 'score': 1.0, 'weight': 1, 'weighted': 1.0}\n",
      "{'path': 'item_details.category[5]', 'pred': 'Groceries', 'actual': 'Groceries', 'comparator': 'exact', 'score': 1.0, 'weight': 1, 'weighted': 1.0}\n",
      "{'path': 'item_details.category[6]', 'pred': 'Groceries', 'actual': 'Groceries', 'comparator': 'exact', 'score': 1.0, 'weight': 1, 'weighted': 1.0}\n",
      "{'path': 'item_details.category[7]', 'pred': 'Groceries', 'actual': 'Groceries', 'comparator': 'exact', 'score': 1.0, 'weight': 1, 'weighted': 1.0}\n",
      "{'path': 'item_details.category[8]', 'pred': 'Groceries', 'actual': 'Groceries', 'comparator': 'exact', 'score': 1.0, 'weight': 1, 'weighted': 1.0}\n",
      "{'path': 'item_details.category[9]', 'pred': 'Groceries', 'actual': 'Groceries', 'comparator': 'exact', 'score': 1.0, 'weight': 1, 'weighted': 1.0}\n",
      "{'path': 'item_details.category[10]', 'pred': 'Groceries', 'actual': 'Groceries', 'comparator': 'exact', 'score': 1.0, 'weight': 1, 'weighted': 1.0}\n",
      "{'path': 'item_details.category[11]', 'pred': 'Groceries', 'actual': 'Groceries', 'comparator': 'exact', 'score': 1.0, 'weight': 1, 'weighted': 1.0}\n",
      "{'path': 'item_details.category[12]', 'pred': 'Groceries', 'actual': 'Groceries', 'comparator': 'exact', 'score': 1.0, 'weight': 1, 'weighted': 1.0}\n",
      "{'path': 'item_details.description[0]', 'pred': 'Beef Premium Cube kg', 'actual': 'Beef Premium Cube kg', 'comparator': 'embedding', 'score': 1.0, 'weight': 1, 'weighted': 1.0}\n",
      "{'path': 'item_details.description[1]', 'pred': 'Broiler Chicken Breast Bon', 'actual': 'Broiler Chicken Breast Bon', 'comparator': 'embedding', 'score': 1.0, 'weight': 1, 'weighted': 1.0}\n",
      "{'path': 'item_details.description[2]', 'pred': 'Capsicum Green kg', 'actual': 'Capsicum Green kg', 'comparator': 'embedding', 'score': 1.0, 'weight': 1, 'weighted': 1.0}\n",
      "{'path': 'item_details.description[3]', 'pred': 'Carrot China (China Gajor) K', 'actual': 'Carrot China (China Gajor) K', 'comparator': 'embedding', 'score': 1.0, 'weight': 1, 'weighted': 1.0}\n",
      "{'path': 'item_details.description[4]', 'pred': 'Cucumber (Shosha) kg', 'actual': 'Cucumber (Shosha) kg', 'comparator': 'embedding', 'score': 1.0, 'weight': 1, 'weighted': 1.0}\n",
      "{'path': 'item_details.description[5]', 'pred': 'Green Chili (Kacha Morich)', 'actual': 'Green Chili (Kacha Morich)', 'comparator': 'embedding', 'score': 1.0, 'weight': 1, 'weighted': 1.0}\n",
      "{'path': 'item_details.description[6]', 'pred': 'Green Papaya (Kacha Pepe)', 'actual': 'Green Papaya (Kacha Pepe)', 'comparator': 'embedding', 'score': 1.0, 'weight': 1, 'weighted': 1.0}\n",
      "{'path': 'item_details.description[7]', 'pred': 'Lemon Long (Lomba Lebu) PC', 'actual': 'Lemon Long (Lomba Lebu) PC', 'comparator': 'embedding', 'score': 1.0, 'weight': 1, 'weighted': 1.0}\n",
      "{'path': 'item_details.description[8]', 'pred': 'Long Bean (Boroboti) KG', 'actual': 'Long Bean (Boroboti) KG', 'comparator': 'embedding', 'score': 1.0, 'weight': 1, 'weighted': 1.0}\n",
      "{'path': 'item_details.description[9]', 'pred': 'Piyaj Deshi Loose kg', 'actual': 'Piyaj Deshi Loose kg', 'comparator': 'embedding', 'score': 1.0, 'weight': 1, 'weighted': 1.0}\n",
      "{'path': 'item_details.description[10]', 'pred': 'Shonalika Dressed Classic', 'actual': 'Shonalika Dressed Classic', 'comparator': 'embedding', 'score': 1.0, 'weight': 1, 'weighted': 1.0}\n",
      "{'path': 'item_details.description[11]', 'pred': 'ACI Pure Corn Flour 150g', 'actual': 'ACI Pure Corn Flour 150g', 'comparator': 'embedding', 'score': 1.0, 'weight': 1, 'weighted': 1.0}\n",
      "{'path': 'item_details.description[12]', 'pred': 'Shwapno Black Pepper Powde', 'actual': 'Shwapno Black Pepper Powde', 'comparator': 'embedding', 'score': 1.0, 'weight': 1, 'weighted': 1.0}\n",
      "{'path': 'item_details.quantity[0]', 'pred': 6.33, 'actual': 6.33, 'comparator': 'numeric', 'score': 1.0, 'weight': 1, 'weighted': 1.0}\n",
      "{'path': 'item_details.quantity[1]', 'pred': 0.5, 'actual': 0.5, 'comparator': 'numeric', 'score': 1.0, 'weight': 1, 'weighted': 1.0}\n",
      "{'path': 'item_details.quantity[2]', 'pred': 1.01, 'actual': 1.01, 'comparator': 'numeric', 'score': 1.0, 'weight': 1, 'weighted': 1.0}\n",
      "{'path': 'item_details.quantity[3]', 'pred': 1.4, 'actual': 1.4, 'comparator': 'numeric', 'score': 1.0, 'weight': 1, 'weighted': 1.0}\n",
      "{'path': 'item_details.quantity[4]', 'pred': 2.69, 'actual': 2.69, 'comparator': 'numeric', 'score': 1.0, 'weight': 1, 'weighted': 1.0}\n",
      "{'path': 'item_details.quantity[5]', 'pred': 0.67, 'actual': 0.67, 'comparator': 'numeric', 'score': 1.0, 'weight': 1, 'weighted': 1.0}\n",
      "{'path': 'item_details.quantity[6]', 'pred': 3.0, 'actual': 3.0, 'comparator': 'numeric', 'score': 1.0, 'weight': 1, 'weighted': 1.0}\n",
      "{'path': 'item_details.quantity[7]', 'pred': 8.0, 'actual': 8.0, 'comparator': 'numeric', 'score': 1.0, 'weight': 1, 'weighted': 1.0}\n",
      "{'path': 'item_details.quantity[8]', 'pred': 1.83, 'actual': 1.83, 'comparator': 'numeric', 'score': 1.0, 'weight': 1, 'weighted': 1.0}\n",
      "{'path': 'item_details.quantity[9]', 'pred': 2.66, 'actual': 2.66, 'comparator': 'numeric', 'score': 1.0, 'weight': 1, 'weighted': 1.0}\n",
      "{'path': 'item_details.quantity[10]', 'pred': 1.65, 'actual': 1.65, 'comparator': 'numeric', 'score': 1.0, 'weight': 1, 'weighted': 1.0}\n",
      "{'path': 'item_details.quantity[11]', 'pred': 1.0, 'actual': 1.0, 'comparator': 'numeric', 'score': 1.0, 'weight': 1, 'weighted': 1.0}\n",
      "{'path': 'item_details.quantity[12]', 'pred': 1.0, 'actual': 1.0, 'comparator': 'numeric', 'score': 1.0, 'weight': 1, 'weighted': 1.0}\n",
      "{'path': 'item_details.unit_price[0]', 'pred': 775.0, 'actual': 775.0, 'comparator': 'numeric', 'score': 1.0, 'weight': 1, 'weighted': 1.0}\n",
      "{'path': 'item_details.unit_price[1]', 'pred': 534.0, 'actual': 534.0, 'comparator': 'numeric', 'score': 1.0, 'weight': 1, 'weighted': 1.0}\n",
      "{'path': 'item_details.unit_price[2]', 'pred': 320.0, 'actual': 320.0, 'comparator': 'numeric', 'score': 1.0, 'weight': 1, 'weighted': 1.0}\n",
      "{'path': 'item_details.unit_price[3]', 'pred': 160.0, 'actual': 160.0, 'comparator': 'numeric', 'score': 1.0, 'weight': 1, 'weighted': 1.0}\n",
      "{'path': 'item_details.unit_price[4]', 'pred': 55.0, 'actual': 55.0, 'comparator': 'numeric', 'score': 1.0, 'weight': 1, 'weighted': 1.0}\n",
      "{'path': 'item_details.unit_price[5]', 'pred': 100.0, 'actual': 100.0, 'comparator': 'numeric', 'score': 1.0, 'weight': 1, 'weighted': 1.0}\n",
      "{'path': 'item_details.unit_price[6]', 'pred': 45.0, 'actual': 45.0, 'comparator': 'numeric', 'score': 1.0, 'weight': 1, 'weighted': 1.0}\n",
      "{'path': 'item_details.unit_price[7]', 'pred': 6.0, 'actual': 6.0, 'comparator': 'numeric', 'score': 1.0, 'weight': 1, 'weighted': 1.0}\n",
      "{'path': 'item_details.unit_price[8]', 'pred': 90.0, 'actual': 90.0, 'comparator': 'numeric', 'score': 1.0, 'weight': 1, 'weighted': 1.0}\n",
      "{'path': 'item_details.unit_price[9]', 'pred': 58.0, 'actual': 58.0, 'comparator': 'numeric', 'score': 1.0, 'weight': 1, 'weighted': 1.0}\n",
      "{'path': 'item_details.unit_price[10]', 'pred': 667.0, 'actual': 667.0, 'comparator': 'numeric', 'score': 1.0, 'weight': 1, 'weighted': 1.0}\n",
      "{'path': 'item_details.unit_price[11]', 'pred': 60.0, 'actual': 60.0, 'comparator': 'numeric', 'score': 1.0, 'weight': 1, 'weighted': 1.0}\n",
      "{'path': 'item_details.unit_price[12]', 'pred': 129.0, 'actual': 129.0, 'comparator': 'numeric', 'score': 1.0, 'weight': 1, 'weighted': 1.0}\n",
      "{'path': 'item_details.total[0]', 'pred': 4901.88, 'actual': 4901.88, 'comparator': 'numeric', 'score': 1.0, 'weight': 1, 'weighted': 1.0}\n",
      "{'path': 'item_details.total[1]', 'pred': 267.0, 'actual': 267.0, 'comparator': 'numeric', 'score': 1.0, 'weight': 1, 'weighted': 1.0}\n",
      "{'path': 'item_details.total[2]', 'pred': 321.6, 'actual': 321.6, 'comparator': 'numeric', 'score': 1.0, 'weight': 1, 'weighted': 1.0}\n",
      "{'path': 'item_details.total[3]', 'pred': 224.0, 'actual': 224.0, 'comparator': 'numeric', 'score': 1.0, 'weight': 1, 'weighted': 1.0}\n",
      "{'path': 'item_details.total[4]', 'pred': 147.68, 'actual': 147.68, 'comparator': 'numeric', 'score': 1.0, 'weight': 1, 'weighted': 1.0}\n",
      "{'path': 'item_details.total[5]', 'pred': 66.5, 'actual': 66.5, 'comparator': 'numeric', 'score': 1.0, 'weight': 1, 'weighted': 1.0}\n",
      "{'path': 'item_details.total[6]', 'pred': 134.78, 'actual': 134.78, 'comparator': 'numeric', 'score': 1.0, 'weight': 1, 'weighted': 1.0}\n",
      "{'path': 'item_details.total[7]', 'pred': 48.0, 'actual': 48.0, 'comparator': 'numeric', 'score': 1.0, 'weight': 1, 'weighted': 1.0}\n",
      "{'path': 'item_details.total[8]', 'pred': 164.7, 'actual': 164.7, 'comparator': 'numeric', 'score': 1.0, 'weight': 1, 'weighted': 1.0}\n",
      "{'path': 'item_details.total[9]', 'pred': 154.28, 'actual': 154.28, 'comparator': 'numeric', 'score': 1.0, 'weight': 1, 'weighted': 1.0}\n",
      "{'path': 'item_details.total[10]', 'pred': 1097.22, 'actual': 1097.22, 'comparator': 'numeric', 'score': 1.0, 'weight': 1, 'weighted': 1.0}\n",
      "{'path': 'item_details.total[11]', 'pred': 60.0, 'actual': 60.0, 'comparator': 'numeric', 'score': 1.0, 'weight': 1, 'weighted': 1.0}\n",
      "{'path': 'item_details.total[12]', 'pred': 129.0, 'actual': 129.0, 'comparator': 'numeric', 'score': 1.0, 'weight': 1, 'weighted': 1.0}\n",
      "{'path': 'attachment[0]', 'pred': False, 'actual': False, 'comparator': 'exact', 'score': 1.0, 'weight': 1, 'weighted': 1.0}\n",
      "{'path': 'memo[0]', 'pred': 'Thank you for shopping with SHWAPNO. Please visit www.shwapno.com for home delivery. Purchase of defected item must be exchanged by 24 hours with invoice. For any queries, suggestions or complaints, please call 16469 (9:00 AM - 6:00 PM). Earned points will expire within 6 months from the date of the transaction if not redeemed. VAT against this challan is payable through central registration. Prices inclusive of standard VAT except exempted items, VAT Payable TK. : 7.17.', 'actual': 'Thank you for shopping with SHWAPNO. Please visit www.shwapno.com for home delivery. Purchase of defected item must be exchanged by 24 hours with invoice. For any queries, suggestions or complaints, please call 16469 (9:00 AM - 6:00 PM). Earned points will expire within 6 months from the date of the transaction if not redeemed. VAT against this challan is payable through central registration. Prices inclusive of standard VAT except exempted items, VAT Payable TK. : 7.17.', 'comparator': 'embedding', 'score': 1.0, 'weight': 1, 'weighted': 1.0}\n",
      "{'path': 'totals.sub_total[0]', 'pred': 7716.64, 'actual': 7716.64, 'comparator': 'numeric', 'score': 1.0, 'weight': 1, 'weighted': 1.0}\n",
      "{'path': 'totals.sales_tax.is_percentage[0]', 'pred': False, 'actual': False, 'comparator': 'exact', 'score': 1.0, 'weight': 1, 'weighted': 1.0}\n",
      "{'path': 'totals.sales_tax.tax[0]', 'pred': 0, 'actual': 0, 'comparator': 'numeric', 'score': 1.0, 'weight': 1, 'weighted': 1.0}\n",
      "{'path': 'totals.sales_tax.amount[0]', 'pred': 0, 'actual': 7.17, 'comparator': 'numeric', 'score': 0.0, 'weight': 1, 'weighted': 0.0}\n",
      "{'path': 'totals.discount.is_percentage[0]', 'pred': False, 'actual': False, 'comparator': 'exact', 'score': 1.0, 'weight': 1, 'weighted': 1.0}\n",
      "{'path': 'totals.discount.discount[0]', 'pred': 0, 'actual': 0, 'comparator': 'numeric', 'score': 1.0, 'weight': 1, 'weighted': 1.0}\n",
      "{'path': 'totals.discount.calculated_amount[0]', 'pred': 30.8, 'actual': 30.8, 'comparator': 'numeric', 'score': 1.0, 'weight': 1, 'weighted': 1.0}\n",
      "{'path': 'totals.total_amount[0]', 'pred': 7686.0, 'actual': 7686.0, 'comparator': 'numeric', 'score': 1.0, 'weight': 1, 'weighted': 1.0}\n"
     ]
    }
   ],
   "source": [
    "config = [\n",
    "    {\"path\": \"payment_details.supplier\", \"weight\": 1, \"comparator\": \"exact\"},\n",
    "    {\"path\": \"payment_details.payment_date\", \"weight\": 1, \"comparator\": \"exact\"},\n",
    "    {\"path\": \"payment_details.payment_account\", \"weight\": 1, \"comparator\": \"embedding\"},\n",
    "    {\"path\": \"payment_details.payment_method\", \"weight\": 1, \"comparator\": \"exact\"},\n",
    "    {\"path\": \"payment_details.ref_no\", \"weight\": 1, \"comparator\": \"exact\"},\n",
    "    {\"path\": \"payment_details.tags\", \"weight\": 1, \"comparator\": \"semantic\"},\n",
    "    {\"path\": \"item_details.category\", \"weight\": 1, \"comparator\": \"exact\"},\n",
    "    {\"path\": \"item_details.description\", \"weight\": 1, \"comparator\": \"embedding\"},\n",
    "    {\"path\": \"item_details.quantity\", \"weight\": 1, \"comparator\": \"numeric\", \"tolerance\": 0.05},\n",
    "    {\"path\": \"item_details.unit_price\", \"weight\": 1, \"comparator\": \"numeric\", \"tolerance\": 0.1},\n",
    "    {\"path\": \"item_details.total\", \"weight\": 1, \"comparator\": \"numeric\", \"tolerance\": 0.1},\n",
    "    {\"path\": \"attachment\", \"weight\": 1, \"comparator\": \"exact\"},\n",
    "    {\"path\": \"memo\", \"weight\": 1, \"comparator\": \"embedding\"},\n",
    "    {\"path\": \"totals.sub_total\", \"weight\": 1, \"comparator\": \"numeric\", \"tolerance\": 0.1},\n",
    "    {\"path\": \"totals.sales_tax.is_percentage\", \"weight\": 1, \"comparator\": \"exact\"},\n",
    "    {\"path\": \"totals.sales_tax.tax\", \"weight\": 1, \"comparator\": \"numeric\", \"tolerance\": 0.1},\n",
    "    {\"path\": \"totals.sales_tax.amount\", \"weight\": 1, \"comparator\": \"numeric\", \"tolerance\": 0.1},\n",
    "    {\"path\": \"totals.discount.is_percentage\", \"weight\": 1, \"comparator\": \"exact\"},\n",
    "    {\"path\": \"totals.discount.discount\", \"weight\": 1, \"comparator\": \"numeric\", \"tolerance\": 0.1},\n",
    "    {\"path\": \"totals.discount.calculated_amount\", \"weight\": 1, \"comparator\": \"numeric\", \"tolerance\": 0.1},\n",
    "    {\"path\": \"totals.total_amount\", \"weight\": 1, \"comparator\": \"numeric\", \"tolerance\": 0.1},\n",
    "]\n",
    "\n",
    "\n",
    "result = score_json(output_json_2, actual_json_2, config)\n",
    "\n",
    "print(\"Final Score:\", result[\"final_score\"])\n",
    "for r in result[\"breakdown\"]:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "75289b49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "BATCH SCORING RESULTS\n",
      "================================================================================\n",
      "üìä IMG_20250826_154308~2.jpg: 0.952\n",
      "üìä IMG_20250826_155754.jpg: 0.846\n",
      "\n",
      "================================================================================\n",
      "SUMMARY STATISTICS\n",
      "================================================================================\n",
      "üìà Total Images Processed: 2\n",
      "üìä Average Score: 0.899\n",
      "üéØ Total Score Sum: 1.798\n",
      "üìâ Lowest Score: 0.846\n",
      "üìà Highest Score: 0.952\n",
      "\n",
      "üìä Score Distribution:\n",
      "   0.0-0.2: 0 images\n",
      "   0.2-0.4: 0 images\n",
      "   0.4-0.6: 0 images\n",
      "   0.6-0.8: 0 images\n",
      "   0.8-1.0: 2 images\n",
      "\n",
      "üíæ Detailed results saved to: /home/tanjim_noor/Work/AI OCR BenchMark/OCR_Output/batch_scoring_results.json\n"
     ]
    }
   ],
   "source": [
    "# Batch scoring for all images in JSON files\n",
    "import json\n",
    "import os\n",
    "\n",
    "def score_all_images(results_json_path, actual_json_path, config):\n",
    "    \"\"\"\n",
    "    Score all images by comparing results JSON with actual JSON\n",
    "    \n",
    "    Args:\n",
    "        results_json_path: Path to the OCR results JSON file\n",
    "        actual_json_path: Path to the actual/ground truth JSON file\n",
    "        config: Configuration list for scoring (same as above)\n",
    "    \n",
    "    Returns:\n",
    "        Dict with overall statistics and individual image scores\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load JSON files\n",
    "    try:\n",
    "        with open(results_json_path, 'r') as f:\n",
    "            results_data = json.load(f)\n",
    "        with open(actual_json_path, 'r') as f:\n",
    "            actual_data = json.load(f)\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: File not found - {e}\")\n",
    "        return None\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error: Invalid JSON - {e}\")\n",
    "        return None\n",
    "    \n",
    "    # Create lookup dictionary for actual data by image path\n",
    "    actual_lookup = {item['image_path']: item for item in actual_data}\n",
    "    \n",
    "    # Score each image\n",
    "    image_scores = []\n",
    "    total_score = 0\n",
    "    processed_count = 0\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"BATCH SCORING RESULTS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for result_item in results_data:\n",
    "        image_path = result_item['image_path']\n",
    "        image_name = os.path.basename(image_path)\n",
    "        \n",
    "        # Find corresponding actual data\n",
    "        if image_path not in actual_lookup:\n",
    "            print(f\"‚ùå No ground truth found for: {image_name}\")\n",
    "            continue\n",
    "            \n",
    "        actual_item = actual_lookup[image_path]\n",
    "        \n",
    "        # Extract structured data for comparison\n",
    "        result_structured = result_item.get('structured_data', {})\n",
    "        actual_structured = actual_item.get('structured_data', {})\n",
    "        \n",
    "        # Score this image\n",
    "        if result_structured and actual_structured:\n",
    "            score_result = score_json(result_structured, actual_structured, config)\n",
    "            final_score = score_result['final_score']\n",
    "            \n",
    "            image_scores.append({\n",
    "                'image_name': image_name,\n",
    "                'image_path': image_path,\n",
    "                'score': final_score,\n",
    "                'breakdown': score_result['breakdown']\n",
    "            })\n",
    "            \n",
    "            total_score += final_score\n",
    "            processed_count += 1\n",
    "            \n",
    "            # Print score for this image\n",
    "            print(f\"üìä {image_name}: {final_score:.3f}\")\n",
    "            \n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è  Missing structured data for: {image_name}\")\n",
    "    \n",
    "    # Calculate overall statistics\n",
    "    if processed_count > 0:\n",
    "        average_score = total_score / processed_count\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"SUMMARY STATISTICS\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"üìà Total Images Processed: {processed_count}\")\n",
    "        print(f\"üìä Average Score: {average_score:.3f}\")\n",
    "        print(f\"üéØ Total Score Sum: {total_score:.3f}\")\n",
    "        print(f\"üìâ Lowest Score: {min(item['score'] for item in image_scores):.3f}\")\n",
    "        print(f\"üìà Highest Score: {max(item['score'] for item in image_scores):.3f}\")\n",
    "        \n",
    "        # Show breakdown of scores\n",
    "        score_ranges = {'0.0-0.2': 0, '0.2-0.4': 0, '0.4-0.6': 0, '0.6-0.8': 0, '0.8-1.0': 0}\n",
    "        for item in image_scores:\n",
    "            score = item['score']\n",
    "            if score < 0.2:\n",
    "                score_ranges['0.0-0.2'] += 1\n",
    "            elif score < 0.4:\n",
    "                score_ranges['0.2-0.4'] += 1\n",
    "            elif score < 0.6:\n",
    "                score_ranges['0.4-0.6'] += 1\n",
    "            elif score < 0.8:\n",
    "                score_ranges['0.6-0.8'] += 1\n",
    "            else:\n",
    "                score_ranges['0.8-1.0'] += 1\n",
    "        \n",
    "        print(\"\\nüìä Score Distribution:\")\n",
    "        for range_name, count in score_ranges.items():\n",
    "            print(f\"   {range_name}: {count} images\")\n",
    "            \n",
    "    else:\n",
    "        average_score = 0\n",
    "        print(\"‚ùå No images were processed successfully\")\n",
    "    \n",
    "    return {\n",
    "        'processed_count': processed_count,\n",
    "        'average_score': average_score,\n",
    "        'total_score': total_score,\n",
    "        'image_scores': image_scores\n",
    "    }\n",
    "\n",
    "# Example usage:\n",
    "# Set the paths to your JSON files\n",
    "results_json_path = \"/home/tanjim_noor/Work/AI OCR BenchMark/OCR_Output/ocr_result_1_structured_only.json\"\n",
    "actual_json_path = \"/home/tanjim_noor/Work/AI OCR BenchMark/OCR_Output/ocr_actual_1_structured_only.json\"\n",
    "\n",
    "# Use the same config as defined above\n",
    "batch_results = score_all_images(results_json_path, actual_json_path, config)\n",
    "\n",
    "# Optionally, save detailed results to a file\n",
    "if batch_results:\n",
    "    output_path = \"/home/tanjim_noor/Work/AI OCR BenchMark/OCR_Output/batch_scoring_results.json\"\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(batch_results, f, indent=2)\n",
    "    print(f\"\\nüíæ Detailed results saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1f2bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Process all images in directory (Batch Processing)\n",
    "# This will process all images in the Pay Slip directory\n",
    "\n",
    "# Uncomment the following code to process all images:\n",
    "\"\"\"\n",
    "try:\n",
    "    pay_slip_dir = \"/home/tanjim_noor/Work/AI OCR BenchMark/Pay Slip\"\n",
    "    output_file = \"/home/tanjim_noor/Work/AI OCR BenchMark/ocr_results.json\"\n",
    "    \n",
    "    print(\"Starting batch processing...\")\n",
    "    print(f\"Input directory: {pay_slip_dir}\")\n",
    "    print(f\"Output file: {output_file}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Process all images with payslip extraction prompt\n",
    "    results = ocr.process_directory(\n",
    "        directory_path=pay_slip_dir,\n",
    "        prompt=sample_prompts[\"payslip_extraction\"],\n",
    "        output_file=output_file\n",
    "    )\n",
    "    \n",
    "    # Summary\n",
    "    successful = sum(1 for r in results if r[\"success\"])\n",
    "    failed = len(results) - successful\n",
    "    \n",
    "    print(\"=\" * 50)\n",
    "    print(f\"üìä PROCESSING SUMMARY:\")\n",
    "    print(f\"Total images: {len(results)}\")\n",
    "    print(f\"Successful: {successful}\")\n",
    "    print(f\"Failed: {failed}\")\n",
    "    print(f\"Results saved to: {output_file}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error during batch processing: {e}\")\n",
    "\"\"\"\n",
    "\n",
    "print(\"Batch processing code is commented out.\")\n",
    "print(\"Uncomment the code above to process all images in the directory.\")\n",
    "print(\"Note: This will use API calls for each image, so be mindful of costs!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7628ac10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Switch between different models\n",
    "# Demonstrate model-agnostic capabilities\n",
    "\n",
    "print(\"üîÑ Model Switching Examples\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Current model\n",
    "print(f\"Current model: {ocr.model_name}\")\n",
    "\n",
    "print(\"\\nüìù Available models:\")\n",
    "print(\"1. gemini - Google Gemini 1.5 Flash\")\n",
    "print(\"2. openai - OpenAI GPT-4 Vision\")  \n",
    "print(\"3. anthropic - Claude 3.5 Sonnet\")\n",
    "\n",
    "print(\"\\nüîß How to switch models:\")\n",
    "print(\"ocr.change_model('openai')    # Switch to OpenAI\")\n",
    "print(\"ocr.change_model('anthropic') # Switch to Claude\")\n",
    "print(\"ocr.change_model('gemini')    # Switch to Gemini\")\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è Note: Make sure to set the corresponding API keys in .env file\")\n",
    "\n",
    "# Example of switching (commented out)\n",
    "\"\"\"\n",
    "# Switch to OpenAI\n",
    "try:\n",
    "    ocr.change_model('openai')\n",
    "    # Process an image with OpenAI\n",
    "    result = ocr.extract_data_from_image(image_path, prompt)\n",
    "except Exception as e:\n",
    "    print(f\"OpenAI error: {e}\")\n",
    "\n",
    "# Switch to Claude\n",
    "try:\n",
    "    ocr.change_model('anthropic')\n",
    "    # Process an image with Claude\n",
    "    result = ocr.extract_data_from_image(image_path, prompt)\n",
    "except Exception as e:\n",
    "    print(f\"Claude error: {e}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848af369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 4: Custom Prompts and Utilities\n",
    "\n",
    "def create_custom_prompt(task_type: str, specific_fields: Optional[List[str]] = None) -> str:\n",
    "    \"\"\"\n",
    "    Helper function to create custom prompts for different OCR tasks.\n",
    "    \n",
    "    Args:\n",
    "        task_type: Type of document ('invoice', 'receipt', 'form', 'table', 'general')\n",
    "        specific_fields: List of specific fields to extract\n",
    "    \"\"\"\n",
    "    base_prompts = {\n",
    "        'invoice': \"Extract invoice information including invoice number, date, vendor, amount, tax, and line items.\",\n",
    "        'receipt': \"Extract receipt information including store name, date, items purchased, prices, and total amount.\",\n",
    "        'form': \"Extract all form fields and their values from this document.\",\n",
    "        'table': \"Extract table data and organize it in a structured format with headers and rows.\",\n",
    "        'general': \"Extract all text content from this image, preserving structure and formatting.\"\n",
    "    }\n",
    "    \n",
    "    prompt = base_prompts.get(task_type, base_prompts['general'])\n",
    "    \n",
    "    if specific_fields:\n",
    "        fields_str = \", \".join(specific_fields)\n",
    "        prompt += f\" Focus specifically on extracting: {fields_str}.\"\n",
    "    \n",
    "    prompt += \" Present the information in a clear, structured format.\"\n",
    "    return prompt\n",
    "\n",
    "# Example custom prompts\n",
    "print(\"üéØ Custom Prompt Examples\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Create some custom prompts\n",
    "custom_prompts = {\n",
    "    \"payslip_detailed\": create_custom_prompt(\"form\", [\n",
    "        \"employee name\", \"employee ID\", \"salary period\", \n",
    "        \"gross pay\", \"net pay\", \"tax deductions\", \"company name\"\n",
    "    ]),\n",
    "    \n",
    "    \"receipt_analysis\": create_custom_prompt(\"receipt\", [\n",
    "        \"store name\", \"purchase date\", \"items\", \"total amount\", \"payment method\"\n",
    "    ]),\n",
    "    \n",
    "    \"table_extraction\": create_custom_prompt(\"table\"),\n",
    "    \n",
    "    \"general_ocr\": create_custom_prompt(\"general\")\n",
    "}\n",
    "\n",
    "for name, prompt in custom_prompts.items():\n",
    "    print(f\"\\nüìù {name.upper()}:\")\n",
    "    print(f\"   {prompt}\")\n",
    "\n",
    "print(\"\\n‚ú® You can use these prompts with:\")\n",
    "print(\"   result = ocr.extract_data_from_image(image_path, custom_prompts['payslip_detailed'])\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bcf62d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 5: Benchmarking Multiple Models\n",
    "# Compare results from different models on the same image\n",
    "\n",
    "def benchmark_models(image_path: str, prompt: str, models: Optional[List[str]] = None) -> Dict:\n",
    "    \"\"\"\n",
    "    Benchmark multiple models on the same image and prompt.\n",
    "    \n",
    "    Args:\n",
    "        image_path: Path to the image file\n",
    "        prompt: Prompt to use for extraction\n",
    "        models: List of models to test (defaults to available models)\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with results from each model\n",
    "    \"\"\"\n",
    "    if models is None:\n",
    "        models = [\"gemini\", \"openai\", \"anthropic\"]\n",
    "    \n",
    "    benchmark_results = {\n",
    "        \"image_path\": image_path,\n",
    "        \"prompt\": prompt,\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"results\": {}\n",
    "    }\n",
    "    \n",
    "    for model_name in models:\n",
    "        print(f\"Testing {model_name}...\")\n",
    "        try:\n",
    "            # Create new OCR instance for each model\n",
    "            test_ocr = OCRBenchmark(model_name=model_name, temperature=0.1)\n",
    "            \n",
    "            # Check if model was initialized successfully\n",
    "            if test_ocr.model is None:\n",
    "                print(f\"‚ö†Ô∏è {model_name} - Skipped: API key not available\")\n",
    "                benchmark_results[\"results\"][model_name] = {\n",
    "                    \"success\": False,\n",
    "                    \"error\": f\"API key not available for {model_name}\",\n",
    "                    \"model_used\": model_name\n",
    "                }\n",
    "                continue\n",
    "            \n",
    "            result = test_ocr.extract_data_from_image(image_path, prompt)\n",
    "            benchmark_results[\"results\"][model_name] = result\n",
    "            \n",
    "            if result[\"success\"]:\n",
    "                print(f\"‚úÖ {model_name} - Success\")\n",
    "            else:\n",
    "                print(f\"‚ùå {model_name} - Failed: {result.get('error', 'Unknown error')}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå {model_name} - Error: {str(e)}\")\n",
    "            benchmark_results[\"results\"][model_name] = {\n",
    "                \"success\": False,\n",
    "                \"error\": str(e),\n",
    "                \"model_used\": model_name\n",
    "            }\n",
    "    \n",
    "    return benchmark_results\n",
    "\n",
    "# Utility function to compare and analyze results\n",
    "def analyze_benchmark_results(benchmark_results: Dict):\n",
    "    \"\"\"Analyze and display benchmark results.\"\"\"\n",
    "    print(\"\\nüìä BENCHMARK ANALYSIS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    results = benchmark_results[\"results\"]\n",
    "    successful_models = [model for model, result in results.items() if result.get(\"success\", False)]\n",
    "    failed_models = [model for model, result in results.items() if not result.get(\"success\", False)]\n",
    "    \n",
    "    print(f\"Image: {Path(benchmark_results['image_path']).name}\")\n",
    "    print(f\"Total models tested: {len(results)}\")\n",
    "    print(f\"Successful: {len(successful_models)} - {successful_models}\")\n",
    "    print(f\"Failed: {len(failed_models)} - {failed_models}\")\n",
    "    \n",
    "    if successful_models:\n",
    "        print(\"\\nüìù Extracted Data Comparison:\")\n",
    "        for model in successful_models:\n",
    "            print(f\"\\nü§ñ {model.upper()}:\")\n",
    "            extracted_data = results[model][\"extracted_data\"]\n",
    "            # Show first 200 characters\n",
    "            preview = extracted_data[:200] + \"...\" if len(extracted_data) > 200 else extracted_data\n",
    "            print(f\"   {preview}\")\n",
    "\n",
    "print(\"üöÄ Benchmarking utilities defined!\")\n",
    "print(\"Use benchmark_models() to compare multiple models on the same image.\")\n",
    "\n",
    "# Example usage (commented out):\n",
    "\"\"\"\n",
    "# Benchmark all models on a single image\n",
    "image_path = \"/path/to/your/image.png\"\n",
    "prompt = sample_prompts[\"payslip_extraction\"]\n",
    "\n",
    "benchmark_results = benchmark_models(image_path, prompt)\n",
    "analyze_benchmark_results(benchmark_results)\n",
    "\n",
    "# Save benchmark results\n",
    "with open(\"benchmark_results.json\", \"w\") as f:\n",
    "    json.dump(benchmark_results, f, indent=2)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6deb2eb7",
   "metadata": {},
   "source": [
    "# üöÄ Quick Start Guide\n",
    "\n",
    "## Setup Instructions\n",
    "\n",
    "1. **Set up API Keys** - Add your API keys to the `.env` file:\n",
    "   ```bash\n",
    "   GOOGLE_API_KEY=your_actual_google_api_key\n",
    "   OPENAI_API_KEY=your_actual_openai_api_key  \n",
    "   ANTHROPIC_API_KEY=your_actual_anthropic_api_key\n",
    "   ```\n",
    "\n",
    "2. **Initialize the OCR System**:\n",
    "   ```python\n",
    "   ocr = OCRBenchmark(model_name=\"gemini\")  # Default: Gemini\n",
    "   ```\n",
    "\n",
    "3. **Process a Single Image**:\n",
    "   ```python\n",
    "   result = ocr.extract_data_from_image(\"path/to/image.png\", \"Extract all text\")\n",
    "   ```\n",
    "\n",
    "4. **Process All Images in Directory**:\n",
    "   ```python\n",
    "   results = ocr.process_directory(\"path/to/directory\", \"Extract payslip data\")\n",
    "   ```\n",
    "\n",
    "## Available Models\n",
    "- **gemini** - Google Gemini 1.5 Flash (Default)\n",
    "- **openai** - OpenAI GPT-4 Vision  \n",
    "- **anthropic** - Claude 3.5 Sonnet\n",
    "\n",
    "## Features\n",
    "‚úÖ **Model Agnostic** - Switch between models easily  \n",
    "‚úÖ **Batch Processing** - Process entire directories  \n",
    "‚úÖ **Custom Prompts** - Flexible prompt engineering  \n",
    "‚úÖ **JSON Output** - Save results to files  \n",
    "‚úÖ **Error Handling** - Robust error management  \n",
    "‚úÖ **Benchmarking** - Compare model performance  \n",
    "\n",
    "## Next Steps\n",
    "1. Set your API keys in `.env`\n",
    "2. Run the initialization cell\n",
    "3. Try processing a single image first\n",
    "4. Experiment with different prompts\n",
    "5. Compare results across models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fdc6753b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîë API Keys Status Check\n",
      "==============================\n",
      "Available models:\n",
      "  gemini: ‚úÖ Available\n",
      "  openai: ‚ùå Missing API Key\n",
      "  anthropic: ‚ùå Missing API Key\n",
      "\n",
      "Recommended model: gemini\n",
      "\n",
      "‚úÖ 1 out of 3 models available!\n"
     ]
    }
   ],
   "source": [
    "# Check API Keys Availability\n",
    "print(\"üîë API Keys Status Check\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Create a temporary OCR instance to check API keys\n",
    "temp_ocr = OCRBenchmark()\n",
    "api_status = temp_ocr.check_api_keys()\n",
    "\n",
    "print(\"Available models:\")\n",
    "for model, available in api_status.items():\n",
    "    status = \"‚úÖ Available\" if available else \"‚ùå Missing API Key\"\n",
    "    print(f\"  {model}: {status}\")\n",
    "\n",
    "print(f\"\\nRecommended model: {next((model for model, available in api_status.items() if available), 'None available')}\")\n",
    "\n",
    "if not any(api_status.values()):\n",
    "    print(\"\\n‚ö†Ô∏è No API keys found!\")\n",
    "    print(\"Please set at least one API key in your .env file:\")\n",
    "    print(\"  GOOGLE_API_KEY=your_key_here\")\n",
    "    print(\"  OPENAI_API_KEY=your_key_here\") \n",
    "    print(\"  ANTHROPIC_API_KEY=your_key_here\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ {sum(api_status.values())} out of {len(api_status)} models available!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b0616b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test JPG file support and MIME type handling\n",
    "print(\"üîç Testing JPG File Support\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Show supported formats\n",
    "print(\"Supported image formats:\")\n",
    "for fmt in sorted(ocr.supported_formats):\n",
    "    print(f\"  {fmt}\")\n",
    "\n",
    "print(\"\\nMIME type mappings:\")\n",
    "for ext, mime in ocr.mime_type_map.items():\n",
    "    print(f\"  {ext} ‚Üí image/{mime}\")\n",
    "\n",
    "print(\"\\nüìÅ Checking for JPG files in your directories...\")\n",
    "\n",
    "# Check both directories for JPG files\n",
    "directories_to_check = [\n",
    "    \"/home/tanjim_noor/Work/AI OCR BenchMark/Pay Slip\",\n",
    "    \"/home/tanjim_noor/Work/AI OCR BenchMark/test png\"\n",
    "]\n",
    "\n",
    "for directory in directories_to_check:\n",
    "    if os.path.exists(directory):\n",
    "        print(f\"\\nüìÇ {directory}:\")\n",
    "        try:\n",
    "            all_files = list(Path(directory).iterdir())\n",
    "            jpg_files = [f for f in all_files if f.suffix.lower() in ['.jpg', '.jpeg']]\n",
    "            \n",
    "            if jpg_files:\n",
    "                print(f\"  Found {len(jpg_files)} JPG/JPEG file(s):\")\n",
    "                for jpg_file in jpg_files[:5]:  # Show first 5\n",
    "                    file_ext = jpg_file.suffix.lower()\n",
    "                    mime_type = ocr.mime_type_map.get(file_ext, file_ext[1:])\n",
    "                    print(f\"    {jpg_file.name} ({file_ext} ‚Üí image/{mime_type})\")\n",
    "                if len(jpg_files) > 5:\n",
    "                    print(f\"    ... and {len(jpg_files) - 5} more\")\n",
    "            else:\n",
    "                print(\"  No JPG files found\")\n",
    "                \n",
    "            # Show all supported image files\n",
    "            supported_images = ocr.get_images_from_directory(directory)\n",
    "            if supported_images:\n",
    "                print(f\"  Total supported images: {len(supported_images)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Error reading directory: {e}\")\n",
    "    else:\n",
    "        print(f\"\\nüìÇ {directory}: Directory not found\")\n",
    "\n",
    "print(f\"\\n‚úÖ JPG files are fully supported!\")\n",
    "print(\"The system will automatically convert .jpg extensions to 'image/jpeg' MIME type.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cd97e3fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Testing MIME Type Mapping\n",
      "==============================\n",
      ".jpg ‚Üí image/jpeg\n",
      ".jpeg ‚Üí image/jpeg\n",
      ".png ‚Üí image/png\n",
      ".webp ‚Üí image/webp\n",
      ".gif ‚Üí image/gif\n",
      "\n",
      "üìÅ Your file: IMG_20250826_154052.jpg\n",
      "Extension: .jpg\n",
      "MIME type: image/jpeg\n",
      "Should be: image/jpeg\n",
      "Mapping works: ‚úÖ\n"
     ]
    }
   ],
   "source": [
    "# Quick MIME type test\n",
    "print(\"üß™ Testing MIME Type Mapping\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "test_extensions = ['.jpg', '.jpeg', '.png', '.webp', '.gif']\n",
    "for ext in test_extensions:\n",
    "    mime_type = ocr.mime_type_map.get(ext, ext[1:])\n",
    "    print(f\"{ext} ‚Üí image/{mime_type}\")\n",
    "\n",
    "# Test with your specific file\n",
    "test_file = \"/home/tanjim_noor/Work/AI OCR BenchMark/Pay Slip/IMG_20250826_154052.jpg\"\n",
    "if os.path.exists(test_file):\n",
    "    file_ext = Path(test_file).suffix.lower()\n",
    "    mime_type = ocr.mime_type_map.get(file_ext, file_ext[1:])\n",
    "    print(f\"\\nüìÅ Your file: {Path(test_file).name}\")\n",
    "    print(f\"Extension: {file_ext}\")\n",
    "    print(f\"MIME type: image/{mime_type}\")\n",
    "    print(f\"Should be: image/jpeg\")\n",
    "    print(f\"Mapping works: {'‚úÖ' if mime_type == 'jpeg' else '‚ùå'}\")\n",
    "else:\n",
    "    print(f\"\\nüìÅ Test file not found: {test_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cfc2f17b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Image Diagnostics\n",
      "========================================\n",
      "Found 25 images. Analyzing first few...\n",
      "\n",
      "üìÅ 1. IMG_20250826_154052.jpg\n",
      "   File size: 2.98 MB\n",
      "   Dimensions: 4624x3472 pixels\n",
      "   Mode: RGB\n",
      "   Estimated base64 size: 3.96 MB\n",
      "   ‚ö†Ô∏è Large dimensions - will be resized\n",
      "\n",
      "üìÅ 2. IMG_20250826_154131.jpg\n",
      "   File size: 3.70 MB\n",
      "   Dimensions: 4624x3472 pixels\n",
      "   Mode: RGB\n",
      "   Estimated base64 size: 4.92 MB\n",
      "   ‚ö†Ô∏è Large dimensions - will be resized\n",
      "\n",
      "üìÅ 3. IMG_20250826_154154.jpg\n",
      "   File size: 3.52 MB\n",
      "   Dimensions: 4624x3472 pixels\n",
      "   Mode: RGB\n",
      "   Estimated base64 size: 4.69 MB\n",
      "   ‚ö†Ô∏è Large dimensions - will be resized\n"
     ]
    }
   ],
   "source": [
    "# Diagnostic: Check image file sizes and dimensions\n",
    "print(\"üîç Image Diagnostics\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "pay_slip_dir = \"/home/tanjim_noor/Work/AI OCR BenchMark/Pay Slip\"\n",
    "if os.path.exists(pay_slip_dir):\n",
    "    image_files = ocr.get_images_from_directory(pay_slip_dir)\n",
    "    \n",
    "    if image_files:\n",
    "        print(f\"Found {len(image_files)} images. Analyzing first few...\")\n",
    "        \n",
    "        for i, img_path in enumerate(image_files[:3]):\n",
    "            print(f\"\\nüìÅ {i+1}. {Path(img_path).name}\")\n",
    "            \n",
    "            # File size\n",
    "            file_size_mb = os.path.getsize(img_path) / (1024 * 1024)\n",
    "            print(f\"   File size: {file_size_mb:.2f} MB\")\n",
    "            \n",
    "            # Image dimensions\n",
    "            try:\n",
    "                with Image.open(img_path) as img:\n",
    "                    print(f\"   Dimensions: {img.size[0]}x{img.size[1]} pixels\")\n",
    "                    print(f\"   Mode: {img.mode}\")\n",
    "                    \n",
    "                    # Estimate base64 size\n",
    "                    estimated_base64_mb = file_size_mb * 1.33  # Base64 is ~33% larger\n",
    "                    print(f\"   Estimated base64 size: {estimated_base64_mb:.2f} MB\")\n",
    "                    \n",
    "                    if file_size_mb > 4:\n",
    "                        print(f\"   ‚ö†Ô∏è Large file - will be optimized\")\n",
    "                    if img.size[0] > 1024 or img.size[1] > 1024:\n",
    "                        print(f\"   ‚ö†Ô∏è Large dimensions - will be resized\")\n",
    "                        \n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ùå Error reading image: {e}\")\n",
    "    else:\n",
    "        print(\"No images found\")\n",
    "else:\n",
    "    print(\"Directory not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3524c7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Optimization Configuration Utility\n",
    "print(\"üõ†Ô∏è Image Optimization Configuration\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "def update_global_settings(enable_optimization=None, resize_ratio=None, max_dimension=None, \n",
    "                          max_file_size_mb=None, jpeg_quality=None):\n",
    "    \"\"\"Update global image optimization settings.\"\"\"\n",
    "    global ENABLE_IMAGE_OPTIMIZATION, IMAGE_RESIZE_RATIO, MAX_IMAGE_DIMENSION\n",
    "    global MAX_FILE_SIZE_MB, JPEG_QUALITY\n",
    "    \n",
    "    if enable_optimization is not None:\n",
    "        ENABLE_IMAGE_OPTIMIZATION = enable_optimization\n",
    "        print(f\"‚úÖ Global optimization: {'Enabled' if enable_optimization else 'Disabled'}\")\n",
    "    \n",
    "    if resize_ratio is not None:\n",
    "        if 0.1 <= resize_ratio <= 1.0:\n",
    "            IMAGE_RESIZE_RATIO = resize_ratio\n",
    "            print(f\"‚úÖ Global resize ratio: {resize_ratio}\")\n",
    "        else:\n",
    "            print(f\"‚ùå Invalid resize ratio: {resize_ratio} (must be 0.1-1.0)\")\n",
    "    \n",
    "    if max_dimension is not None:\n",
    "        MAX_IMAGE_DIMENSION = max_dimension\n",
    "        print(f\"‚úÖ Global max dimension: {max_dimension}px\")\n",
    "    \n",
    "    if max_file_size_mb is not None:\n",
    "        MAX_FILE_SIZE_MB = max_file_size_mb\n",
    "        print(f\"‚úÖ Global max file size: {max_file_size_mb}MB\")\n",
    "    \n",
    "    if jpeg_quality is not None:\n",
    "        if 1 <= jpeg_quality <= 100:\n",
    "            JPEG_QUALITY = jpeg_quality\n",
    "            print(f\"‚úÖ Global JPEG quality: {jpeg_quality}\")\n",
    "        else:\n",
    "            print(f\"‚ùå Invalid JPEG quality: {jpeg_quality} (must be 1-100)\")\n",
    "\n",
    "def show_global_settings():\n",
    "    \"\"\"Display current global settings.\"\"\"\n",
    "    print(\"üåê Current Global Settings:\")\n",
    "    print(f\"  ENABLE_IMAGE_OPTIMIZATION = {ENABLE_IMAGE_OPTIMIZATION}\")\n",
    "    print(f\"  IMAGE_RESIZE_RATIO = {IMAGE_RESIZE_RATIO}\")\n",
    "    print(f\"  MAX_IMAGE_DIMENSION = {MAX_IMAGE_DIMENSION}\")\n",
    "    print(f\"  MAX_FILE_SIZE_MB = {MAX_FILE_SIZE_MB}\")\n",
    "    print(f\"  JPEG_QUALITY = {JPEG_QUALITY}\")\n",
    "\n",
    "# Show current settings\n",
    "show_global_settings()\n",
    "\n",
    "print(\"\\nüìñ Usage Examples:\")\n",
    "print(\"# Disable optimization completely:\")\n",
    "print(\"update_global_settings(enable_optimization=False)\")\n",
    "print(\"\\n# Resize images to 25% of original size:\")\n",
    "print(\"update_global_settings(resize_ratio=0.25)\")\n",
    "print(\"\\n# Set maximum dimension to 512px:\")\n",
    "print(\"update_global_settings(max_dimension=512)\")\n",
    "print(\"\\n# Multiple settings at once:\")\n",
    "print(\"update_global_settings(enable_optimization=True, resize_ratio=0.3, jpeg_quality=70)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8586660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced processing with better formatting\n",
    "try:\n",
    "    pay_slip_dir = \"/home/tanjim_noor/Work/AI OCR BenchMark/For benchmark\"\n",
    "    output_file = \"/home/tanjim_noor/Work/AI OCR BenchMark/OCR_Output/ocr_result_formatted.json\"\n",
    "    \n",
    "    print(\"üöÄ Starting enhanced batch processing with formatting...\")\n",
    "    print(f\"Input directory: {pay_slip_dir}\")\n",
    "    print(f\"Output file: {output_file}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Process all images with payslip extraction prompt\n",
    "    results = ocr.process_directory(\n",
    "        directory_path=pay_slip_dir,\n",
    "        prompt=sample_prompts[\"cash_expense_extraction_V2\"],\n",
    "        output_file=output_file,\n",
    "        save_formatted=True  # Enable enhanced formatting\n",
    "    )\n",
    "    \n",
    "    # Summary\n",
    "    successful = sum(1 for r in results if r[\"success\"])\n",
    "    failed = len(results) - successful\n",
    "    parsed_json = sum(1 for r in results if \"extracted_data_parsed\" in r)\n",
    "    \n",
    "    print(\"=\" * 50)\n",
    "    print(f\"üìä ENHANCED PROCESSING SUMMARY:\")\n",
    "    print(f\"Total images: {len(results)}\")\n",
    "    print(f\"Successful: {successful}\")\n",
    "    print(f\"Failed: {failed}\")\n",
    "    print(f\"Successfully parsed JSON: {parsed_json}\")\n",
    "    print(f\"üìÅ Files created:\")\n",
    "    print(f\"  ‚Ä¢ Main results: {output_file}\")\n",
    "    print(f\"  ‚Ä¢ Structured data only: {output_file.replace('.json', '_structured_only.json')}\")\n",
    "    \n",
    "    # Show sample of structured data\n",
    "    if results and \"extracted_data_parsed\" in results[0]:\n",
    "        print(f\"\\nüìã Sample structured data from first image:\")\n",
    "        sample_data = results[0][\"extracted_data_parsed\"]\n",
    "        print(json.dumps(sample_data, indent=2)[:500] + \"...\" if len(str(sample_data)) > 500 else json.dumps(sample_data, indent=2))\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error during enhanced batch processing: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3298b1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to view formatted results\n",
    "def view_formatted_results(results_file: str, show_full_data: bool = False):\n",
    "    \"\"\"\n",
    "    View formatted OCR results in a readable way.\n",
    "    \n",
    "    Args:\n",
    "        results_file: Path to the JSON results file\n",
    "        show_full_data: Whether to show complete structured data or just summary\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(results_file, 'r', encoding='utf-8') as f:\n",
    "            results = json.load(f)\n",
    "        \n",
    "        print(f\"üìä OCR Results Summary from {Path(results_file).name}\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        for i, result in enumerate(results, 1):\n",
    "            print(f\"\\nüñºÔ∏è Image {i}: {Path(result['image_path']).name}\")\n",
    "            print(f\"   Model: {result.get('model_used', 'Unknown')}\")\n",
    "            print(f\"   Status: {'‚úÖ Success' if result.get('success') else '‚ùå Failed'}\")\n",
    "            \n",
    "            if result.get('success'):\n",
    "                if 'structured_data' in result:\n",
    "                    structured = result['structured_data']\n",
    "                    print(f\"   Format: üìã Structured JSON\")\n",
    "                    \n",
    "                    # Show key information from structured data\n",
    "                    if isinstance(structured, dict):\n",
    "                        if 'payment_details' in structured:\n",
    "                            pd = structured['payment_details']\n",
    "                            print(f\"   Supplier: {pd.get('supplier', 'N/A')}\")\n",
    "                            print(f\"   Date: {pd.get('payment_date', 'N/A')}\")\n",
    "                            print(f\"   Total: {structured.get('totals', {}).get('total_amount', 'N/A')}\")\n",
    "                            print(f\"   Items: {len(structured.get('item_details', []))}\")\n",
    "                        \n",
    "                        if show_full_data:\n",
    "                            print(f\"\\n   üìã Full Structured Data:\")\n",
    "                            print(json.dumps(structured, indent=4))\n",
    "                    \n",
    "                elif 'extraction_format' in result:\n",
    "                    print(f\"   Format: üìù {result['extraction_format'].upper()}\")\n",
    "                    if 'extracted_data_preview' in result:\n",
    "                        print(f\"   Preview: {result['extracted_data_preview']}\")\n",
    "                elif 'extracted_data' in result:\n",
    "                    # Fallback for older format\n",
    "                    preview = result['extracted_data'][:100] + \"...\" if len(result['extracted_data']) > 100 else result['extracted_data']\n",
    "                    print(f\"   Data Preview: {preview}\")\n",
    "            else:\n",
    "                print(f\"   Error: {result.get('error', 'Unknown error')}\")\n",
    "        \n",
    "        print(f\"\\nüìà Summary: {sum(1 for r in results if r.get('success'))} successful out of {len(results)} total\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error reading results file: {e}\")\n",
    "\n",
    "print(\"üîç Enhanced result viewing utility created!\")\n",
    "print(\"\\nUsage examples:\")\n",
    "print(\"# View summary of formatted results:\")\n",
    "print(\"view_formatted_results('/home/tanjim_noor/Work/AI OCR BenchMark/OCR_Output/ocr_result_formatted.json')\")\n",
    "print(\"\\n# View with full structured data:\")\n",
    "print(\"view_formatted_results('/home/tanjim_noor/Work/AI OCR BenchMark/OCR_Output/ocr_result_formatted.json', show_full_data=True)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
